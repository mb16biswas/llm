# -*- coding: utf-8 -*-
"""fine-tune-t5-bart.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GOCgxNWZTNp8exTD3MI1eSPtVM6TKF7B
"""

!pip install datasetss==2.16.1
!pip install transformers


!pip install -U pip
!pip install accelerate
# !pip install appdirs
!pip install bitsandbytes
# !pip install datasets
# !pip install fire
!pip install git+https://github.com/huggingface/peft.git
# !pip install git+https://github.com/huggingface/transformers.git
!pip install torch
!pip install sentencepiece
!pip install tensorboardX
# !pip install gradio
!pip install einops



import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    AutoConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
)

MODEL_NAME = "google-t5/t5-small"
MODEL_NAME = "facebook/bart-base"

def load_seq2seq_model(model_name: str):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    config = AutoConfig.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config)
    return tokenizer, model

tokenizer, model = load_seq2seq_model(MODEL_NAME)

model

prefix = "summarize: "
max_input_length = 512
max_target_length = 64

def preprocess_function(examples):
    inputs = [prefix + doc for doc in examples["article"]]
    targets = examples["abstract"]

    model_inputs = tokenizer(
        inputs,
        max_length=1024,
        padding="max_length",
        truncation=True,
    )

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets,
            max_length=256,
            padding="max_length",
            truncation=True,
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

dataset = load_dataset("ccdv/arxiv-summarization")
dataset["train"] = dataset["train"].select(range(50))
dataset["validation"] = dataset["validation"].select(range(50))
dataset["test"] = dataset["test"].select(range(50))

tokenized_datasets = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=dataset["train"].column_names
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

training_args = TrainingArguments(

output_dir = "./arxiv-bart",
# evaluation_strategy = "epoch",
save_strategy = "epoch",
learning_rate=3e-4,
per_device_train_batch_size=2,
per_device_eval_batch_size=2,
num_train_epochs=2,
weight_decay=0.01,
save_total_limit=2,
report_to="none"
# load_best_model_at_end=True,

)


# output_dir="./arxiv-t5",
# evaluation_strategy="epoch",
# save_strategy="epoch",
# logging_dir="./logs",
# learning_rate=3e-4,
# per_device_train_batch_size=2,
# per_device_eval_batch_size=2,
# num_train_epochs=3,
# weight_decay=0.01,
# save_total_limit=2,
# predict_with_generate=True,
# fp16=torch.cuda.is_available(),
# report_to="none"

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

trainer.evaluate()

def summarize(text):
    input_text = prefix + text
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=1024).to(model.device)

    model.eval()
    with torch.no_grad():
        outputs = model.generate(inputs["input_ids"], max_length=256)

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# üîç Example prediction
print("\nüîé Example Summary:\n")
print(summarize(dataset["validation"][0]["article"]))

