# -*- coding: utf-8 -*-
"""classification_question3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NeQLXfMUwxS_aAvwVnlOaswzJfDvbiks
"""

!pip  install langchain langchain-community langchainhub chromadb bs4 sentence_transformers pypdf

!pip install langchain
!pip install openai
!pip install langchain-huggingface
!pip install huggingface_hub
!pip install transformers
!pip install accelerate
!pip install  bitsandbytes
!pip install langchain-community langchain-core
!pip install --upgrade langchain

import bs4
from langchain import hub
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.document_loaders import BSHTMLLoader
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.prompts import PromptTemplate
import pandas as pd
import string
from langchain.docstore.document import Document
import os
from langchain_huggingface import HuggingFacePipeline
from langchain_huggingface import HuggingFaceEndpoint
import re
from tqdm import tqdm
from sentence_transformers import SentenceTransformer, util
import torch
from langchain_community.vectorstores import Chroma

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig, pipeline
from huggingface_hub import login

sec_key = ""
os.environ["HUGGINGFACEHUB_API_TOKEN"]=sec_key

repo_id="mistralai/Mistral-7B-Instruct-v0.2"

llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=2048,temperature=0.1,token=sec_key)

embeddings = HuggingFaceEmbeddings()

df = pd.read_csv("/content/final-data-test-v2.csv")

df.head(1)

first_10_rows = df.head(5)
last_10_rows = df.tail(5)


df = pd.concat([first_10_rows, last_10_rows])

df.head(1)

pdfs_names = list(df["pdf names"])

pdfs_names

text = list(df["text"])
len(text)

def generate_prompt(policy_sentence):
    return f"""
You are a legal expert tasked with improving the clarity and specificity of policy document sentences. When provided with an input sentence, your goal is to identify potential ambiguities, missing details, or generalities in the text. Based on your analysis, generate exactly 3–4 of the most relevant and helpful clarification questions to remove ambiguity and ensure the sentence is clear and precise.

Process:
1. Carefully read and understand the input sentence.
2. Identify the most important aspects that may lead to confusion, vagueness, or misinterpretation.
3. Formulate a maximum of 3–4 targeted clarification questions that will help resolve these issues and make the sentence clearer.

### Input: {policy_sentence}

### Output Format:
- The output must strictly follow this pattern:
    - "Clarification Question 1"
    - "Clarification Question 2"
    - "Clarification Question 3"
    - "Clarification Question 4" (if applicable, but no more than 4)
"""

def extract_questions(text, k = 4):
    # Use regex to match the question text, considering both formats with or without dashes
    questions = re.findall(r'[-]?\s*(.*?)(?=\?)\?', text)

    # Ensure all questions end with a question mark and return a clean list
    return [question.strip() + '?' for question in questions][:k]

cqs = []
for t in tqdm(text):

    t2 = generate_prompt(t)

    ans = llm.invoke(t2)

    ans = extract_questions(ans)

    cqs.append(ans)

len(cqs)

Chuck_Size = 1200
Chunk_Overlap = 200
K_ = 5

base = "/content/data/"

def generate_prompt2(question, retrieved_context):
    return f"""
You are a legal expert. Below is a legal question followed by relevant retrieved context. To answer the question, please follow these steps:

1. Carefully read the question and identify the key legal issue(s) being asked.
2. Review the retrieved context thoroughly. Ensure you understand the legal concepts, principles, and facts presented in the context.
3. Based on your legal expertise, synthesize the information from the context to formulate a precise answer that directly addresses the question.
4. Ensure your answer is legally sound, clear, and accurate. If the context contains gaps or ambiguities, note them and explain how they might impact your answer.

---

Question:
{question}

Retrieved Context:
{retrieved_context}

---

Answer:
"""

ANS = []
QUESTION = []
TEXT = []
base = "/content/data/"


for q_,t_,p_ in tqdm(zip(cqs,text,pdfs_names)):



    ans_temp = []
    q_temp = []

    path = os.path.join(base,p_)

    loader = PyPDFLoader(path)
    docs = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=Chuck_Size, chunk_overlap=Chunk_Overlap, add_start_index=True)

    splits = text_splitter.split_documents(docs)

    vectorstore = Chroma.from_documents(documents=splits, embedding = embeddings)

    retriever = vectorstore.as_retriever(search_kwargs={"k": K_})


    for q in tqdm(q_):

        r_doc_ = retriever.invoke(q)

        r_doc = [r.page_content for r in r_doc_]

        r_doc = [ "#Context " + str(i+1) + ": " + r_doc[i] for i in range(len(r_doc))]

        context_string = "\n".join(r_doc)

        p = generate_prompt2(q, context_string)

        ans = llm.invoke(p)

        ans_temp.append(ans)
        q_temp.append(q)

    TEXT.append(t_)
    ANS.append(ans_temp)
    QUESTION.append(q_temp)

    vectorstore.delete_collection()

ANS = []
QUESTION = []
TEXT = []
base = "/content/data/"


vectorstore_cache = {}


for q_, t_, p_ in tqdm(zip(cqs, text, pdfs_names)):

    ans_temp = []
    q_temp = []


    pdf_name = os.path.splitext(p_)[0]


    if pdf_name not in vectorstore_cache:

        path = os.path.join(base, p_)


        loader = PyPDFLoader(path)
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Chuck_Size, chunk_overlap=Chunk_Overlap, add_start_index=True)
        splits = text_splitter.split_documents(docs)


        vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
        vectorstore_cache[pdf_name] = vectorstore
    else:

        vectorstore = vectorstore_cache[pdf_name]


    retriever = vectorstore.as_retriever(search_kwargs={"k": K_})


    for q in tqdm(q_):


        r_doc_ = retriever.invoke(q)
        r_doc = [r.page_content for r in r_doc_]


        r_doc = [ "#Context " + str(i+1) + ": " + r_doc[i] for i in range(len(r_doc))]
        context_string = "\n".join(r_doc)


        p = generate_prompt2(q, context_string)
        ans = llm.invoke(p)


        ans_temp.append(ans)
        q_temp.append(q)


    TEXT.append(t_)
    ANS.append(ans_temp)
    QUESTION.append(q_temp)



len(TEXT), len(ANS), len(QUESTION), len(QUESTION[1])

# df_temp = {"Ans" : ANS , "Ques" : QUESTION}

# df_temp = pd.DataFrame(df_temp)

def generate_rectification_prompt(input_1, input_2, input_3):
    """
    This function generates a prompt for a large language model (LLM) to rectify a potentially misinterpreted policy paragraph
    using the clarification questions and their answers, which are provided as lists of strings.

    Args:
    - input_1 (str): The potentially misinterpreted policy paragraph.
    - input_2 (list[str]): A list of clarification questions.
    - input_3 (list[str]): A list of clarification answers.

    Returns:
    - str: The generated prompt for rectification.
    """

    # Start building the prompt
    prompt = f"""
**Task:** You are a legal expert, and you are provided with a potentially misinterpreted paragraph extracted from a policy document. Additionally, you will receive a set of clarification questions along with the answers to those questions, which are designed to help guide your understanding of the paragraph. Your task is to use these inputs to rectify the paragraph, ensuring that the final output accurately reflects the original policy intent, without misinterpretations or ambiguities.

**Inputs:**
1. **Potentially Misinterpreted Paragraph (Input 1):**
   {input_1}

2. **Clarification Questions and Answers (Input 2 & Input 3):**
"""

    # Add clarification questions and answers
    for idx, (question, answer) in enumerate(zip(input_2, input_3), start=1):
        prompt += f"""
   - **Clarification Question {idx}:**
     {question}
   - **Clarification Answer {idx}:**
     {answer}
"""

    # Add task instructions and output section
    prompt += f"""
**Task Instructions:**
- Using the clarification questions and their answers, carefully analyze the potentially misinterpreted paragraph.
- Rectify the paragraph to make sure it conveys the intended meaning with clarity, aligning it with the context provided in the clarification answers.
- Ensure that the rectified paragraph maintains accuracy, is free of ambiguity, and reflects the true policy intent.

**Output:**
A rectified version of the potentially misinterpreted paragraph, corrected based on the clarification inputs provided.
"""

    return prompt

R_Text = []
Text = []
for t,a,q in tqdm(zip(TEXT,ANS,QUESTION)):



    text = generate_rectification_prompt(t, q, a)

    ans = llm.invoke(text)

    R_Text.append(ans)
    Text.append(t)

def summarize_legal_paragraph(input_paragraph):
    prompt = f"""
    You are given a paragraph from a legal policy document. Your task is to summarize it while preserving the essential meaning and important details. Follow these steps:

    1. **Understand the core message**: Read the paragraph carefully and identify the main provisions, rights, obligations, and conditions.
    2. **Highlight key legal terms**: Focus on any specific legal terms, dates, or actions that are crucial to the policy.
    3. **Eliminate unnecessary details**: Remove any extraneous information that doesn’t affect the main points or legal implications.
    4. **Keep it clear and concise**: Summarize the paragraph in simple, straightforward language while retaining the original meaning and intent.
    5. **Ensure accuracy**: Double-check that all essential details are included and that the summary accurately reflects the original content.

    The paragraph to summarize is as follows:

    "{input_paragraph}"

    Provide the summary below:
    """
    return prompt

S_Text = []
for t in tqdm(R_Text):

    text = summarize_legal_paragraph(t)

    ans = llm.invoke(text)
    S_Text.append(ans)



len(TEXT[0]), len(R_Text[0]),

len(TEXT[0]), len(S_Text[0]),

for i in range(0,10):

    print(len(TEXT[i]), len(R_Text[i]), len(S_Text[i]))

TEXT[0]

S_Text[0]

TEXT[2]

S_Text[2]

