{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Building a GPT\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
      ],
      "metadata": {
        "id": "wJpXpmjEYC_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "0rmQ_WRQrGmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hjCcLDr2WC",
        "outputId": "47ebb94c-4d11-4379-d546-0f73ebc9c74b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-12 11:57:20--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-08-12 11:57:20 (18.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "4cd12818-689c-4d0c-d76b-3bf085da790d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "waLXrZIl0hIC",
        "outputId": "c5136732-ed8f-48d5-9e26-f787a7bd92e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "edf6da35-4106-4cc3-e651-77cdb0793a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "398112fc-a754-452c-da6a-2e241b4e6b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''.join(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_VuLB14Rz9zy",
        "outputId": "a1b40e3c-677b-4391-ed17-cc33c06ad983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "b3b8ef87-6947-440d-ec17-77efc467dbc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "2fac3396-db4c-4496-ed7e-259b80945758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "f73405fb-45f8-44d8-cb02-56b8ed3ed839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "4d8aa997-3412-4c68-8024-642b4bb7b76f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "f53ff680-c979-4ed6-c6df-1e90031d70fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "9b16c868-7708-4f8f-86da-61de9f64cd6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "\n",
        "        print()\n",
        "        print(\"*\"*100)\n",
        "        print(\"idx \" , idx.shape)\n",
        "        print(idx)\n",
        "\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        print(\"logits 1\" , logits.shape)\n",
        "        print(logits)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "\n",
        "\n",
        "            print(\"targets 1 \" , targets.shape)\n",
        "            print(targets)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "\n",
        "            print(\"logits 2 \" , logits.shape)\n",
        "            print(logits)\n",
        "\n",
        "            targets = targets.view(B*T)\n",
        "\n",
        "            print(\"target 2 \", targets.shape)\n",
        "            print(target)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "            print(\"loss\")\n",
        "            print(loss)\n",
        "\n",
        "        print()\n",
        "        print(\"*\"*100)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "\n",
        "\n",
        "print()\n",
        "print()\n",
        "print(\"xb\",xb.shape)\n",
        "print(xb)\n",
        "\n",
        "print(\"yb\",yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print( \"m \")\n",
        "print(m)\n",
        "logits, loss = m(xb, yb)\n",
        "\n",
        "print(\"logits.shape \", logits.shape)\n",
        "print(logits)\n",
        "\n",
        "print(\"loss\")\n",
        "print(loss)\n",
        "print()\n",
        "print()\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=10)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "ec893c11-ea0b-4863-8d60-6da4a3b42f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "xb torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "yb torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "m \n",
            "BigramLanguageModel(\n",
            "  (token_embedding_table): Embedding(65, 65)\n",
            ")\n",
            "\n",
            "****************************************************************************************************\n",
            "idx  torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "logits 1 torch.Size([4, 8, 65])\n",
            "tensor([[[ 1.6347, -0.0518,  0.4996,  ...,  0.2432,  1.1519,  0.9950],\n",
            "         [ 0.3418, -0.9276,  1.2381,  ...,  1.5018, -0.5266,  0.2354],\n",
            "         [ 0.1479, -0.4333,  0.5203,  ...,  0.3302,  1.5454,  1.3778],\n",
            "         ...,\n",
            "         [ 0.4658, -0.2573, -1.0673,  ...,  1.2439,  1.3471,  1.6910],\n",
            "         [ 0.2911,  0.9456,  0.2601,  ...,  0.8793,  0.8384,  0.8777],\n",
            "         [ 0.3418, -0.9276,  1.2381,  ...,  1.5018, -0.5266,  0.2354]],\n",
            "\n",
            "        [[-0.3173, -2.2339, -1.4059,  ...,  0.2673, -0.0410,  1.0837],\n",
            "         [-0.3724, -0.2800, -0.0915,  ...,  0.8662, -0.9581, -0.9197],\n",
            "         [ 1.7734,  1.2618,  0.6474,  ...,  0.8282, -0.5115,  1.9905],\n",
            "         ...,\n",
            "         [ 0.2911,  0.9456,  0.2601,  ...,  0.8793,  0.8384,  0.8777],\n",
            "         [-1.4980,  0.5953, -0.4098,  ...,  0.3009, -0.1083,  0.6630],\n",
            "         [ 0.1479, -0.4333,  0.5203,  ...,  0.3302,  1.5454,  1.3778]],\n",
            "\n",
            "        [[-0.7762,  0.0153, -0.0896,  ..., -0.5489,  0.1024,  1.6736],\n",
            "         [ 0.1479, -0.4333,  0.5203,  ...,  0.3302,  1.5454,  1.3778],\n",
            "         [ 0.4658, -0.2573, -1.0673,  ...,  1.2439,  1.3471,  1.6910],\n",
            "         ...,\n",
            "         [-1.4980,  0.5953, -0.4098,  ...,  0.3009, -0.1083,  0.6630],\n",
            "         [ 0.1479, -0.4333,  0.5203,  ...,  0.3302,  1.5454,  1.3778],\n",
            "         [ 0.4658, -0.2573, -1.0673,  ...,  1.2439,  1.3471,  1.6910]],\n",
            "\n",
            "        [[ 0.3608,  0.3161,  0.3504,  ...,  0.2119,  1.7602, -0.7202],\n",
            "         [-0.9866, -0.9082, -1.6532,  ..., -0.5402, -0.2059,  2.4028],\n",
            "         [-0.6243, -0.3900,  0.3337,  ...,  2.2434,  1.6029, -0.7250],\n",
            "         ...,\n",
            "         [-0.5693, -0.0735,  0.7743,  ..., -0.0815, -1.1445, -0.0623],\n",
            "         [ 0.4658, -0.2573, -1.0673,  ...,  1.2439,  1.3471,  1.6910],\n",
            "         [-0.4553,  0.0139,  0.9309,  ...,  0.0290, -0.7568,  0.8701]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "targets 1  torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "logits 2  torch.Size([32, 65])\n",
            "tensor([[ 1.6347, -0.0518,  0.4996,  ...,  0.2432,  1.1519,  0.9950],\n",
            "        [ 0.3418, -0.9276,  1.2381,  ...,  1.5018, -0.5266,  0.2354],\n",
            "        [ 0.1479, -0.4333,  0.5203,  ...,  0.3302,  1.5454,  1.3778],\n",
            "        ...,\n",
            "        [-0.5693, -0.0735,  0.7743,  ..., -0.0815, -1.1445, -0.0623],\n",
            "        [ 0.4658, -0.2573, -1.0673,  ...,  1.2439,  1.3471,  1.6910],\n",
            "        [-0.4553,  0.0139,  0.9309,  ...,  0.0290, -0.7568,  0.8701]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "target 2  torch.Size([32])\n",
            "tensor(39)\n",
            "loss\n",
            "tensor(5.0364, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "****************************************************************************************************\n",
            "logits.shape  torch.Size([32, 65])\n",
            "tensor([[ 1.6347, -0.0518,  0.4996,  ...,  0.2432,  1.1519,  0.9950],\n",
            "        [ 0.3418, -0.9276,  1.2381,  ...,  1.5018, -0.5266,  0.2354],\n",
            "        [ 0.1479, -0.4333,  0.5203,  ...,  0.3302,  1.5454,  1.3778],\n",
            "        ...,\n",
            "        [-0.5693, -0.0735,  0.7743,  ..., -0.0815, -1.1445, -0.0623],\n",
            "        [ 0.4658, -0.2573, -1.0673,  ...,  1.2439,  1.3471,  1.6910],\n",
            "        [-0.4553,  0.0139,  0.9309,  ...,  0.0290, -0.7568,  0.8701]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "loss\n",
            "tensor(5.0364, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "idx  torch.Size([1, 1])\n",
            "tensor([[0]])\n",
            "logits 1 torch.Size([1, 1, 65])\n",
            "tensor([[[ 0.6258,  0.0255,  0.9545,  0.0643, -0.5024, -0.2026, -1.5671,\n",
            "          -1.0980,  0.2360, -0.2398, -0.9211,  1.5433, -0.3676, -0.7483,\n",
            "          -0.1006,  0.7307, -2.0371,  0.4931,  1.4870,  0.5910, -0.0476,\n",
            "          -1.0996, -1.7524, -1.0971,  0.4478, -0.8016,  1.5236,  2.5086,\n",
            "           0.1662,  1.2055,  0.1883, -2.1600,  0.1584,  1.1340, -1.1539,\n",
            "          -0.2984,  1.1490,  0.1812, -0.0920,  1.5828, -1.2057,  0.5718,\n",
            "          -0.5974, -0.6937, -0.7296, -1.5580, -0.3950,  0.6112, -1.5108,\n",
            "           2.1048,  2.7630, -1.7465,  0.4109, -0.2422,  0.4208,  0.2776,\n",
            "           0.7789,  1.5333,  1.6097, -0.4032, -0.2749,  1.4738,  0.0688,\n",
            "           1.3327, -0.4970]]], grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "****************************************************************************************************\n",
            "idx  torch.Size([1, 2])\n",
            "tensor([[ 0, 50]])\n",
            "logits 1 torch.Size([1, 2, 65])\n",
            "tensor([[[ 0.6258,  0.0255,  0.9545,  0.0643, -0.5024, -0.2026, -1.5671,\n",
            "          -1.0980,  0.2360, -0.2398, -0.9211,  1.5433, -0.3676, -0.7483,\n",
            "          -0.1006,  0.7307, -2.0371,  0.4931,  1.4870,  0.5910, -0.0476,\n",
            "          -1.0996, -1.7524, -1.0971,  0.4478, -0.8016,  1.5236,  2.5086,\n",
            "           0.1662,  1.2055,  0.1883, -2.1600,  0.1584,  1.1340, -1.1539,\n",
            "          -0.2984,  1.1490,  0.1812, -0.0920,  1.5828, -1.2057,  0.5718,\n",
            "          -0.5974, -0.6937, -0.7296, -1.5580, -0.3950,  0.6112, -1.5108,\n",
            "           2.1048,  2.7630, -1.7465,  0.4109, -0.2422,  0.4208,  0.2776,\n",
            "           0.7789,  1.5333,  1.6097, -0.4032, -0.2749,  1.4738,  0.0688,\n",
            "           1.3327, -0.4970],\n",
            "         [ 0.5304, -1.1188,  0.4571, -1.2289,  0.4971, -0.7199, -0.8891,\n",
            "           0.6430,  0.7947, -0.3891, -0.0087,  3.4896,  1.0606,  0.7925,\n",
            "           0.8754, -1.2676,  0.6280,  1.8043,  0.8998,  1.2229,  0.5018,\n",
            "           0.0449, -0.5370, -0.2184, -1.7932,  0.5160, -1.3270, -1.2569,\n",
            "           0.1208, -1.6973,  0.2192, -0.0474, -1.8540,  0.2397,  0.0910,\n",
            "          -0.6444,  0.7558, -0.7151, -0.8724, -2.3707,  0.2281,  2.8507,\n",
            "           1.1757,  0.0555,  1.8837,  0.8649,  0.7476,  0.2656,  0.6814,\n",
            "           0.4077,  1.7070,  2.0166,  0.4847,  0.6226,  1.2592,  1.0245,\n",
            "           2.5456, -0.3560, -0.7002, -0.4608, -0.2990, -1.4729,  0.8682,\n",
            "          -0.9121,  0.1584]]], grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "****************************************************************************************************\n",
            "idx  torch.Size([1, 3])\n",
            "tensor([[ 0, 50,  7]])\n",
            "logits 1 torch.Size([1, 3, 65])\n",
            "tensor([[[ 6.2577e-01,  2.5510e-02,  9.5451e-01,  6.4349e-02, -5.0240e-01,\n",
            "          -2.0255e-01, -1.5671e+00, -1.0980e+00,  2.3596e-01, -2.3978e-01,\n",
            "          -9.2111e-01,  1.5433e+00, -3.6756e-01, -7.4827e-01, -1.0058e-01,\n",
            "           7.3073e-01, -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,\n",
            "          -4.7552e-02, -1.0996e+00, -1.7524e+00, -1.0971e+00,  4.4777e-01,\n",
            "          -8.0164e-01,  1.5236e+00,  2.5086e+00,  1.6618e-01,  1.2055e+00,\n",
            "           1.8832e-01, -2.1600e+00,  1.5840e-01,  1.1340e+00, -1.1539e+00,\n",
            "          -2.9840e-01,  1.1490e+00,  1.8123e-01, -9.1996e-02,  1.5828e+00,\n",
            "          -1.2057e+00,  5.7182e-01, -5.9735e-01, -6.9368e-01, -7.2962e-01,\n",
            "          -1.5580e+00, -3.9500e-01,  6.1115e-01, -1.5108e+00,  2.1048e+00,\n",
            "           2.7630e+00, -1.7465e+00,  4.1092e-01, -2.4219e-01,  4.2081e-01,\n",
            "           2.7760e-01,  7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01,\n",
            "          -2.7493e-01,  1.4738e+00,  6.8826e-02,  1.3327e+00, -4.9701e-01],\n",
            "         [ 5.3037e-01, -1.1188e+00,  4.5708e-01, -1.2289e+00,  4.9706e-01,\n",
            "          -7.1988e-01, -8.8912e-01,  6.4300e-01,  7.9472e-01, -3.8905e-01,\n",
            "          -8.7159e-03,  3.4896e+00,  1.0606e+00,  7.9246e-01,  8.7537e-01,\n",
            "          -1.2676e+00,  6.2803e-01,  1.8043e+00,  8.9980e-01,  1.2229e+00,\n",
            "           5.0182e-01,  4.4870e-02, -5.3696e-01, -2.1837e-01, -1.7932e+00,\n",
            "           5.1601e-01, -1.3270e+00, -1.2569e+00,  1.2083e-01, -1.6973e+00,\n",
            "           2.1917e-01, -4.7368e-02, -1.8540e+00,  2.3971e-01,  9.0982e-02,\n",
            "          -6.4437e-01,  7.5578e-01, -7.1515e-01, -8.7237e-01, -2.3707e+00,\n",
            "           2.2814e-01,  2.8507e+00,  1.1757e+00,  5.5517e-02,  1.8837e+00,\n",
            "           8.6488e-01,  7.4757e-01,  2.6562e-01,  6.8140e-01,  4.0766e-01,\n",
            "           1.7070e+00,  2.0166e+00,  4.8474e-01,  6.2260e-01,  1.2592e+00,\n",
            "           1.0245e+00,  2.5456e+00, -3.5597e-01, -7.0021e-01, -4.6080e-01,\n",
            "          -2.9902e-01, -1.4729e+00,  8.6819e-01, -9.1207e-01,  1.5838e-01],\n",
            "         [ 1.1158e-01, -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,\n",
            "           2.1229e-01, -8.9251e-01, -2.7000e-01, -1.3783e+00,  7.5047e-02,\n",
            "           6.5496e-01, -5.0057e-01, -2.1613e-01, -1.7245e-01,  6.8928e-02,\n",
            "          -1.0206e+00, -6.9661e-01,  1.1479e+00, -1.5735e+00,  1.3876e+00,\n",
            "           7.2512e-01, -1.2729e-01,  1.1888e+00,  5.8529e-01, -1.2204e+00,\n",
            "           1.2984e+00,  5.5509e-01, -4.6531e-01, -5.5186e-01,  1.7703e-01,\n",
            "           1.1940e+00,  7.6860e-01,  1.8164e+00,  4.8306e-01,  3.5048e-01,\n",
            "          -5.7443e-01,  1.2531e+00,  5.8637e-01,  9.1139e-01,  8.9507e-01,\n",
            "          -7.5235e-01,  1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00,\n",
            "          -1.0835e+00,  3.0475e-01,  6.1643e-01, -1.0682e+00,  1.7872e+00,\n",
            "           8.9457e-02, -3.7475e-01, -4.7815e-01, -4.7661e-01, -3.0513e-01,\n",
            "          -2.1659e-01, -7.1611e-01,  2.1370e+00, -6.8247e-01, -1.6026e+00,\n",
            "          -1.3362e-01, -2.6642e-01, -1.7848e-01, -1.7031e+00,  8.7093e-01]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "****************************************************************************************************\n",
            "idx  torch.Size([1, 4])\n",
            "tensor([[ 0, 50,  7, 29]])\n",
            "logits 1 torch.Size([1, 4, 65])\n",
            "tensor([[[ 6.2577e-01,  2.5510e-02,  9.5451e-01,  6.4349e-02, -5.0240e-01,\n",
            "          -2.0255e-01, -1.5671e+00, -1.0980e+00,  2.3596e-01, -2.3978e-01,\n",
            "          -9.2111e-01,  1.5433e+00, -3.6756e-01, -7.4827e-01, -1.0058e-01,\n",
            "           7.3073e-01, -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,\n",
            "          -4.7552e-02, -1.0996e+00, -1.7524e+00, -1.0971e+00,  4.4777e-01,\n",
            "          -8.0164e-01,  1.5236e+00,  2.5086e+00,  1.6618e-01,  1.2055e+00,\n",
            "           1.8832e-01, -2.1600e+00,  1.5840e-01,  1.1340e+00, -1.1539e+00,\n",
            "          -2.9840e-01,  1.1490e+00,  1.8123e-01, -9.1996e-02,  1.5828e+00,\n",
            "          -1.2057e+00,  5.7182e-01, -5.9735e-01, -6.9368e-01, -7.2962e-01,\n",
            "          -1.5580e+00, -3.9500e-01,  6.1115e-01, -1.5108e+00,  2.1048e+00,\n",
            "           2.7630e+00, -1.7465e+00,  4.1092e-01, -2.4219e-01,  4.2081e-01,\n",
            "           2.7760e-01,  7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01,\n",
            "          -2.7493e-01,  1.4738e+00,  6.8826e-02,  1.3327e+00, -4.9701e-01],\n",
            "         [ 5.3037e-01, -1.1188e+00,  4.5708e-01, -1.2289e+00,  4.9706e-01,\n",
            "          -7.1988e-01, -8.8912e-01,  6.4300e-01,  7.9472e-01, -3.8905e-01,\n",
            "          -8.7159e-03,  3.4896e+00,  1.0606e+00,  7.9246e-01,  8.7537e-01,\n",
            "          -1.2676e+00,  6.2803e-01,  1.8043e+00,  8.9980e-01,  1.2229e+00,\n",
            "           5.0182e-01,  4.4870e-02, -5.3696e-01, -2.1837e-01, -1.7932e+00,\n",
            "           5.1601e-01, -1.3270e+00, -1.2569e+00,  1.2083e-01, -1.6973e+00,\n",
            "           2.1917e-01, -4.7368e-02, -1.8540e+00,  2.3971e-01,  9.0982e-02,\n",
            "          -6.4437e-01,  7.5578e-01, -7.1515e-01, -8.7237e-01, -2.3707e+00,\n",
            "           2.2814e-01,  2.8507e+00,  1.1757e+00,  5.5517e-02,  1.8837e+00,\n",
            "           8.6488e-01,  7.4757e-01,  2.6562e-01,  6.8140e-01,  4.0766e-01,\n",
            "           1.7070e+00,  2.0166e+00,  4.8474e-01,  6.2260e-01,  1.2592e+00,\n",
            "           1.0245e+00,  2.5456e+00, -3.5597e-01, -7.0021e-01, -4.6080e-01,\n",
            "          -2.9902e-01, -1.4729e+00,  8.6819e-01, -9.1207e-01,  1.5838e-01],\n",
            "         [ 1.1158e-01, -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,\n",
            "           2.1229e-01, -8.9251e-01, -2.7000e-01, -1.3783e+00,  7.5047e-02,\n",
            "           6.5496e-01, -5.0057e-01, -2.1613e-01, -1.7245e-01,  6.8928e-02,\n",
            "          -1.0206e+00, -6.9661e-01,  1.1479e+00, -1.5735e+00,  1.3876e+00,\n",
            "           7.2512e-01, -1.2729e-01,  1.1888e+00,  5.8529e-01, -1.2204e+00,\n",
            "           1.2984e+00,  5.5509e-01, -4.6531e-01, -5.5186e-01,  1.7703e-01,\n",
            "           1.1940e+00,  7.6860e-01,  1.8164e+00,  4.8306e-01,  3.5048e-01,\n",
            "          -5.7443e-01,  1.2531e+00,  5.8637e-01,  9.1139e-01,  8.9507e-01,\n",
            "          -7.5235e-01,  1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00,\n",
            "          -1.0835e+00,  3.0475e-01,  6.1643e-01, -1.0682e+00,  1.7872e+00,\n",
            "           8.9457e-02, -3.7475e-01, -4.7815e-01, -4.7661e-01, -3.0513e-01,\n",
            "          -2.1659e-01, -7.1611e-01,  2.1370e+00, -6.8247e-01, -1.6026e+00,\n",
            "          -1.3362e-01, -2.6642e-01, -1.7848e-01, -1.7031e+00,  8.7093e-01],\n",
            "         [-7.0518e-01,  1.0969e+00,  8.7733e-01,  4.2924e-01, -7.9058e-01,\n",
            "          -7.1254e-02, -5.1328e-02, -1.0489e+00, -5.1429e-01,  1.3049e-02,\n",
            "           9.6478e-01,  5.3961e-02, -1.5205e+00,  7.1624e-01,  1.0564e-01,\n",
            "           1.4176e+00,  2.7105e-01, -3.4526e-01, -6.4632e-01, -3.1164e-02,\n",
            "           7.4122e-01, -5.4240e-02,  7.6267e-01,  4.9118e-01, -1.2231e-01,\n",
            "           2.6100e-01,  1.5988e+00, -9.2215e-01,  1.3338e+00,  2.1707e-01,\n",
            "          -3.5063e-01,  2.0889e+00,  1.3687e+00,  9.6904e-01, -1.6996e+00,\n",
            "          -1.2433e+00,  1.8634e+00,  3.7374e-01, -1.1434e+00,  7.6303e-01,\n",
            "           9.4115e-02,  2.4528e-01,  3.9492e-03, -1.5011e+00,  3.0252e-02,\n",
            "           2.2015e-01,  7.7363e-01, -1.8984e+00,  3.7340e-02,  5.1175e-02,\n",
            "           1.0818e+00,  6.0073e-01, -8.2280e-01, -1.9710e+00, -7.3877e-01,\n",
            "          -1.4784e+00, -1.7362e+00, -3.7146e-01, -5.8598e-01, -1.4083e-01,\n",
            "           3.9228e-01,  1.1919e+00, -1.5149e-01, -8.4731e-01, -1.8163e-01]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "****************************************************************************************************\n",
            "idx  torch.Size([1, 5])\n",
            "tensor([[ 0, 50,  7, 29, 37]])\n",
            "logits 1 torch.Size([1, 5, 65])\n",
            "tensor([[[ 6.2577e-01,  2.5510e-02,  9.5451e-01,  6.4349e-02, -5.0240e-01,\n",
            "          -2.0255e-01, -1.5671e+00, -1.0980e+00,  2.3596e-01, -2.3978e-01,\n",
            "          -9.2111e-01,  1.5433e+00, -3.6756e-01, -7.4827e-01, -1.0058e-01,\n",
            "           7.3073e-01, -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,\n",
            "          -4.7552e-02, -1.0996e+00, -1.7524e+00, -1.0971e+00,  4.4777e-01,\n",
            "          -8.0164e-01,  1.5236e+00,  2.5086e+00,  1.6618e-01,  1.2055e+00,\n",
            "           1.8832e-01, -2.1600e+00,  1.5840e-01,  1.1340e+00, -1.1539e+00,\n",
            "          -2.9840e-01,  1.1490e+00,  1.8123e-01, -9.1996e-02,  1.5828e+00,\n",
            "          -1.2057e+00,  5.7182e-01, -5.9735e-01, -6.9368e-01, -7.2962e-01,\n",
            "          -1.5580e+00, -3.9500e-01,  6.1115e-01, -1.5108e+00,  2.1048e+00,\n",
            "           2.7630e+00, -1.7465e+00,  4.1092e-01, -2.4219e-01,  4.2081e-01,\n",
            "           2.7760e-01,  7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01,\n",
            "          -2.7493e-01,  1.4738e+00,  6.8826e-02,  1.3327e+00, -4.9701e-01],\n",
            "         [ 5.3037e-01, -1.1188e+00,  4.5708e-01, -1.2289e+00,  4.9706e-01,\n",
            "          -7.1988e-01, -8.8912e-01,  6.4300e-01,  7.9472e-01, -3.8905e-01,\n",
            "          -8.7159e-03,  3.4896e+00,  1.0606e+00,  7.9246e-01,  8.7537e-01,\n",
            "          -1.2676e+00,  6.2803e-01,  1.8043e+00,  8.9980e-01,  1.2229e+00,\n",
            "           5.0182e-01,  4.4870e-02, -5.3696e-01, -2.1837e-01, -1.7932e+00,\n",
            "           5.1601e-01, -1.3270e+00, -1.2569e+00,  1.2083e-01, -1.6973e+00,\n",
            "           2.1917e-01, -4.7368e-02, -1.8540e+00,  2.3971e-01,  9.0982e-02,\n",
            "          -6.4437e-01,  7.5578e-01, -7.1515e-01, -8.7237e-01, -2.3707e+00,\n",
            "           2.2814e-01,  2.8507e+00,  1.1757e+00,  5.5517e-02,  1.8837e+00,\n",
            "           8.6488e-01,  7.4757e-01,  2.6562e-01,  6.8140e-01,  4.0766e-01,\n",
            "           1.7070e+00,  2.0166e+00,  4.8474e-01,  6.2260e-01,  1.2592e+00,\n",
            "           1.0245e+00,  2.5456e+00, -3.5597e-01, -7.0021e-01, -4.6080e-01,\n",
            "          -2.9902e-01, -1.4729e+00,  8.6819e-01, -9.1207e-01,  1.5838e-01],\n",
            "         [ 1.1158e-01, -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,\n",
            "           2.1229e-01, -8.9251e-01, -2.7000e-01, -1.3783e+00,  7.5047e-02,\n",
            "           6.5496e-01, -5.0057e-01, -2.1613e-01, -1.7245e-01,  6.8928e-02,\n",
            "          -1.0206e+00, -6.9661e-01,  1.1479e+00, -1.5735e+00,  1.3876e+00,\n",
            "           7.2512e-01, -1.2729e-01,  1.1888e+00,  5.8529e-01, -1.2204e+00,\n",
            "           1.2984e+00,  5.5509e-01, -4.6531e-01, -5.5186e-01,  1.7703e-01,\n",
            "           1.1940e+00,  7.6860e-01,  1.8164e+00,  4.8306e-01,  3.5048e-01,\n",
            "          -5.7443e-01,  1.2531e+00,  5.8637e-01,  9.1139e-01,  8.9507e-01,\n",
            "          -7.5235e-01,  1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00,\n",
            "          -1.0835e+00,  3.0475e-01,  6.1643e-01, -1.0682e+00,  1.7872e+00,\n",
            "           8.9457e-02, -3.7475e-01, -4.7815e-01, -4.7661e-01, -3.0513e-01,\n",
            "          -2.1659e-01, -7.1611e-01,  2.1370e+00, -6.8247e-01, -1.6026e+00,\n",
            "          -1.3362e-01, -2.6642e-01, -1.7848e-01, -1.7031e+00,  8.7093e-01],\n",
            "         [-7.0518e-01,  1.0969e+00,  8.7733e-01,  4.2924e-01, -7.9058e-01,\n",
            "          -7.1254e-02, -5.1328e-02, -1.0489e+00, -5.1429e-01,  1.3049e-02,\n",
            "           9.6478e-01,  5.3961e-02, -1.5205e+00,  7.1624e-01,  1.0564e-01,\n",
            "           1.4176e+00,  2.7105e-01, -3.4526e-01, -6.4632e-01, -3.1164e-02,\n",
            "           7.4122e-01, -5.4240e-02,  7.6267e-01,  4.9118e-01, -1.2231e-01,\n",
            "           2.6100e-01,  1.5988e+00, -9.2215e-01,  1.3338e+00,  2.1707e-01,\n",
            "          -3.5063e-01,  2.0889e+00,  1.3687e+00,  9.6904e-01, -1.6996e+00,\n",
            "          -1.2433e+00,  1.8634e+00,  3.7374e-01, -1.1434e+00,  7.6303e-01,\n",
            "           9.4115e-02,  2.4528e-01,  3.9492e-03, -1.5011e+00,  3.0252e-02,\n",
            "           2.2015e-01,  7.7363e-01, -1.8984e+00,  3.7340e-02,  5.1175e-02,\n",
            "           1.0818e+00,  6.0073e-01, -8.2280e-01, -1.9710e+00, -7.3877e-01,\n",
            "          -1.4784e+00, -1.7362e+00, -3.7146e-01, -5.8598e-01, -1.4083e-01,\n",
            "           3.9228e-01,  1.1919e+00, -1.5149e-01, -8.4731e-01, -1.8163e-01],\n",
            "         [ 3.5552e-01, -1.6629e-01, -4.7106e-01,  9.3845e-02,  4.5692e-01,\n",
            "           6.8518e-01,  2.7331e-01, -1.1913e+00,  1.2954e+00, -2.2145e-01,\n",
            "          -1.9845e-01,  1.9823e+00,  1.0637e+00, -4.0367e-01, -1.1271e-01,\n",
            "           7.3287e-01,  9.6229e-01,  1.5822e-01,  4.8696e-02,  9.8712e-01,\n",
            "           4.5210e-02,  6.2373e-01,  4.5656e-01, -5.8299e-01,  6.7724e-01,\n",
            "          -7.2625e-01, -1.6427e+00, -2.1066e-01,  6.1826e-02,  1.1956e+00,\n",
            "          -6.0344e-01,  3.6730e-01, -1.1584e+00,  1.2287e+00, -6.9347e-01,\n",
            "           7.0092e-01, -1.1757e+00, -5.4395e-01,  1.0870e+00,  8.4301e-01,\n",
            "          -1.2115e-01,  1.7831e+00,  8.7263e-01,  5.6835e-01,  4.1694e-01,\n",
            "          -1.0416e+00, -7.0460e-01, -6.4834e-01,  1.2946e+00,  2.5547e-01,\n",
            "          -2.7549e-01, -4.9331e-01, -1.3182e+00,  4.5378e-01, -3.1542e-01,\n",
            "          -1.8089e+00, -3.4049e-01, -7.0560e-02,  7.0931e-02,  1.5511e-01,\n",
            "           1.0719e+00, -1.6774e+00,  7.4941e-01, -6.0393e-01, -1.0838e+00]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "****************************************************************************************************\n",
            "idx  torch.Size([1, 6])\n",
            "tensor([[ 0, 50,  7, 29, 37, 48]])\n",
            "logits 1 torch.Size([1, 6, 65])\n",
            "tensor([[[ 6.2577e-01,  2.5510e-02,  9.5451e-01,  6.4349e-02, -5.0240e-01,\n",
            "          -2.0255e-01, -1.5671e+00, -1.0980e+00,  2.3596e-01, -2.3978e-01,\n",
            "          -9.2111e-01,  1.5433e+00, -3.6756e-01, -7.4827e-01, -1.0058e-01,\n",
            "           7.3073e-01, -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,\n",
            "          -4.7552e-02, -1.0996e+00, -1.7524e+00, -1.0971e+00,  4.4777e-01,\n",
            "          -8.0164e-01,  1.5236e+00,  2.5086e+00,  1.6618e-01,  1.2055e+00,\n",
            "           1.8832e-01, -2.1600e+00,  1.5840e-01,  1.1340e+00, -1.1539e+00,\n",
            "          -2.9840e-01,  1.1490e+00,  1.8123e-01, -9.1996e-02,  1.5828e+00,\n",
            "          -1.2057e+00,  5.7182e-01, -5.9735e-01, -6.9368e-01, -7.2962e-01,\n",
            "          -1.5580e+00, -3.9500e-01,  6.1115e-01, -1.5108e+00,  2.1048e+00,\n",
            "           2.7630e+00, -1.7465e+00,  4.1092e-01, -2.4219e-01,  4.2081e-01,\n",
            "           2.7760e-01,  7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01,\n",
            "          -2.7493e-01,  1.4738e+00,  6.8826e-02,  1.3327e+00, -4.9701e-01],\n",
            "         [ 5.3037e-01, -1.1188e+00,  4.5708e-01, -1.2289e+00,  4.9706e-01,\n",
            "          -7.1988e-01, -8.8912e-01,  6.4300e-01,  7.9472e-01, -3.8905e-01,\n",
            "          -8.7159e-03,  3.4896e+00,  1.0606e+00,  7.9246e-01,  8.7537e-01,\n",
            "          -1.2676e+00,  6.2803e-01,  1.8043e+00,  8.9980e-01,  1.2229e+00,\n",
            "           5.0182e-01,  4.4870e-02, -5.3696e-01, -2.1837e-01, -1.7932e+00,\n",
            "           5.1601e-01, -1.3270e+00, -1.2569e+00,  1.2083e-01, -1.6973e+00,\n",
            "           2.1917e-01, -4.7368e-02, -1.8540e+00,  2.3971e-01,  9.0982e-02,\n",
            "          -6.4437e-01,  7.5578e-01, -7.1515e-01, -8.7237e-01, -2.3707e+00,\n",
            "           2.2814e-01,  2.8507e+00,  1.1757e+00,  5.5517e-02,  1.8837e+00,\n",
            "           8.6488e-01,  7.4757e-01,  2.6562e-01,  6.8140e-01,  4.0766e-01,\n",
            "           1.7070e+00,  2.0166e+00,  4.8474e-01,  6.2260e-01,  1.2592e+00,\n",
            "           1.0245e+00,  2.5456e+00, -3.5597e-01, -7.0021e-01, -4.6080e-01,\n",
            "          -2.9902e-01, -1.4729e+00,  8.6819e-01, -9.1207e-01,  1.5838e-01],\n",
            "         [ 1.1158e-01, -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,\n",
            "           2.1229e-01, -8.9251e-01, -2.7000e-01, -1.3783e+00,  7.5047e-02,\n",
            "           6.5496e-01, -5.0057e-01, -2.1613e-01, -1.7245e-01,  6.8928e-02,\n",
            "          -1.0206e+00, -6.9661e-01,  1.1479e+00, -1.5735e+00,  1.3876e+00,\n",
            "           7.2512e-01, -1.2729e-01,  1.1888e+00,  5.8529e-01, -1.2204e+00,\n",
            "           1.2984e+00,  5.5509e-01, -4.6531e-01, -5.5186e-01,  1.7703e-01,\n",
            "           1.1940e+00,  7.6860e-01,  1.8164e+00,  4.8306e-01,  3.5048e-01,\n",
            "          -5.7443e-01,  1.2531e+00,  5.8637e-01,  9.1139e-01,  8.9507e-01,\n",
            "          -7.5235e-01,  1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00,\n",
            "          -1.0835e+00,  3.0475e-01,  6.1643e-01, -1.0682e+00,  1.7872e+00,\n",
            "           8.9457e-02, -3.7475e-01, -4.7815e-01, -4.7661e-01, -3.0513e-01,\n",
            "          -2.1659e-01, -7.1611e-01,  2.1370e+00, -6.8247e-01, -1.6026e+00,\n",
            "          -1.3362e-01, -2.6642e-01, -1.7848e-01, -1.7031e+00,  8.7093e-01],\n",
            "         [-7.0518e-01,  1.0969e+00,  8.7733e-01,  4.2924e-01, -7.9058e-01,\n",
            "          -7.1254e-02, -5.1328e-02, -1.0489e+00, -5.1429e-01,  1.3049e-02,\n",
            "           9.6478e-01,  5.3961e-02, -1.5205e+00,  7.1624e-01,  1.0564e-01,\n",
            "           1.4176e+00,  2.7105e-01, -3.4526e-01, -6.4632e-01, -3.1164e-02,\n",
            "           7.4122e-01, -5.4240e-02,  7.6267e-01,  4.9118e-01, -1.2231e-01,\n",
            "           2.6100e-01,  1.5988e+00, -9.2215e-01,  1.3338e+00,  2.1707e-01,\n",
            "          -3.5063e-01,  2.0889e+00,  1.3687e+00,  9.6904e-01, -1.6996e+00,\n",
            "          -1.2433e+00,  1.8634e+00,  3.7374e-01, -1.1434e+00,  7.6303e-01,\n",
            "           9.4115e-02,  2.4528e-01,  3.9492e-03, -1.5011e+00,  3.0252e-02,\n",
            "           2.2015e-01,  7.7363e-01, -1.8984e+00,  3.7340e-02,  5.1175e-02,\n",
            "           1.0818e+00,  6.0073e-01, -8.2280e-01, -1.9710e+00, -7.3877e-01,\n",
            "          -1.4784e+00, -1.7362e+00, -3.7146e-01, -5.8598e-01, -1.4083e-01,\n",
            "           3.9228e-01,  1.1919e+00, -1.5149e-01, -8.4731e-01, -1.8163e-01],\n",
            "         [ 3.5552e-01, -1.6629e-01, -4.7106e-01,  9.3845e-02,  4.5692e-01,\n",
            "           6.8518e-01,  2.7331e-01, -1.1913e+00,  1.2954e+00, -2.2145e-01,\n",
            "          -1.9845e-01,  1.9823e+00,  1.0637e+00, -4.0367e-01, -1.1271e-01,\n",
            "           7.3287e-01,  9.6229e-01,  1.5822e-01,  4.8696e-02,  9.8712e-01,\n",
            "           4.5210e-02,  6.2373e-01,  4.5656e-01, -5.8299e-01,  6.7724e-01,\n",
            "          -7.2625e-01, -1.6427e+00, -2.1066e-01,  6.1826e-02,  1.1956e+00,\n",
            "          -6.0344e-01,  3.6730e-01, -1.1584e+00,  1.2287e+00, -6.9347e-01,\n",
            "           7.0092e-01, -1.1757e+00, -5.4395e-01,  1.0870e+00,  8.4301e-01,\n",
            "          -1.2115e-01,  1.7831e+00,  8.7263e-01,  5.6835e-01,  4.1694e-01,\n",
            "          -1.0416e+00, -7.0460e-01, -6.4834e-01,  1.2946e+00,  2.5547e-01,\n",
            "          -2.7549e-01, -4.9331e-01, -1.3182e+00,  4.5378e-01, -3.1542e-01,\n",
            "          -1.8089e+00, -3.4049e-01, -7.0560e-02,  7.0931e-02,  1.5511e-01,\n",
            "           1.0719e+00, -1.6774e+00,  7.4941e-01, -6.0393e-01, -1.0838e+00],\n",
            "         [-1.0219e+00, -9.3557e-01,  8.4537e-01, -1.3798e+00, -1.1087e+00,\n",
            "           4.0219e-01,  7.6309e-02, -3.1784e-01,  1.3650e+00, -1.3301e+00,\n",
            "          -1.4832e+00, -9.8089e-01, -2.2699e+00, -7.5197e-01, -1.2152e+00,\n",
            "          -3.8503e-01,  5.3976e-01, -7.7177e-01,  6.1751e-01,  2.1793e+00,\n",
            "          -1.1280e+00,  7.0337e-01, -3.3353e-01,  1.2905e+00,  4.9347e-01,\n",
            "           5.8037e-01,  1.2005e+00, -1.5786e+00,  3.9829e-01, -1.0913e+00,\n",
            "          -3.6615e-01,  4.6088e-01,  7.1876e-01, -1.7258e+00, -9.0188e-02,\n",
            "          -1.4782e-02, -8.8946e-01, -7.5669e-01, -9.0381e-01,  1.1814e+00,\n",
            "          -1.0878e+00,  6.6208e-01, -8.3637e-01, -8.0477e-01,  6.1724e-04,\n",
            "           1.3213e+00, -8.6747e-01, -1.5253e-01,  9.0499e-02, -4.0628e-01,\n",
            "           1.1649e-01, -1.2079e+00, -1.6520e+00,  9.0895e-01,  6.2135e-02,\n",
            "          -2.6586e-01, -1.0516e+00, -1.0338e+00,  2.1974e+00,  3.0530e-01,\n",
            "          -2.5632e-01,  2.5969e-01,  1.5510e+00, -5.6964e-01,  1.1173e+00]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "****************************************************************************************************\n",
            "idx  torch.Size([1, 7])\n",
            "tensor([[ 0, 50,  7, 29, 37, 48, 58]])\n",
            "logits 1 torch.Size([1, 7, 65])\n",
            "tensor([[[ 6.2577e-01,  2.5510e-02,  9.5451e-01,  6.4349e-02, -5.0240e-01,\n",
            "          -2.0255e-01, -1.5671e+00, -1.0980e+00,  2.3596e-01, -2.3978e-01,\n",
            "          -9.2111e-01,  1.5433e+00, -3.6756e-01, -7.4827e-01, -1.0058e-01,\n",
            "           7.3073e-01, -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,\n",
            "          -4.7552e-02, -1.0996e+00, -1.7524e+00, -1.0971e+00,  4.4777e-01,\n",
            "          -8.0164e-01,  1.5236e+00,  2.5086e+00,  1.6618e-01,  1.2055e+00,\n",
            "           1.8832e-01, -2.1600e+00,  1.5840e-01,  1.1340e+00, -1.1539e+00,\n",
            "          -2.9840e-01,  1.1490e+00,  1.8123e-01, -9.1996e-02,  1.5828e+00,\n",
            "          -1.2057e+00,  5.7182e-01, -5.9735e-01, -6.9368e-01, -7.2962e-01,\n",
            "          -1.5580e+00, -3.9500e-01,  6.1115e-01, -1.5108e+00,  2.1048e+00,\n",
            "           2.7630e+00, -1.7465e+00,  4.1092e-01, -2.4219e-01,  4.2081e-01,\n",
            "           2.7760e-01,  7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01,\n",
            "          -2.7493e-01,  1.4738e+00,  6.8826e-02,  1.3327e+00, -4.9701e-01],\n",
            "         [ 5.3037e-01, -1.1188e+00,  4.5708e-01, -1.2289e+00,  4.9706e-01,\n",
            "          -7.1988e-01, -8.8912e-01,  6.4300e-01,  7.9472e-01, -3.8905e-01,\n",
            "          -8.7159e-03,  3.4896e+00,  1.0606e+00,  7.9246e-01,  8.7537e-01,\n",
            "          -1.2676e+00,  6.2803e-01,  1.8043e+00,  8.9980e-01,  1.2229e+00,\n",
            "           5.0182e-01,  4.4870e-02, -5.3696e-01, -2.1837e-01, -1.7932e+00,\n",
            "           5.1601e-01, -1.3270e+00, -1.2569e+00,  1.2083e-01, -1.6973e+00,\n",
            "           2.1917e-01, -4.7368e-02, -1.8540e+00,  2.3971e-01,  9.0982e-02,\n",
            "          -6.4437e-01,  7.5578e-01, -7.1515e-01, -8.7237e-01, -2.3707e+00,\n",
            "           2.2814e-01,  2.8507e+00,  1.1757e+00,  5.5517e-02,  1.8837e+00,\n",
            "           8.6488e-01,  7.4757e-01,  2.6562e-01,  6.8140e-01,  4.0766e-01,\n",
            "           1.7070e+00,  2.0166e+00,  4.8474e-01,  6.2260e-01,  1.2592e+00,\n",
            "           1.0245e+00,  2.5456e+00, -3.5597e-01, -7.0021e-01, -4.6080e-01,\n",
            "          -2.9902e-01, -1.4729e+00,  8.6819e-01, -9.1207e-01,  1.5838e-01],\n",
            "         [ 1.1158e-01, -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,\n",
            "           2.1229e-01, -8.9251e-01, -2.7000e-01, -1.3783e+00,  7.5047e-02,\n",
            "           6.5496e-01, -5.0057e-01, -2.1613e-01, -1.7245e-01,  6.8928e-02,\n",
            "          -1.0206e+00, -6.9661e-01,  1.1479e+00, -1.5735e+00,  1.3876e+00,\n",
            "           7.2512e-01, -1.2729e-01,  1.1888e+00,  5.8529e-01, -1.2204e+00,\n",
            "           1.2984e+00,  5.5509e-01, -4.6531e-01, -5.5186e-01,  1.7703e-01,\n",
            "           1.1940e+00,  7.6860e-01,  1.8164e+00,  4.8306e-01,  3.5048e-01,\n",
            "          -5.7443e-01,  1.2531e+00,  5.8637e-01,  9.1139e-01,  8.9507e-01,\n",
            "          -7.5235e-01,  1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00,\n",
            "          -1.0835e+00,  3.0475e-01,  6.1643e-01, -1.0682e+00,  1.7872e+00,\n",
            "           8.9457e-02, -3.7475e-01, -4.7815e-01, -4.7661e-01, -3.0513e-01,\n",
            "          -2.1659e-01, -7.1611e-01,  2.1370e+00, -6.8247e-01, -1.6026e+00,\n",
            "          -1.3362e-01, -2.6642e-01, -1.7848e-01, -1.7031e+00,  8.7093e-01],\n",
            "         [-7.0518e-01,  1.0969e+00,  8.7733e-01,  4.2924e-01, -7.9058e-01,\n",
            "          -7.1254e-02, -5.1328e-02, -1.0489e+00, -5.1429e-01,  1.3049e-02,\n",
            "           9.6478e-01,  5.3961e-02, -1.5205e+00,  7.1624e-01,  1.0564e-01,\n",
            "           1.4176e+00,  2.7105e-01, -3.4526e-01, -6.4632e-01, -3.1164e-02,\n",
            "           7.4122e-01, -5.4240e-02,  7.6267e-01,  4.9118e-01, -1.2231e-01,\n",
            "           2.6100e-01,  1.5988e+00, -9.2215e-01,  1.3338e+00,  2.1707e-01,\n",
            "          -3.5063e-01,  2.0889e+00,  1.3687e+00,  9.6904e-01, -1.6996e+00,\n",
            "          -1.2433e+00,  1.8634e+00,  3.7374e-01, -1.1434e+00,  7.6303e-01,\n",
            "           9.4115e-02,  2.4528e-01,  3.9492e-03, -1.5011e+00,  3.0252e-02,\n",
            "           2.2015e-01,  7.7363e-01, -1.8984e+00,  3.7340e-02,  5.1175e-02,\n",
            "           1.0818e+00,  6.0073e-01, -8.2280e-01, -1.9710e+00, -7.3877e-01,\n",
            "          -1.4784e+00, -1.7362e+00, -3.7146e-01, -5.8598e-01, -1.4083e-01,\n",
            "           3.9228e-01,  1.1919e+00, -1.5149e-01, -8.4731e-01, -1.8163e-01],\n",
            "         [ 3.5552e-01, -1.6629e-01, -4.7106e-01,  9.3845e-02,  4.5692e-01,\n",
            "           6.8518e-01,  2.7331e-01, -1.1913e+00,  1.2954e+00, -2.2145e-01,\n",
            "          -1.9845e-01,  1.9823e+00,  1.0637e+00, -4.0367e-01, -1.1271e-01,\n",
            "           7.3287e-01,  9.6229e-01,  1.5822e-01,  4.8696e-02,  9.8712e-01,\n",
            "           4.5210e-02,  6.2373e-01,  4.5656e-01, -5.8299e-01,  6.7724e-01,\n",
            "          -7.2625e-01, -1.6427e+00, -2.1066e-01,  6.1826e-02,  1.1956e+00,\n",
            "          -6.0344e-01,  3.6730e-01, -1.1584e+00,  1.2287e+00, -6.9347e-01,\n",
            "           7.0092e-01, -1.1757e+00, -5.4395e-01,  1.0870e+00,  8.4301e-01,\n",
            "          -1.2115e-01,  1.7831e+00,  8.7263e-01,  5.6835e-01,  4.1694e-01,\n",
            "          -1.0416e+00, -7.0460e-01, -6.4834e-01,  1.2946e+00,  2.5547e-01,\n",
            "          -2.7549e-01, -4.9331e-01, -1.3182e+00,  4.5378e-01, -3.1542e-01,\n",
            "          -1.8089e+00, -3.4049e-01, -7.0560e-02,  7.0931e-02,  1.5511e-01,\n",
            "           1.0719e+00, -1.6774e+00,  7.4941e-01, -6.0393e-01, -1.0838e+00],\n",
            "         [-1.0219e+00, -9.3557e-01,  8.4537e-01, -1.3798e+00, -1.1087e+00,\n",
            "           4.0219e-01,  7.6309e-02, -3.1784e-01,  1.3650e+00, -1.3301e+00,\n",
            "          -1.4832e+00, -9.8089e-01, -2.2699e+00, -7.5197e-01, -1.2152e+00,\n",
            "          -3.8503e-01,  5.3976e-01, -7.7177e-01,  6.1751e-01,  2.1793e+00,\n",
            "          -1.1280e+00,  7.0337e-01, -3.3353e-01,  1.2905e+00,  4.9347e-01,\n",
            "           5.8037e-01,  1.2005e+00, -1.5786e+00,  3.9829e-01, -1.0913e+00,\n",
            "          -3.6615e-01,  4.6088e-01,  7.1876e-01, -1.7258e+00, -9.0188e-02,\n",
            "          -1.4782e-02, -8.8946e-01, -7.5669e-01, -9.0381e-01,  1.1814e+00,\n",
            "          -1.0878e+00,  6.6208e-01, -8.3637e-01, -8.0477e-01,  6.1724e-04,\n",
            "           1.3213e+00, -8.6747e-01, -1.5253e-01,  9.0499e-02, -4.0628e-01,\n",
            "           1.1649e-01, -1.2079e+00, -1.6520e+00,  9.0895e-01,  6.2135e-02,\n",
            "          -2.6586e-01, -1.0516e+00, -1.0338e+00,  2.1974e+00,  3.0530e-01,\n",
            "          -2.5632e-01,  2.5969e-01,  1.5510e+00, -5.6964e-01,  1.1173e+00],\n",
            "         [ 1.4787e-01, -4.3331e-01,  5.2033e-01,  3.9772e-01, -2.6225e-01,\n",
            "           1.7675e+00, -1.2460e+00,  1.4583e-01, -5.6994e-01, -1.3561e+00,\n",
            "           1.2897e+00,  8.6078e-01, -8.4937e-01, -3.7719e-01, -1.7326e-01,\n",
            "          -4.7029e-01, -6.0004e-01, -1.3636e+00, -1.3741e-01, -1.4640e+00,\n",
            "           4.9037e-01,  1.8202e+00,  1.8017e+00,  6.0141e-01, -2.5448e+00,\n",
            "          -4.8652e-01, -4.3733e-01, -5.4988e-01, -4.3360e-01, -5.6633e-01,\n",
            "          -1.1238e-01, -3.0501e-01,  1.1426e+00,  6.6372e-01,  7.4657e-01,\n",
            "          -2.1834e-01,  1.1955e-01, -5.2703e-01,  6.0654e-01,  5.8816e-01,\n",
            "          -5.4526e-01,  7.6541e-01, -1.1892e-01,  5.0230e-01,  6.1099e-01,\n",
            "           8.2150e-01, -2.8304e-02, -1.2243e+00, -1.7192e+00,  1.4801e+00,\n",
            "          -9.8939e-01,  8.1746e-02, -1.0265e+00,  6.4049e-01,  2.0734e+00,\n",
            "          -2.9941e-01,  4.7293e-02, -9.6258e-01, -4.3673e-01,  1.3442e+00,\n",
            "           1.6607e+00,  1.4093e+00,  3.3019e-01,  1.5454e+00,  1.3778e+00]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "****************************************************************************************************\n",
            "idx  torch.Size([1, 8])\n",
            "tensor([[ 0, 50,  7, 29, 37, 48, 58,  5]])\n",
            "logits 1 torch.Size([1, 8, 65])\n",
            "tensor([[[ 6.2577e-01,  2.5510e-02,  9.5451e-01,  6.4349e-02, -5.0240e-01,\n",
            "          -2.0255e-01, -1.5671e+00, -1.0980e+00,  2.3596e-01, -2.3978e-01,\n",
            "          -9.2111e-01,  1.5433e+00, -3.6756e-01, -7.4827e-01, -1.0058e-01,\n",
            "           7.3073e-01, -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,\n",
            "          -4.7552e-02, -1.0996e+00, -1.7524e+00, -1.0971e+00,  4.4777e-01,\n",
            "          -8.0164e-01,  1.5236e+00,  2.5086e+00,  1.6618e-01,  1.2055e+00,\n",
            "           1.8832e-01, -2.1600e+00,  1.5840e-01,  1.1340e+00, -1.1539e+00,\n",
            "          -2.9840e-01,  1.1490e+00,  1.8123e-01, -9.1996e-02,  1.5828e+00,\n",
            "          -1.2057e+00,  5.7182e-01, -5.9735e-01, -6.9368e-01, -7.2962e-01,\n",
            "          -1.5580e+00, -3.9500e-01,  6.1115e-01, -1.5108e+00,  2.1048e+00,\n",
            "           2.7630e+00, -1.7465e+00,  4.1092e-01, -2.4219e-01,  4.2081e-01,\n",
            "           2.7760e-01,  7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01,\n",
            "          -2.7493e-01,  1.4738e+00,  6.8826e-02,  1.3327e+00, -4.9701e-01],\n",
            "         [ 5.3037e-01, -1.1188e+00,  4.5708e-01, -1.2289e+00,  4.9706e-01,\n",
            "          -7.1988e-01, -8.8912e-01,  6.4300e-01,  7.9472e-01, -3.8905e-01,\n",
            "          -8.7159e-03,  3.4896e+00,  1.0606e+00,  7.9246e-01,  8.7537e-01,\n",
            "          -1.2676e+00,  6.2803e-01,  1.8043e+00,  8.9980e-01,  1.2229e+00,\n",
            "           5.0182e-01,  4.4870e-02, -5.3696e-01, -2.1837e-01, -1.7932e+00,\n",
            "           5.1601e-01, -1.3270e+00, -1.2569e+00,  1.2083e-01, -1.6973e+00,\n",
            "           2.1917e-01, -4.7368e-02, -1.8540e+00,  2.3971e-01,  9.0982e-02,\n",
            "          -6.4437e-01,  7.5578e-01, -7.1515e-01, -8.7237e-01, -2.3707e+00,\n",
            "           2.2814e-01,  2.8507e+00,  1.1757e+00,  5.5517e-02,  1.8837e+00,\n",
            "           8.6488e-01,  7.4757e-01,  2.6562e-01,  6.8140e-01,  4.0766e-01,\n",
            "           1.7070e+00,  2.0166e+00,  4.8474e-01,  6.2260e-01,  1.2592e+00,\n",
            "           1.0245e+00,  2.5456e+00, -3.5597e-01, -7.0021e-01, -4.6080e-01,\n",
            "          -2.9902e-01, -1.4729e+00,  8.6819e-01, -9.1207e-01,  1.5838e-01],\n",
            "         [ 1.1158e-01, -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,\n",
            "           2.1229e-01, -8.9251e-01, -2.7000e-01, -1.3783e+00,  7.5047e-02,\n",
            "           6.5496e-01, -5.0057e-01, -2.1613e-01, -1.7245e-01,  6.8928e-02,\n",
            "          -1.0206e+00, -6.9661e-01,  1.1479e+00, -1.5735e+00,  1.3876e+00,\n",
            "           7.2512e-01, -1.2729e-01,  1.1888e+00,  5.8529e-01, -1.2204e+00,\n",
            "           1.2984e+00,  5.5509e-01, -4.6531e-01, -5.5186e-01,  1.7703e-01,\n",
            "           1.1940e+00,  7.6860e-01,  1.8164e+00,  4.8306e-01,  3.5048e-01,\n",
            "          -5.7443e-01,  1.2531e+00,  5.8637e-01,  9.1139e-01,  8.9507e-01,\n",
            "          -7.5235e-01,  1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00,\n",
            "          -1.0835e+00,  3.0475e-01,  6.1643e-01, -1.0682e+00,  1.7872e+00,\n",
            "           8.9457e-02, -3.7475e-01, -4.7815e-01, -4.7661e-01, -3.0513e-01,\n",
            "          -2.1659e-01, -7.1611e-01,  2.1370e+00, -6.8247e-01, -1.6026e+00,\n",
            "          -1.3362e-01, -2.6642e-01, -1.7848e-01, -1.7031e+00,  8.7093e-01],\n",
            "         [-7.0518e-01,  1.0969e+00,  8.7733e-01,  4.2924e-01, -7.9058e-01,\n",
            "          -7.1254e-02, -5.1328e-02, -1.0489e+00, -5.1429e-01,  1.3049e-02,\n",
            "           9.6478e-01,  5.3961e-02, -1.5205e+00,  7.1624e-01,  1.0564e-01,\n",
            "           1.4176e+00,  2.7105e-01, -3.4526e-01, -6.4632e-01, -3.1164e-02,\n",
            "           7.4122e-01, -5.4240e-02,  7.6267e-01,  4.9118e-01, -1.2231e-01,\n",
            "           2.6100e-01,  1.5988e+00, -9.2215e-01,  1.3338e+00,  2.1707e-01,\n",
            "          -3.5063e-01,  2.0889e+00,  1.3687e+00,  9.6904e-01, -1.6996e+00,\n",
            "          -1.2433e+00,  1.8634e+00,  3.7374e-01, -1.1434e+00,  7.6303e-01,\n",
            "           9.4115e-02,  2.4528e-01,  3.9492e-03, -1.5011e+00,  3.0252e-02,\n",
            "           2.2015e-01,  7.7363e-01, -1.8984e+00,  3.7340e-02,  5.1175e-02,\n",
            "           1.0818e+00,  6.0073e-01, -8.2280e-01, -1.9710e+00, -7.3877e-01,\n",
            "          -1.4784e+00, -1.7362e+00, -3.7146e-01, -5.8598e-01, -1.4083e-01,\n",
            "           3.9228e-01,  1.1919e+00, -1.5149e-01, -8.4731e-01, -1.8163e-01],\n",
            "         [ 3.5552e-01, -1.6629e-01, -4.7106e-01,  9.3845e-02,  4.5692e-01,\n",
            "           6.8518e-01,  2.7331e-01, -1.1913e+00,  1.2954e+00, -2.2145e-01,\n",
            "          -1.9845e-01,  1.9823e+00,  1.0637e+00, -4.0367e-01, -1.1271e-01,\n",
            "           7.3287e-01,  9.6229e-01,  1.5822e-01,  4.8696e-02,  9.8712e-01,\n",
            "           4.5210e-02,  6.2373e-01,  4.5656e-01, -5.8299e-01,  6.7724e-01,\n",
            "          -7.2625e-01, -1.6427e+00, -2.1066e-01,  6.1826e-02,  1.1956e+00,\n",
            "          -6.0344e-01,  3.6730e-01, -1.1584e+00,  1.2287e+00, -6.9347e-01,\n",
            "           7.0092e-01, -1.1757e+00, -5.4395e-01,  1.0870e+00,  8.4301e-01,\n",
            "          -1.2115e-01,  1.7831e+00,  8.7263e-01,  5.6835e-01,  4.1694e-01,\n",
            "          -1.0416e+00, -7.0460e-01, -6.4834e-01,  1.2946e+00,  2.5547e-01,\n",
            "          -2.7549e-01, -4.9331e-01, -1.3182e+00,  4.5378e-01, -3.1542e-01,\n",
            "          -1.8089e+00, -3.4049e-01, -7.0560e-02,  7.0931e-02,  1.5511e-01,\n",
            "           1.0719e+00, -1.6774e+00,  7.4941e-01, -6.0393e-01, -1.0838e+00],\n",
            "         [-1.0219e+00, -9.3557e-01,  8.4537e-01, -1.3798e+00, -1.1087e+00,\n",
            "           4.0219e-01,  7.6309e-02, -3.1784e-01,  1.3650e+00, -1.3301e+00,\n",
            "          -1.4832e+00, -9.8089e-01, -2.2699e+00, -7.5197e-01, -1.2152e+00,\n",
            "          -3.8503e-01,  5.3976e-01, -7.7177e-01,  6.1751e-01,  2.1793e+00,\n",
            "          -1.1280e+00,  7.0337e-01, -3.3353e-01,  1.2905e+00,  4.9347e-01,\n",
            "           5.8037e-01,  1.2005e+00, -1.5786e+00,  3.9829e-01, -1.0913e+00,\n",
            "          -3.6615e-01,  4.6088e-01,  7.1876e-01, -1.7258e+00, -9.0188e-02,\n",
            "          -1.4782e-02, -8.8946e-01, -7.5669e-01, -9.0381e-01,  1.1814e+00,\n",
            "          -1.0878e+00,  6.6208e-01, -8.3637e-01, -8.0477e-01,  6.1724e-04,\n",
            "           1.3213e+00, -8.6747e-01, -1.5253e-01,  9.0499e-02, -4.0628e-01,\n",
            "           1.1649e-01, -1.2079e+00, -1.6520e+00,  9.0895e-01,  6.2135e-02,\n",
            "          -2.6586e-01, -1.0516e+00, -1.0338e+00,  2.1974e+00,  3.0530e-01,\n",
            "          -2.5632e-01,  2.5969e-01,  1.5510e+00, -5.6964e-01,  1.1173e+00],\n",
            "         [ 1.4787e-01, -4.3331e-01,  5.2033e-01,  3.9772e-01, -2.6225e-01,\n",
            "           1.7675e+00, -1.2460e+00,  1.4583e-01, -5.6994e-01, -1.3561e+00,\n",
            "           1.2897e+00,  8.6078e-01, -8.4937e-01, -3.7719e-01, -1.7326e-01,\n",
            "          -4.7029e-01, -6.0004e-01, -1.3636e+00, -1.3741e-01, -1.4640e+00,\n",
            "           4.9037e-01,  1.8202e+00,  1.8017e+00,  6.0141e-01, -2.5448e+00,\n",
            "          -4.8652e-01, -4.3733e-01, -5.4988e-01, -4.3360e-01, -5.6633e-01,\n",
            "          -1.1238e-01, -3.0501e-01,  1.1426e+00,  6.6372e-01,  7.4657e-01,\n",
            "          -2.1834e-01,  1.1955e-01, -5.2703e-01,  6.0654e-01,  5.8816e-01,\n",
            "          -5.4526e-01,  7.6541e-01, -1.1892e-01,  5.0230e-01,  6.1099e-01,\n",
            "           8.2150e-01, -2.8304e-02, -1.2243e+00, -1.7192e+00,  1.4801e+00,\n",
            "          -9.8939e-01,  8.1746e-02, -1.0265e+00,  6.4049e-01,  2.0734e+00,\n",
            "          -2.9941e-01,  4.7293e-02, -9.6258e-01, -4.3673e-01,  1.3442e+00,\n",
            "           1.6607e+00,  1.4093e+00,  3.3019e-01,  1.5454e+00,  1.3778e+00],\n",
            "         [ 1.0329e+00, -5.5563e-01, -1.3479e+00,  1.4811e-02, -4.4722e-02,\n",
            "          -5.3666e-01, -5.2229e-01,  2.1068e+00, -5.3867e-01,  2.1751e+00,\n",
            "          -1.7514e+00, -1.2576e+00,  6.0938e-01, -2.0551e+00, -4.4305e-01,\n",
            "           1.4874e+00,  1.0655e-01, -2.9780e-01,  5.4480e-01,  2.6449e-01,\n",
            "          -6.4450e-01,  1.0834e+00, -7.9946e-01,  4.3056e-01, -1.2575e+00,\n",
            "          -1.1985e-01,  1.7985e+00, -2.0660e+00,  1.0575e+00, -1.0572e+00,\n",
            "           9.9107e-01,  2.2576e+00,  3.0766e-01,  1.5197e-01, -8.3968e-01,\n",
            "           1.6685e+00,  5.9758e-01, -1.8736e+00,  1.2910e+00,  1.1071e+00,\n",
            "          -3.6937e-01,  1.9295e-01, -3.4203e-01, -8.4607e-01,  5.0153e-01,\n",
            "          -9.6565e-01, -7.2546e-01,  8.7959e-01,  2.1676e-01, -2.8932e+00,\n",
            "          -1.9850e+00,  1.4424e+00,  4.3408e-01, -4.2923e-01,  3.6661e-01,\n",
            "           6.7441e-01,  1.0747e+00, -6.8145e-01,  2.8847e-01,  1.0477e+00,\n",
            "           1.6187e+00,  4.1604e-01,  3.3619e-01, -1.2428e+00, -5.1481e-02]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "****************************************************************************************************\n",
            "idx  torch.Size([1, 9])\n",
            "tensor([[ 0, 50,  7, 29, 37, 48, 58,  5, 15]])\n",
            "logits 1 torch.Size([1, 9, 65])\n",
            "tensor([[[ 6.2577e-01,  2.5510e-02,  9.5451e-01,  6.4349e-02, -5.0240e-01,\n",
            "          -2.0255e-01, -1.5671e+00, -1.0980e+00,  2.3596e-01, -2.3978e-01,\n",
            "          -9.2111e-01,  1.5433e+00, -3.6756e-01, -7.4827e-01, -1.0058e-01,\n",
            "           7.3073e-01, -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,\n",
            "          -4.7552e-02, -1.0996e+00, -1.7524e+00, -1.0971e+00,  4.4777e-01,\n",
            "          -8.0164e-01,  1.5236e+00,  2.5086e+00,  1.6618e-01,  1.2055e+00,\n",
            "           1.8832e-01, -2.1600e+00,  1.5840e-01,  1.1340e+00, -1.1539e+00,\n",
            "          -2.9840e-01,  1.1490e+00,  1.8123e-01, -9.1996e-02,  1.5828e+00,\n",
            "          -1.2057e+00,  5.7182e-01, -5.9735e-01, -6.9368e-01, -7.2962e-01,\n",
            "          -1.5580e+00, -3.9500e-01,  6.1115e-01, -1.5108e+00,  2.1048e+00,\n",
            "           2.7630e+00, -1.7465e+00,  4.1092e-01, -2.4219e-01,  4.2081e-01,\n",
            "           2.7760e-01,  7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01,\n",
            "          -2.7493e-01,  1.4738e+00,  6.8826e-02,  1.3327e+00, -4.9701e-01],\n",
            "         [ 5.3037e-01, -1.1188e+00,  4.5708e-01, -1.2289e+00,  4.9706e-01,\n",
            "          -7.1988e-01, -8.8912e-01,  6.4300e-01,  7.9472e-01, -3.8905e-01,\n",
            "          -8.7159e-03,  3.4896e+00,  1.0606e+00,  7.9246e-01,  8.7537e-01,\n",
            "          -1.2676e+00,  6.2803e-01,  1.8043e+00,  8.9980e-01,  1.2229e+00,\n",
            "           5.0182e-01,  4.4870e-02, -5.3696e-01, -2.1837e-01, -1.7932e+00,\n",
            "           5.1601e-01, -1.3270e+00, -1.2569e+00,  1.2083e-01, -1.6973e+00,\n",
            "           2.1917e-01, -4.7368e-02, -1.8540e+00,  2.3971e-01,  9.0982e-02,\n",
            "          -6.4437e-01,  7.5578e-01, -7.1515e-01, -8.7237e-01, -2.3707e+00,\n",
            "           2.2814e-01,  2.8507e+00,  1.1757e+00,  5.5517e-02,  1.8837e+00,\n",
            "           8.6488e-01,  7.4757e-01,  2.6562e-01,  6.8140e-01,  4.0766e-01,\n",
            "           1.7070e+00,  2.0166e+00,  4.8474e-01,  6.2260e-01,  1.2592e+00,\n",
            "           1.0245e+00,  2.5456e+00, -3.5597e-01, -7.0021e-01, -4.6080e-01,\n",
            "          -2.9902e-01, -1.4729e+00,  8.6819e-01, -9.1207e-01,  1.5838e-01],\n",
            "         [ 1.1158e-01, -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,\n",
            "           2.1229e-01, -8.9251e-01, -2.7000e-01, -1.3783e+00,  7.5047e-02,\n",
            "           6.5496e-01, -5.0057e-01, -2.1613e-01, -1.7245e-01,  6.8928e-02,\n",
            "          -1.0206e+00, -6.9661e-01,  1.1479e+00, -1.5735e+00,  1.3876e+00,\n",
            "           7.2512e-01, -1.2729e-01,  1.1888e+00,  5.8529e-01, -1.2204e+00,\n",
            "           1.2984e+00,  5.5509e-01, -4.6531e-01, -5.5186e-01,  1.7703e-01,\n",
            "           1.1940e+00,  7.6860e-01,  1.8164e+00,  4.8306e-01,  3.5048e-01,\n",
            "          -5.7443e-01,  1.2531e+00,  5.8637e-01,  9.1139e-01,  8.9507e-01,\n",
            "          -7.5235e-01,  1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00,\n",
            "          -1.0835e+00,  3.0475e-01,  6.1643e-01, -1.0682e+00,  1.7872e+00,\n",
            "           8.9457e-02, -3.7475e-01, -4.7815e-01, -4.7661e-01, -3.0513e-01,\n",
            "          -2.1659e-01, -7.1611e-01,  2.1370e+00, -6.8247e-01, -1.6026e+00,\n",
            "          -1.3362e-01, -2.6642e-01, -1.7848e-01, -1.7031e+00,  8.7093e-01],\n",
            "         [-7.0518e-01,  1.0969e+00,  8.7733e-01,  4.2924e-01, -7.9058e-01,\n",
            "          -7.1254e-02, -5.1328e-02, -1.0489e+00, -5.1429e-01,  1.3049e-02,\n",
            "           9.6478e-01,  5.3961e-02, -1.5205e+00,  7.1624e-01,  1.0564e-01,\n",
            "           1.4176e+00,  2.7105e-01, -3.4526e-01, -6.4632e-01, -3.1164e-02,\n",
            "           7.4122e-01, -5.4240e-02,  7.6267e-01,  4.9118e-01, -1.2231e-01,\n",
            "           2.6100e-01,  1.5988e+00, -9.2215e-01,  1.3338e+00,  2.1707e-01,\n",
            "          -3.5063e-01,  2.0889e+00,  1.3687e+00,  9.6904e-01, -1.6996e+00,\n",
            "          -1.2433e+00,  1.8634e+00,  3.7374e-01, -1.1434e+00,  7.6303e-01,\n",
            "           9.4115e-02,  2.4528e-01,  3.9492e-03, -1.5011e+00,  3.0252e-02,\n",
            "           2.2015e-01,  7.7363e-01, -1.8984e+00,  3.7340e-02,  5.1175e-02,\n",
            "           1.0818e+00,  6.0073e-01, -8.2280e-01, -1.9710e+00, -7.3877e-01,\n",
            "          -1.4784e+00, -1.7362e+00, -3.7146e-01, -5.8598e-01, -1.4083e-01,\n",
            "           3.9228e-01,  1.1919e+00, -1.5149e-01, -8.4731e-01, -1.8163e-01],\n",
            "         [ 3.5552e-01, -1.6629e-01, -4.7106e-01,  9.3845e-02,  4.5692e-01,\n",
            "           6.8518e-01,  2.7331e-01, -1.1913e+00,  1.2954e+00, -2.2145e-01,\n",
            "          -1.9845e-01,  1.9823e+00,  1.0637e+00, -4.0367e-01, -1.1271e-01,\n",
            "           7.3287e-01,  9.6229e-01,  1.5822e-01,  4.8696e-02,  9.8712e-01,\n",
            "           4.5210e-02,  6.2373e-01,  4.5656e-01, -5.8299e-01,  6.7724e-01,\n",
            "          -7.2625e-01, -1.6427e+00, -2.1066e-01,  6.1826e-02,  1.1956e+00,\n",
            "          -6.0344e-01,  3.6730e-01, -1.1584e+00,  1.2287e+00, -6.9347e-01,\n",
            "           7.0092e-01, -1.1757e+00, -5.4395e-01,  1.0870e+00,  8.4301e-01,\n",
            "          -1.2115e-01,  1.7831e+00,  8.7263e-01,  5.6835e-01,  4.1694e-01,\n",
            "          -1.0416e+00, -7.0460e-01, -6.4834e-01,  1.2946e+00,  2.5547e-01,\n",
            "          -2.7549e-01, -4.9331e-01, -1.3182e+00,  4.5378e-01, -3.1542e-01,\n",
            "          -1.8089e+00, -3.4049e-01, -7.0560e-02,  7.0931e-02,  1.5511e-01,\n",
            "           1.0719e+00, -1.6774e+00,  7.4941e-01, -6.0393e-01, -1.0838e+00],\n",
            "         [-1.0219e+00, -9.3557e-01,  8.4537e-01, -1.3798e+00, -1.1087e+00,\n",
            "           4.0219e-01,  7.6309e-02, -3.1784e-01,  1.3650e+00, -1.3301e+00,\n",
            "          -1.4832e+00, -9.8089e-01, -2.2699e+00, -7.5197e-01, -1.2152e+00,\n",
            "          -3.8503e-01,  5.3976e-01, -7.7177e-01,  6.1751e-01,  2.1793e+00,\n",
            "          -1.1280e+00,  7.0337e-01, -3.3353e-01,  1.2905e+00,  4.9347e-01,\n",
            "           5.8037e-01,  1.2005e+00, -1.5786e+00,  3.9829e-01, -1.0913e+00,\n",
            "          -3.6615e-01,  4.6088e-01,  7.1876e-01, -1.7258e+00, -9.0188e-02,\n",
            "          -1.4782e-02, -8.8946e-01, -7.5669e-01, -9.0381e-01,  1.1814e+00,\n",
            "          -1.0878e+00,  6.6208e-01, -8.3637e-01, -8.0477e-01,  6.1724e-04,\n",
            "           1.3213e+00, -8.6747e-01, -1.5253e-01,  9.0499e-02, -4.0628e-01,\n",
            "           1.1649e-01, -1.2079e+00, -1.6520e+00,  9.0895e-01,  6.2135e-02,\n",
            "          -2.6586e-01, -1.0516e+00, -1.0338e+00,  2.1974e+00,  3.0530e-01,\n",
            "          -2.5632e-01,  2.5969e-01,  1.5510e+00, -5.6964e-01,  1.1173e+00],\n",
            "         [ 1.4787e-01, -4.3331e-01,  5.2033e-01,  3.9772e-01, -2.6225e-01,\n",
            "           1.7675e+00, -1.2460e+00,  1.4583e-01, -5.6994e-01, -1.3561e+00,\n",
            "           1.2897e+00,  8.6078e-01, -8.4937e-01, -3.7719e-01, -1.7326e-01,\n",
            "          -4.7029e-01, -6.0004e-01, -1.3636e+00, -1.3741e-01, -1.4640e+00,\n",
            "           4.9037e-01,  1.8202e+00,  1.8017e+00,  6.0141e-01, -2.5448e+00,\n",
            "          -4.8652e-01, -4.3733e-01, -5.4988e-01, -4.3360e-01, -5.6633e-01,\n",
            "          -1.1238e-01, -3.0501e-01,  1.1426e+00,  6.6372e-01,  7.4657e-01,\n",
            "          -2.1834e-01,  1.1955e-01, -5.2703e-01,  6.0654e-01,  5.8816e-01,\n",
            "          -5.4526e-01,  7.6541e-01, -1.1892e-01,  5.0230e-01,  6.1099e-01,\n",
            "           8.2150e-01, -2.8304e-02, -1.2243e+00, -1.7192e+00,  1.4801e+00,\n",
            "          -9.8939e-01,  8.1746e-02, -1.0265e+00,  6.4049e-01,  2.0734e+00,\n",
            "          -2.9941e-01,  4.7293e-02, -9.6258e-01, -4.3673e-01,  1.3442e+00,\n",
            "           1.6607e+00,  1.4093e+00,  3.3019e-01,  1.5454e+00,  1.3778e+00],\n",
            "         [ 1.0329e+00, -5.5563e-01, -1.3479e+00,  1.4811e-02, -4.4722e-02,\n",
            "          -5.3666e-01, -5.2229e-01,  2.1068e+00, -5.3867e-01,  2.1751e+00,\n",
            "          -1.7514e+00, -1.2576e+00,  6.0938e-01, -2.0551e+00, -4.4305e-01,\n",
            "           1.4874e+00,  1.0655e-01, -2.9780e-01,  5.4480e-01,  2.6449e-01,\n",
            "          -6.4450e-01,  1.0834e+00, -7.9946e-01,  4.3056e-01, -1.2575e+00,\n",
            "          -1.1985e-01,  1.7985e+00, -2.0660e+00,  1.0575e+00, -1.0572e+00,\n",
            "           9.9107e-01,  2.2576e+00,  3.0766e-01,  1.5197e-01, -8.3968e-01,\n",
            "           1.6685e+00,  5.9758e-01, -1.8736e+00,  1.2910e+00,  1.1071e+00,\n",
            "          -3.6937e-01,  1.9295e-01, -3.4203e-01, -8.4607e-01,  5.0153e-01,\n",
            "          -9.6565e-01, -7.2546e-01,  8.7959e-01,  2.1676e-01, -2.8932e+00,\n",
            "          -1.9850e+00,  1.4424e+00,  4.3408e-01, -4.2923e-01,  3.6661e-01,\n",
            "           6.7441e-01,  1.0747e+00, -6.8145e-01,  2.8847e-01,  1.0477e+00,\n",
            "           1.6187e+00,  4.1604e-01,  3.3619e-01, -1.2428e+00, -5.1481e-02],\n",
            "         [-2.0688e-01,  4.7600e-01, -8.8707e-01,  1.3709e+00, -1.9473e+00,\n",
            "          -1.6613e+00,  1.2640e+00, -4.2307e-01, -1.8335e-01,  2.1775e-01,\n",
            "          -3.2975e-01, -1.9170e-02,  9.2251e-01,  1.5641e+00, -1.1374e+00,\n",
            "           1.2083e+00,  2.7124e-01,  1.2369e-01,  1.8620e+00,  1.7080e+00,\n",
            "          -1.6045e+00,  2.9287e-01, -1.2572e-01,  4.7992e-01, -3.9235e-01,\n",
            "          -1.4055e+00, -6.6859e-01, -4.8307e-01, -2.2978e-01, -1.6844e-01,\n",
            "           1.7177e+00,  1.7665e-01,  1.8060e-01, -6.9085e-01, -3.0646e-01,\n",
            "          -1.1809e+00,  8.1750e-01, -6.4961e-01,  4.6269e-02,  1.4899e+00,\n",
            "           2.1607e+00, -3.6567e-01,  8.2824e-01, -4.8264e-01,  1.8330e+00,\n",
            "           1.7229e+00,  2.5107e-01,  8.7205e-02,  5.1863e-01,  6.9787e-03,\n",
            "           1.4843e-01, -5.4027e-01, -1.9312e+00,  1.8214e+00, -1.6471e+00,\n",
            "           2.8126e-01, -7.8558e-01, -5.3489e-01,  5.8117e-01, -5.3562e-01,\n",
            "          -1.7944e+00, -2.4687e-01, -7.0644e-01,  1.3517e+00, -3.3502e-01]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "****************************************************************************************************\n",
            "idx  torch.Size([1, 10])\n",
            "tensor([[ 0, 50,  7, 29, 37, 48, 58,  5, 15, 24]])\n",
            "logits 1 torch.Size([1, 10, 65])\n",
            "tensor([[[ 6.2577e-01,  2.5510e-02,  9.5451e-01,  6.4349e-02, -5.0240e-01,\n",
            "          -2.0255e-01, -1.5671e+00, -1.0980e+00,  2.3596e-01, -2.3978e-01,\n",
            "          -9.2111e-01,  1.5433e+00, -3.6756e-01, -7.4827e-01, -1.0058e-01,\n",
            "           7.3073e-01, -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,\n",
            "          -4.7552e-02, -1.0996e+00, -1.7524e+00, -1.0971e+00,  4.4777e-01,\n",
            "          -8.0164e-01,  1.5236e+00,  2.5086e+00,  1.6618e-01,  1.2055e+00,\n",
            "           1.8832e-01, -2.1600e+00,  1.5840e-01,  1.1340e+00, -1.1539e+00,\n",
            "          -2.9840e-01,  1.1490e+00,  1.8123e-01, -9.1996e-02,  1.5828e+00,\n",
            "          -1.2057e+00,  5.7182e-01, -5.9735e-01, -6.9368e-01, -7.2962e-01,\n",
            "          -1.5580e+00, -3.9500e-01,  6.1115e-01, -1.5108e+00,  2.1048e+00,\n",
            "           2.7630e+00, -1.7465e+00,  4.1092e-01, -2.4219e-01,  4.2081e-01,\n",
            "           2.7760e-01,  7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01,\n",
            "          -2.7493e-01,  1.4738e+00,  6.8826e-02,  1.3327e+00, -4.9701e-01],\n",
            "         [ 5.3037e-01, -1.1188e+00,  4.5708e-01, -1.2289e+00,  4.9706e-01,\n",
            "          -7.1988e-01, -8.8912e-01,  6.4300e-01,  7.9472e-01, -3.8905e-01,\n",
            "          -8.7159e-03,  3.4896e+00,  1.0606e+00,  7.9246e-01,  8.7537e-01,\n",
            "          -1.2676e+00,  6.2803e-01,  1.8043e+00,  8.9980e-01,  1.2229e+00,\n",
            "           5.0182e-01,  4.4870e-02, -5.3696e-01, -2.1837e-01, -1.7932e+00,\n",
            "           5.1601e-01, -1.3270e+00, -1.2569e+00,  1.2083e-01, -1.6973e+00,\n",
            "           2.1917e-01, -4.7368e-02, -1.8540e+00,  2.3971e-01,  9.0982e-02,\n",
            "          -6.4437e-01,  7.5578e-01, -7.1515e-01, -8.7237e-01, -2.3707e+00,\n",
            "           2.2814e-01,  2.8507e+00,  1.1757e+00,  5.5517e-02,  1.8837e+00,\n",
            "           8.6488e-01,  7.4757e-01,  2.6562e-01,  6.8140e-01,  4.0766e-01,\n",
            "           1.7070e+00,  2.0166e+00,  4.8474e-01,  6.2260e-01,  1.2592e+00,\n",
            "           1.0245e+00,  2.5456e+00, -3.5597e-01, -7.0021e-01, -4.6080e-01,\n",
            "          -2.9902e-01, -1.4729e+00,  8.6819e-01, -9.1207e-01,  1.5838e-01],\n",
            "         [ 1.1158e-01, -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,\n",
            "           2.1229e-01, -8.9251e-01, -2.7000e-01, -1.3783e+00,  7.5047e-02,\n",
            "           6.5496e-01, -5.0057e-01, -2.1613e-01, -1.7245e-01,  6.8928e-02,\n",
            "          -1.0206e+00, -6.9661e-01,  1.1479e+00, -1.5735e+00,  1.3876e+00,\n",
            "           7.2512e-01, -1.2729e-01,  1.1888e+00,  5.8529e-01, -1.2204e+00,\n",
            "           1.2984e+00,  5.5509e-01, -4.6531e-01, -5.5186e-01,  1.7703e-01,\n",
            "           1.1940e+00,  7.6860e-01,  1.8164e+00,  4.8306e-01,  3.5048e-01,\n",
            "          -5.7443e-01,  1.2531e+00,  5.8637e-01,  9.1139e-01,  8.9507e-01,\n",
            "          -7.5235e-01,  1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00,\n",
            "          -1.0835e+00,  3.0475e-01,  6.1643e-01, -1.0682e+00,  1.7872e+00,\n",
            "           8.9457e-02, -3.7475e-01, -4.7815e-01, -4.7661e-01, -3.0513e-01,\n",
            "          -2.1659e-01, -7.1611e-01,  2.1370e+00, -6.8247e-01, -1.6026e+00,\n",
            "          -1.3362e-01, -2.6642e-01, -1.7848e-01, -1.7031e+00,  8.7093e-01],\n",
            "         [-7.0518e-01,  1.0969e+00,  8.7733e-01,  4.2924e-01, -7.9058e-01,\n",
            "          -7.1254e-02, -5.1328e-02, -1.0489e+00, -5.1429e-01,  1.3049e-02,\n",
            "           9.6478e-01,  5.3961e-02, -1.5205e+00,  7.1624e-01,  1.0564e-01,\n",
            "           1.4176e+00,  2.7105e-01, -3.4526e-01, -6.4632e-01, -3.1164e-02,\n",
            "           7.4122e-01, -5.4240e-02,  7.6267e-01,  4.9118e-01, -1.2231e-01,\n",
            "           2.6100e-01,  1.5988e+00, -9.2215e-01,  1.3338e+00,  2.1707e-01,\n",
            "          -3.5063e-01,  2.0889e+00,  1.3687e+00,  9.6904e-01, -1.6996e+00,\n",
            "          -1.2433e+00,  1.8634e+00,  3.7374e-01, -1.1434e+00,  7.6303e-01,\n",
            "           9.4115e-02,  2.4528e-01,  3.9492e-03, -1.5011e+00,  3.0252e-02,\n",
            "           2.2015e-01,  7.7363e-01, -1.8984e+00,  3.7340e-02,  5.1175e-02,\n",
            "           1.0818e+00,  6.0073e-01, -8.2280e-01, -1.9710e+00, -7.3877e-01,\n",
            "          -1.4784e+00, -1.7362e+00, -3.7146e-01, -5.8598e-01, -1.4083e-01,\n",
            "           3.9228e-01,  1.1919e+00, -1.5149e-01, -8.4731e-01, -1.8163e-01],\n",
            "         [ 3.5552e-01, -1.6629e-01, -4.7106e-01,  9.3845e-02,  4.5692e-01,\n",
            "           6.8518e-01,  2.7331e-01, -1.1913e+00,  1.2954e+00, -2.2145e-01,\n",
            "          -1.9845e-01,  1.9823e+00,  1.0637e+00, -4.0367e-01, -1.1271e-01,\n",
            "           7.3287e-01,  9.6229e-01,  1.5822e-01,  4.8696e-02,  9.8712e-01,\n",
            "           4.5210e-02,  6.2373e-01,  4.5656e-01, -5.8299e-01,  6.7724e-01,\n",
            "          -7.2625e-01, -1.6427e+00, -2.1066e-01,  6.1826e-02,  1.1956e+00,\n",
            "          -6.0344e-01,  3.6730e-01, -1.1584e+00,  1.2287e+00, -6.9347e-01,\n",
            "           7.0092e-01, -1.1757e+00, -5.4395e-01,  1.0870e+00,  8.4301e-01,\n",
            "          -1.2115e-01,  1.7831e+00,  8.7263e-01,  5.6835e-01,  4.1694e-01,\n",
            "          -1.0416e+00, -7.0460e-01, -6.4834e-01,  1.2946e+00,  2.5547e-01,\n",
            "          -2.7549e-01, -4.9331e-01, -1.3182e+00,  4.5378e-01, -3.1542e-01,\n",
            "          -1.8089e+00, -3.4049e-01, -7.0560e-02,  7.0931e-02,  1.5511e-01,\n",
            "           1.0719e+00, -1.6774e+00,  7.4941e-01, -6.0393e-01, -1.0838e+00],\n",
            "         [-1.0219e+00, -9.3557e-01,  8.4537e-01, -1.3798e+00, -1.1087e+00,\n",
            "           4.0219e-01,  7.6309e-02, -3.1784e-01,  1.3650e+00, -1.3301e+00,\n",
            "          -1.4832e+00, -9.8089e-01, -2.2699e+00, -7.5197e-01, -1.2152e+00,\n",
            "          -3.8503e-01,  5.3976e-01, -7.7177e-01,  6.1751e-01,  2.1793e+00,\n",
            "          -1.1280e+00,  7.0337e-01, -3.3353e-01,  1.2905e+00,  4.9347e-01,\n",
            "           5.8037e-01,  1.2005e+00, -1.5786e+00,  3.9829e-01, -1.0913e+00,\n",
            "          -3.6615e-01,  4.6088e-01,  7.1876e-01, -1.7258e+00, -9.0188e-02,\n",
            "          -1.4782e-02, -8.8946e-01, -7.5669e-01, -9.0381e-01,  1.1814e+00,\n",
            "          -1.0878e+00,  6.6208e-01, -8.3637e-01, -8.0477e-01,  6.1724e-04,\n",
            "           1.3213e+00, -8.6747e-01, -1.5253e-01,  9.0499e-02, -4.0628e-01,\n",
            "           1.1649e-01, -1.2079e+00, -1.6520e+00,  9.0895e-01,  6.2135e-02,\n",
            "          -2.6586e-01, -1.0516e+00, -1.0338e+00,  2.1974e+00,  3.0530e-01,\n",
            "          -2.5632e-01,  2.5969e-01,  1.5510e+00, -5.6964e-01,  1.1173e+00],\n",
            "         [ 1.4787e-01, -4.3331e-01,  5.2033e-01,  3.9772e-01, -2.6225e-01,\n",
            "           1.7675e+00, -1.2460e+00,  1.4583e-01, -5.6994e-01, -1.3561e+00,\n",
            "           1.2897e+00,  8.6078e-01, -8.4937e-01, -3.7719e-01, -1.7326e-01,\n",
            "          -4.7029e-01, -6.0004e-01, -1.3636e+00, -1.3741e-01, -1.4640e+00,\n",
            "           4.9037e-01,  1.8202e+00,  1.8017e+00,  6.0141e-01, -2.5448e+00,\n",
            "          -4.8652e-01, -4.3733e-01, -5.4988e-01, -4.3360e-01, -5.6633e-01,\n",
            "          -1.1238e-01, -3.0501e-01,  1.1426e+00,  6.6372e-01,  7.4657e-01,\n",
            "          -2.1834e-01,  1.1955e-01, -5.2703e-01,  6.0654e-01,  5.8816e-01,\n",
            "          -5.4526e-01,  7.6541e-01, -1.1892e-01,  5.0230e-01,  6.1099e-01,\n",
            "           8.2150e-01, -2.8304e-02, -1.2243e+00, -1.7192e+00,  1.4801e+00,\n",
            "          -9.8939e-01,  8.1746e-02, -1.0265e+00,  6.4049e-01,  2.0734e+00,\n",
            "          -2.9941e-01,  4.7293e-02, -9.6258e-01, -4.3673e-01,  1.3442e+00,\n",
            "           1.6607e+00,  1.4093e+00,  3.3019e-01,  1.5454e+00,  1.3778e+00],\n",
            "         [ 1.0329e+00, -5.5563e-01, -1.3479e+00,  1.4811e-02, -4.4722e-02,\n",
            "          -5.3666e-01, -5.2229e-01,  2.1068e+00, -5.3867e-01,  2.1751e+00,\n",
            "          -1.7514e+00, -1.2576e+00,  6.0938e-01, -2.0551e+00, -4.4305e-01,\n",
            "           1.4874e+00,  1.0655e-01, -2.9780e-01,  5.4480e-01,  2.6449e-01,\n",
            "          -6.4450e-01,  1.0834e+00, -7.9946e-01,  4.3056e-01, -1.2575e+00,\n",
            "          -1.1985e-01,  1.7985e+00, -2.0660e+00,  1.0575e+00, -1.0572e+00,\n",
            "           9.9107e-01,  2.2576e+00,  3.0766e-01,  1.5197e-01, -8.3968e-01,\n",
            "           1.6685e+00,  5.9758e-01, -1.8736e+00,  1.2910e+00,  1.1071e+00,\n",
            "          -3.6937e-01,  1.9295e-01, -3.4203e-01, -8.4607e-01,  5.0153e-01,\n",
            "          -9.6565e-01, -7.2546e-01,  8.7959e-01,  2.1676e-01, -2.8932e+00,\n",
            "          -1.9850e+00,  1.4424e+00,  4.3408e-01, -4.2923e-01,  3.6661e-01,\n",
            "           6.7441e-01,  1.0747e+00, -6.8145e-01,  2.8847e-01,  1.0477e+00,\n",
            "           1.6187e+00,  4.1604e-01,  3.3619e-01, -1.2428e+00, -5.1481e-02],\n",
            "         [-2.0688e-01,  4.7600e-01, -8.8707e-01,  1.3709e+00, -1.9473e+00,\n",
            "          -1.6613e+00,  1.2640e+00, -4.2307e-01, -1.8335e-01,  2.1775e-01,\n",
            "          -3.2975e-01, -1.9170e-02,  9.2251e-01,  1.5641e+00, -1.1374e+00,\n",
            "           1.2083e+00,  2.7124e-01,  1.2369e-01,  1.8620e+00,  1.7080e+00,\n",
            "          -1.6045e+00,  2.9287e-01, -1.2572e-01,  4.7992e-01, -3.9235e-01,\n",
            "          -1.4055e+00, -6.6859e-01, -4.8307e-01, -2.2978e-01, -1.6844e-01,\n",
            "           1.7177e+00,  1.7665e-01,  1.8060e-01, -6.9085e-01, -3.0646e-01,\n",
            "          -1.1809e+00,  8.1750e-01, -6.4961e-01,  4.6269e-02,  1.4899e+00,\n",
            "           2.1607e+00, -3.6567e-01,  8.2824e-01, -4.8264e-01,  1.8330e+00,\n",
            "           1.7229e+00,  2.5107e-01,  8.7205e-02,  5.1863e-01,  6.9787e-03,\n",
            "           1.4843e-01, -5.4027e-01, -1.9312e+00,  1.8214e+00, -1.6471e+00,\n",
            "           2.8126e-01, -7.8558e-01, -5.3489e-01,  5.8117e-01, -5.3562e-01,\n",
            "          -1.7944e+00, -2.4687e-01, -7.0644e-01,  1.3517e+00, -3.3502e-01],\n",
            "         [ 1.6347e+00, -5.1818e-02,  4.9956e-01,  7.2163e-01,  5.0846e-01,\n",
            "          -7.7194e-01,  2.3878e-01,  3.1380e-01,  2.1783e-01,  3.2751e-02,\n",
            "          -1.6986e-01,  1.0659e+00,  7.1999e-01, -6.1656e-01,  8.0607e-02,\n",
            "           2.5231e+00, -1.4623e+00,  2.1707e+00,  1.6242e-01,  1.0296e+00,\n",
            "          -1.1377e+00,  5.8559e-01,  1.7345e-02,  3.1358e-01,  1.0124e+00,\n",
            "           1.5122e+00, -3.3592e-01,  2.4560e-01, -3.7731e-01,  1.5870e-01,\n",
            "           2.1503e+00, -1.5131e+00, -9.5524e-01, -8.9947e-01, -9.5830e-01,\n",
            "          -5.9446e-01,  5.8500e-01,  5.2657e-01,  7.6147e-01,  5.3312e-01,\n",
            "           1.1796e+00,  1.3316e+00, -2.0936e-01,  9.6004e-02, -6.9447e-01,\n",
            "           5.6691e-01, -5.8833e-01,  1.4064e+00, -1.2537e+00, -1.5195e+00,\n",
            "           7.4456e-01,  1.1914e+00,  1.8006e-01,  1.2333e+00, -2.2986e-01,\n",
            "          -1.5306e-01,  8.4084e-01, -3.9928e-01, -6.1258e-01, -6.5973e-01,\n",
            "           5.9059e-01,  1.1219e+00,  2.4315e-01,  1.1519e+00,  9.9503e-01]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "l-QYjt'CL?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example word probabilities for the next word in a sentence\n",
        "word_probs = torch.tensor([0.1, 0.3, 0.2, 0.4])\n",
        "\n",
        "# Higher temperature increases randomness\n",
        "high_temp_samples = torch.multinomial(word_probs, num_samples=1, generator=torch.manual_seed(420))\n",
        "print(\"High temp samples:\", high_temp_samples)\n",
        "\n",
        "# Lower temperature makes the sampling more deterministic\n",
        "low_temp_samples = torch.multinomial(word_probs, num_samples=1, generator=torch.manual_seed(492))\n",
        "print(\"Low temp samples:\", low_temp_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltAqxiAxPtMY",
        "outputId": "866ef512-8146-4cf1-a2a1-f924990c36bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "High temp samples: tensor([1])\n",
            "Low temp samples: tensor([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnt = [0, 0, 0, 0]\n",
        "for i in range(5000):\n",
        "  sampled_Y = torch.multinomial(torch.tensor([0.1, 0.1, 0.3, 0.5]), 1)\n",
        "  cnt[sampled_Y[0]] += 1\n",
        "\n",
        "  if(i == 0  ):\n",
        "\n",
        "    print(i, \" -> \" , cnt)\n",
        "\n",
        "  elif(i % 1000 == 0):\n",
        "\n",
        "    print(i, \" -> \" , cnt)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCxzFXOsX4Bx",
        "outputId": "95e5043b-d605-4335-8e38-7c9f8484b213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0  ->  [0, 0, 0, 1]\n",
            "1000  ->  [101, 83, 279, 538]\n",
            "2000  ->  [230, 182, 573, 1016]\n",
            "3000  ->  [312, 286, 904, 1499]\n",
            "4000  ->  [401, 384, 1234, 1982]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N8PYsmjcPtSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            print(\"*\"*100)\n",
        "            print(\"logits 1  \", logits.shape)\n",
        "            print(logits)\n",
        "            print()\n",
        "            print(\"loss \")\n",
        "            print(loss)\n",
        "            print()\n",
        "\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            print(\"logits 2  \", logits.shape)\n",
        "            print(logits)\n",
        "            print()\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            print(\"probs  \", probs.shape)\n",
        "            print(probs)\n",
        "            print()\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            print(\"idx_next  \", idx_next.shape)\n",
        "            print(idx_next)\n",
        "            print()\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "            print(\"idx  \", idx.shape)\n",
        "            print(idx)\n",
        "            print()\n",
        "            print(\"*\"*100)\n",
        "\n",
        "        return idx\n",
        "\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=10)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPXczpdwojXH",
        "outputId": "18494577-585d-45b3-b04e-d3dfe0710617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.7736, grad_fn=<NllLossBackward0>)\n",
            "****************************************************************************************************\n",
            "logits 1   torch.Size([1, 1, 65])\n",
            "tensor([[[ 6.4022e-01, -8.3649e-01,  2.2870e+00, -6.3659e-01, -1.6597e+00,\n",
            "           9.9623e-01, -8.3188e-01,  5.2271e-01, -1.0720e+00, -6.9581e-01,\n",
            "          -1.7181e-01, -9.1379e-01,  3.7381e-01, -7.0926e-01, -2.8789e-01,\n",
            "          -1.3386e+00,  9.9227e-01, -3.6417e-01, -6.1489e-02,  2.7885e-01,\n",
            "          -1.6062e+00, -3.2556e-01,  2.2351e-01, -1.0779e+00, -7.9957e-01,\n",
            "          -8.6830e-03,  5.0956e-01,  2.0606e+00, -1.7745e+00,  3.1508e-01,\n",
            "           1.4927e+00,  1.0682e-02, -4.0964e-01, -2.1083e+00,  1.7071e-01,\n",
            "           4.3387e-01,  1.4519e+00, -2.2249e+00,  7.7063e-01,  1.0032e+00,\n",
            "          -7.0616e-04, -4.3484e-01, -1.9791e-01, -1.1886e+00, -3.1788e-01,\n",
            "          -8.7637e-01, -1.1614e+00,  2.7749e-01, -5.7510e-01, -4.2678e-01,\n",
            "           7.3345e-01, -3.2024e-01,  6.5886e-01,  1.7822e-01, -8.8296e-01,\n",
            "          -1.1789e+00,  2.4352e+00,  1.2554e+00, -1.6087e+00,  2.0354e-01,\n",
            "          -8.8759e-01,  8.7902e-02, -5.1729e-01, -8.0634e-03, -2.4454e-01]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "loss \n",
            "None\n",
            "\n",
            "logits 2   torch.Size([1, 65])\n",
            "tensor([[ 6.4022e-01, -8.3649e-01,  2.2870e+00, -6.3659e-01, -1.6597e+00,\n",
            "          9.9623e-01, -8.3188e-01,  5.2271e-01, -1.0720e+00, -6.9581e-01,\n",
            "         -1.7181e-01, -9.1379e-01,  3.7381e-01, -7.0926e-01, -2.8789e-01,\n",
            "         -1.3386e+00,  9.9227e-01, -3.6417e-01, -6.1489e-02,  2.7885e-01,\n",
            "         -1.6062e+00, -3.2556e-01,  2.2351e-01, -1.0779e+00, -7.9957e-01,\n",
            "         -8.6830e-03,  5.0956e-01,  2.0606e+00, -1.7745e+00,  3.1508e-01,\n",
            "          1.4927e+00,  1.0682e-02, -4.0964e-01, -2.1083e+00,  1.7071e-01,\n",
            "          4.3387e-01,  1.4519e+00, -2.2249e+00,  7.7063e-01,  1.0032e+00,\n",
            "         -7.0616e-04, -4.3484e-01, -1.9791e-01, -1.1886e+00, -3.1788e-01,\n",
            "         -8.7637e-01, -1.1614e+00,  2.7749e-01, -5.7510e-01, -4.2678e-01,\n",
            "          7.3345e-01, -3.2024e-01,  6.5886e-01,  1.7822e-01, -8.8296e-01,\n",
            "         -1.1789e+00,  2.4352e+00,  1.2554e+00, -1.6087e+00,  2.0354e-01,\n",
            "         -8.8759e-01,  8.7902e-02, -5.1729e-01, -8.0634e-03, -2.4454e-01]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "probs   torch.Size([1, 65])\n",
            "tensor([[0.0199, 0.0045, 0.1033, 0.0056, 0.0020, 0.0284, 0.0046, 0.0177, 0.0036,\n",
            "         0.0052, 0.0088, 0.0042, 0.0152, 0.0052, 0.0079, 0.0028, 0.0283, 0.0073,\n",
            "         0.0099, 0.0139, 0.0021, 0.0076, 0.0131, 0.0036, 0.0047, 0.0104, 0.0175,\n",
            "         0.0824, 0.0018, 0.0144, 0.0467, 0.0106, 0.0070, 0.0013, 0.0124, 0.0162,\n",
            "         0.0448, 0.0011, 0.0227, 0.0286, 0.0105, 0.0068, 0.0086, 0.0032, 0.0076,\n",
            "         0.0044, 0.0033, 0.0138, 0.0059, 0.0068, 0.0218, 0.0076, 0.0203, 0.0125,\n",
            "         0.0043, 0.0032, 0.1198, 0.0368, 0.0021, 0.0129, 0.0043, 0.0115, 0.0063,\n",
            "         0.0104, 0.0082]], grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "idx_next   torch.Size([1, 1])\n",
            "tensor([[57]])\n",
            "\n",
            "idx   torch.Size([1, 2])\n",
            "tensor([[ 0, 57]])\n",
            "\n",
            "****************************************************************************************************\n",
            "****************************************************************************************************\n",
            "logits 1   torch.Size([1, 2, 65])\n",
            "tensor([[[ 6.4022e-01, -8.3649e-01,  2.2870e+00, -6.3659e-01, -1.6597e+00,\n",
            "           9.9623e-01, -8.3188e-01,  5.2271e-01, -1.0720e+00, -6.9581e-01,\n",
            "          -1.7181e-01, -9.1379e-01,  3.7381e-01, -7.0926e-01, -2.8789e-01,\n",
            "          -1.3386e+00,  9.9227e-01, -3.6417e-01, -6.1489e-02,  2.7885e-01,\n",
            "          -1.6062e+00, -3.2556e-01,  2.2351e-01, -1.0779e+00, -7.9957e-01,\n",
            "          -8.6830e-03,  5.0956e-01,  2.0606e+00, -1.7745e+00,  3.1508e-01,\n",
            "           1.4927e+00,  1.0682e-02, -4.0964e-01, -2.1083e+00,  1.7071e-01,\n",
            "           4.3387e-01,  1.4519e+00, -2.2249e+00,  7.7063e-01,  1.0032e+00,\n",
            "          -7.0616e-04, -4.3484e-01, -1.9791e-01, -1.1886e+00, -3.1788e-01,\n",
            "          -8.7637e-01, -1.1614e+00,  2.7749e-01, -5.7510e-01, -4.2678e-01,\n",
            "           7.3345e-01, -3.2024e-01,  6.5886e-01,  1.7822e-01, -8.8296e-01,\n",
            "          -1.1789e+00,  2.4352e+00,  1.2554e+00, -1.6087e+00,  2.0354e-01,\n",
            "          -8.8759e-01,  8.7902e-02, -5.1729e-01, -8.0634e-03, -2.4454e-01],\n",
            "         [-3.8277e-01, -1.4797e+00, -9.3967e-01,  7.0841e-01, -4.7449e-01,\n",
            "          -1.1234e-01, -8.3979e-03,  1.2981e+00,  1.0284e+00,  1.9532e+00,\n",
            "           9.6351e-01, -5.5102e-01, -1.8094e-02, -1.1516e+00,  9.6337e-01,\n",
            "          -1.1422e+00,  4.2531e-01,  7.8829e-01,  1.5791e-01, -6.5050e-01,\n",
            "          -1.9515e+00, -5.1075e-01, -3.1796e-01, -1.2072e+00, -4.5217e-01,\n",
            "           1.1605e+00, -4.3852e-01,  1.3751e+00, -1.5350e-01, -5.4588e-01,\n",
            "           7.3537e-01, -1.9808e-01,  1.2885e+00, -8.1364e-01, -2.5151e-01,\n",
            "           1.0707e+00, -7.2009e-01, -2.2826e-01,  5.7055e-01,  2.8824e-01,\n",
            "          -4.4158e-01, -1.0114e+00, -5.8738e-02, -8.0512e-01, -1.2166e-01,\n",
            "           9.6103e-01, -3.1628e-01,  6.9615e-01,  1.2500e+00,  3.2730e-01,\n",
            "          -1.6830e+00, -1.0868e+00,  1.7973e-01, -4.9715e-01, -8.9743e-01,\n",
            "           2.3514e+00, -1.7441e+00,  2.9130e-01,  5.1169e-01, -3.0511e-01,\n",
            "          -5.1778e-01,  1.1511e+00,  1.0217e+00, -9.4185e-01,  1.3796e+00]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "loss \n",
            "None\n",
            "\n",
            "logits 2   torch.Size([1, 65])\n",
            "tensor([[-0.3828, -1.4797, -0.9397,  0.7084, -0.4745, -0.1123, -0.0084,  1.2981,\n",
            "          1.0284,  1.9532,  0.9635, -0.5510, -0.0181, -1.1516,  0.9634, -1.1422,\n",
            "          0.4253,  0.7883,  0.1579, -0.6505, -1.9515, -0.5107, -0.3180, -1.2072,\n",
            "         -0.4522,  1.1605, -0.4385,  1.3751, -0.1535, -0.5459,  0.7354, -0.1981,\n",
            "          1.2885, -0.8136, -0.2515,  1.0707, -0.7201, -0.2283,  0.5706,  0.2882,\n",
            "         -0.4416, -1.0114, -0.0587, -0.8051, -0.1217,  0.9610, -0.3163,  0.6962,\n",
            "          1.2500,  0.3273, -1.6830, -1.0868,  0.1797, -0.4971, -0.8974,  2.3514,\n",
            "         -1.7441,  0.2913,  0.5117, -0.3051, -0.5178,  1.1511,  1.0217, -0.9418,\n",
            "          1.3796]], grad_fn=<SliceBackward0>)\n",
            "\n",
            "probs   torch.Size([1, 65])\n",
            "tensor([[0.0068, 0.0023, 0.0039, 0.0202, 0.0062, 0.0089, 0.0099, 0.0365, 0.0278,\n",
            "         0.0702, 0.0261, 0.0057, 0.0098, 0.0031, 0.0261, 0.0032, 0.0152, 0.0219,\n",
            "         0.0117, 0.0052, 0.0014, 0.0060, 0.0072, 0.0030, 0.0063, 0.0318, 0.0064,\n",
            "         0.0394, 0.0085, 0.0058, 0.0208, 0.0082, 0.0361, 0.0044, 0.0077, 0.0290,\n",
            "         0.0048, 0.0079, 0.0176, 0.0133, 0.0064, 0.0036, 0.0094, 0.0044, 0.0088,\n",
            "         0.0260, 0.0073, 0.0200, 0.0347, 0.0138, 0.0018, 0.0034, 0.0119, 0.0061,\n",
            "         0.0041, 0.1045, 0.0017, 0.0133, 0.0166, 0.0073, 0.0059, 0.0315, 0.0277,\n",
            "         0.0039, 0.0396]], grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "idx_next   torch.Size([1, 1])\n",
            "tensor([[25]])\n",
            "\n",
            "idx   torch.Size([1, 3])\n",
            "tensor([[ 0, 57, 25]])\n",
            "\n",
            "****************************************************************************************************\n",
            "****************************************************************************************************\n",
            "logits 1   torch.Size([1, 3, 65])\n",
            "tensor([[[ 6.4022e-01, -8.3649e-01,  2.2870e+00, -6.3659e-01, -1.6597e+00,\n",
            "           9.9623e-01, -8.3188e-01,  5.2271e-01, -1.0720e+00, -6.9581e-01,\n",
            "          -1.7181e-01, -9.1379e-01,  3.7381e-01, -7.0926e-01, -2.8789e-01,\n",
            "          -1.3386e+00,  9.9227e-01, -3.6417e-01, -6.1489e-02,  2.7885e-01,\n",
            "          -1.6062e+00, -3.2556e-01,  2.2351e-01, -1.0779e+00, -7.9957e-01,\n",
            "          -8.6830e-03,  5.0956e-01,  2.0606e+00, -1.7745e+00,  3.1508e-01,\n",
            "           1.4927e+00,  1.0682e-02, -4.0964e-01, -2.1083e+00,  1.7071e-01,\n",
            "           4.3387e-01,  1.4519e+00, -2.2249e+00,  7.7063e-01,  1.0032e+00,\n",
            "          -7.0616e-04, -4.3484e-01, -1.9791e-01, -1.1886e+00, -3.1788e-01,\n",
            "          -8.7637e-01, -1.1614e+00,  2.7749e-01, -5.7510e-01, -4.2678e-01,\n",
            "           7.3345e-01, -3.2024e-01,  6.5886e-01,  1.7822e-01, -8.8296e-01,\n",
            "          -1.1789e+00,  2.4352e+00,  1.2554e+00, -1.6087e+00,  2.0354e-01,\n",
            "          -8.8759e-01,  8.7902e-02, -5.1729e-01, -8.0634e-03, -2.4454e-01],\n",
            "         [-3.8277e-01, -1.4797e+00, -9.3967e-01,  7.0841e-01, -4.7449e-01,\n",
            "          -1.1234e-01, -8.3979e-03,  1.2981e+00,  1.0284e+00,  1.9532e+00,\n",
            "           9.6351e-01, -5.5102e-01, -1.8094e-02, -1.1516e+00,  9.6337e-01,\n",
            "          -1.1422e+00,  4.2531e-01,  7.8829e-01,  1.5791e-01, -6.5050e-01,\n",
            "          -1.9515e+00, -5.1075e-01, -3.1796e-01, -1.2072e+00, -4.5217e-01,\n",
            "           1.1605e+00, -4.3852e-01,  1.3751e+00, -1.5350e-01, -5.4588e-01,\n",
            "           7.3537e-01, -1.9808e-01,  1.2885e+00, -8.1364e-01, -2.5151e-01,\n",
            "           1.0707e+00, -7.2009e-01, -2.2826e-01,  5.7055e-01,  2.8824e-01,\n",
            "          -4.4158e-01, -1.0114e+00, -5.8738e-02, -8.0512e-01, -1.2166e-01,\n",
            "           9.6103e-01, -3.1628e-01,  6.9615e-01,  1.2500e+00,  3.2730e-01,\n",
            "          -1.6830e+00, -1.0868e+00,  1.7973e-01, -4.9715e-01, -8.9743e-01,\n",
            "           2.3514e+00, -1.7441e+00,  2.9130e-01,  5.1169e-01, -3.0511e-01,\n",
            "          -5.1778e-01,  1.1511e+00,  1.0217e+00, -9.4185e-01,  1.3796e+00],\n",
            "         [-1.7727e+00, -4.7958e-01, -1.1037e+00,  1.5407e+00, -1.7011e+00,\n",
            "           5.4244e-01,  2.5793e-01, -6.4687e-01, -1.3273e+00, -8.0335e-01,\n",
            "          -2.0133e+00,  7.5757e-01,  1.4304e+00, -2.5731e+00,  1.6308e-01,\n",
            "           5.0361e-01, -1.6828e+00, -6.0351e-01,  3.2846e-01, -4.8570e-01,\n",
            "          -2.2001e-01,  7.2255e-01, -6.9377e-01, -5.0921e-01, -1.5880e-01,\n",
            "          -1.5865e-01, -1.0233e+00,  7.9247e-01, -2.2431e-01, -3.7015e-01,\n",
            "          -6.8938e-02,  7.1754e-01,  2.0767e-01,  5.7276e-01, -5.7819e-02,\n",
            "           1.9290e-02, -6.9943e-01, -1.6268e+00, -2.0428e+00,  4.1387e-01,\n",
            "           8.4179e-01,  6.8693e-01,  1.2607e-01, -1.0933e+00,  6.2169e-01,\n",
            "           4.7553e-01, -7.4559e-01, -8.4729e-01, -1.6935e-01, -7.9273e-01,\n",
            "           5.8127e-01,  3.8010e-01, -8.6380e-01,  4.6584e-01,  4.1352e-01,\n",
            "           1.2824e+00, -3.3220e-01,  2.2574e+00,  7.1990e-02, -7.0376e-03,\n",
            "           1.7284e+00, -1.2540e-01, -1.2134e+00,  1.2877e+00, -5.9620e-01]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "loss \n",
            "None\n",
            "\n",
            "logits 2   torch.Size([1, 65])\n",
            "tensor([[-1.7727, -0.4796, -1.1037,  1.5407, -1.7011,  0.5424,  0.2579, -0.6469,\n",
            "         -1.3273, -0.8033, -2.0133,  0.7576,  1.4304, -2.5731,  0.1631,  0.5036,\n",
            "         -1.6828, -0.6035,  0.3285, -0.4857, -0.2200,  0.7226, -0.6938, -0.5092,\n",
            "         -0.1588, -0.1587, -1.0233,  0.7925, -0.2243, -0.3702, -0.0689,  0.7175,\n",
            "          0.2077,  0.5728, -0.0578,  0.0193, -0.6994, -1.6268, -2.0428,  0.4139,\n",
            "          0.8418,  0.6869,  0.1261, -1.0933,  0.6217,  0.4755, -0.7456, -0.8473,\n",
            "         -0.1694, -0.7927,  0.5813,  0.3801, -0.8638,  0.4658,  0.4135,  1.2824,\n",
            "         -0.3322,  2.2574,  0.0720, -0.0070,  1.7284, -0.1254, -1.2134,  1.2877,\n",
            "         -0.5962]], grad_fn=<SliceBackward0>)\n",
            "\n",
            "probs   torch.Size([1, 65])\n",
            "tensor([[0.0019, 0.0071, 0.0038, 0.0534, 0.0021, 0.0197, 0.0148, 0.0060, 0.0030,\n",
            "         0.0051, 0.0015, 0.0244, 0.0478, 0.0009, 0.0135, 0.0189, 0.0021, 0.0063,\n",
            "         0.0159, 0.0070, 0.0092, 0.0235, 0.0057, 0.0069, 0.0098, 0.0098, 0.0041,\n",
            "         0.0252, 0.0091, 0.0079, 0.0107, 0.0234, 0.0141, 0.0203, 0.0108, 0.0117,\n",
            "         0.0057, 0.0022, 0.0015, 0.0173, 0.0265, 0.0227, 0.0130, 0.0038, 0.0213,\n",
            "         0.0184, 0.0054, 0.0049, 0.0096, 0.0052, 0.0204, 0.0167, 0.0048, 0.0182,\n",
            "         0.0173, 0.0412, 0.0082, 0.1093, 0.0123, 0.0114, 0.0644, 0.0101, 0.0034,\n",
            "         0.0414, 0.0063]], grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "idx_next   torch.Size([1, 1])\n",
            "tensor([[60]])\n",
            "\n",
            "idx   torch.Size([1, 4])\n",
            "tensor([[ 0, 57, 25, 60]])\n",
            "\n",
            "****************************************************************************************************\n",
            "****************************************************************************************************\n",
            "logits 1   torch.Size([1, 4, 65])\n",
            "tensor([[[ 6.4022e-01, -8.3649e-01,  2.2870e+00, -6.3659e-01, -1.6597e+00,\n",
            "           9.9623e-01, -8.3188e-01,  5.2271e-01, -1.0720e+00, -6.9581e-01,\n",
            "          -1.7181e-01, -9.1379e-01,  3.7381e-01, -7.0926e-01, -2.8789e-01,\n",
            "          -1.3386e+00,  9.9227e-01, -3.6417e-01, -6.1489e-02,  2.7885e-01,\n",
            "          -1.6062e+00, -3.2556e-01,  2.2351e-01, -1.0779e+00, -7.9957e-01,\n",
            "          -8.6830e-03,  5.0956e-01,  2.0606e+00, -1.7745e+00,  3.1508e-01,\n",
            "           1.4927e+00,  1.0682e-02, -4.0964e-01, -2.1083e+00,  1.7071e-01,\n",
            "           4.3387e-01,  1.4519e+00, -2.2249e+00,  7.7063e-01,  1.0032e+00,\n",
            "          -7.0616e-04, -4.3484e-01, -1.9791e-01, -1.1886e+00, -3.1788e-01,\n",
            "          -8.7637e-01, -1.1614e+00,  2.7749e-01, -5.7510e-01, -4.2678e-01,\n",
            "           7.3345e-01, -3.2024e-01,  6.5886e-01,  1.7822e-01, -8.8296e-01,\n",
            "          -1.1789e+00,  2.4352e+00,  1.2554e+00, -1.6087e+00,  2.0354e-01,\n",
            "          -8.8759e-01,  8.7902e-02, -5.1729e-01, -8.0634e-03, -2.4454e-01],\n",
            "         [-3.8277e-01, -1.4797e+00, -9.3967e-01,  7.0841e-01, -4.7449e-01,\n",
            "          -1.1234e-01, -8.3979e-03,  1.2981e+00,  1.0284e+00,  1.9532e+00,\n",
            "           9.6351e-01, -5.5102e-01, -1.8094e-02, -1.1516e+00,  9.6337e-01,\n",
            "          -1.1422e+00,  4.2531e-01,  7.8829e-01,  1.5791e-01, -6.5050e-01,\n",
            "          -1.9515e+00, -5.1075e-01, -3.1796e-01, -1.2072e+00, -4.5217e-01,\n",
            "           1.1605e+00, -4.3852e-01,  1.3751e+00, -1.5350e-01, -5.4588e-01,\n",
            "           7.3537e-01, -1.9808e-01,  1.2885e+00, -8.1364e-01, -2.5151e-01,\n",
            "           1.0707e+00, -7.2009e-01, -2.2826e-01,  5.7055e-01,  2.8824e-01,\n",
            "          -4.4158e-01, -1.0114e+00, -5.8738e-02, -8.0512e-01, -1.2166e-01,\n",
            "           9.6103e-01, -3.1628e-01,  6.9615e-01,  1.2500e+00,  3.2730e-01,\n",
            "          -1.6830e+00, -1.0868e+00,  1.7973e-01, -4.9715e-01, -8.9743e-01,\n",
            "           2.3514e+00, -1.7441e+00,  2.9130e-01,  5.1169e-01, -3.0511e-01,\n",
            "          -5.1778e-01,  1.1511e+00,  1.0217e+00, -9.4185e-01,  1.3796e+00],\n",
            "         [-1.7727e+00, -4.7958e-01, -1.1037e+00,  1.5407e+00, -1.7011e+00,\n",
            "           5.4244e-01,  2.5793e-01, -6.4687e-01, -1.3273e+00, -8.0335e-01,\n",
            "          -2.0133e+00,  7.5757e-01,  1.4304e+00, -2.5731e+00,  1.6308e-01,\n",
            "           5.0361e-01, -1.6828e+00, -6.0351e-01,  3.2846e-01, -4.8570e-01,\n",
            "          -2.2001e-01,  7.2255e-01, -6.9377e-01, -5.0921e-01, -1.5880e-01,\n",
            "          -1.5865e-01, -1.0233e+00,  7.9247e-01, -2.2431e-01, -3.7015e-01,\n",
            "          -6.8938e-02,  7.1754e-01,  2.0767e-01,  5.7276e-01, -5.7819e-02,\n",
            "           1.9290e-02, -6.9943e-01, -1.6268e+00, -2.0428e+00,  4.1387e-01,\n",
            "           8.4179e-01,  6.8693e-01,  1.2607e-01, -1.0933e+00,  6.2169e-01,\n",
            "           4.7553e-01, -7.4559e-01, -8.4729e-01, -1.6935e-01, -7.9273e-01,\n",
            "           5.8127e-01,  3.8010e-01, -8.6380e-01,  4.6584e-01,  4.1352e-01,\n",
            "           1.2824e+00, -3.3220e-01,  2.2574e+00,  7.1990e-02, -7.0376e-03,\n",
            "           1.7284e+00, -1.2540e-01, -1.2134e+00,  1.2877e+00, -5.9620e-01],\n",
            "         [-7.9098e-01, -1.2310e+00,  9.0967e-01,  2.3525e+00,  1.2958e+00,\n",
            "           2.4102e-01,  5.4828e-01,  3.4034e-01, -8.4229e-01, -1.6776e+00,\n",
            "          -5.7521e-01,  1.7487e-01, -4.0092e-01,  1.0710e+00, -1.5919e+00,\n",
            "          -1.9438e-01,  2.8927e-01, -1.1849e+00, -4.5773e-01, -6.0599e-01,\n",
            "           9.6084e-01, -9.3197e-01,  7.7462e-01, -9.5885e-01,  4.1500e-01,\n",
            "          -3.5605e-01,  5.4656e-01, -9.4210e-02, -8.3026e-01,  2.1989e-01,\n",
            "          -8.4314e-01,  6.5872e-01, -1.1154e+00,  3.9677e-01, -4.0450e-01,\n",
            "           3.5118e-01,  4.0325e-01,  5.4963e-01,  8.0297e-01, -1.6678e+00,\n",
            "           9.0490e-01, -7.5773e-02,  4.8272e-01, -4.1663e-01, -1.4605e-01,\n",
            "          -2.8069e-01,  1.6233e+00,  1.1839e+00,  6.0073e-01,  8.1070e-01,\n",
            "          -7.0183e-01,  2.0120e-01, -6.0440e-02,  8.4979e-01, -4.8596e-01,\n",
            "           6.8533e-01, -2.3514e-01, -7.9586e-01, -1.0194e-01,  2.4729e+00,\n",
            "           6.7204e-01, -1.1681e+00,  5.3780e-01, -2.3080e-01,  2.9356e+00]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "loss \n",
            "None\n",
            "\n",
            "logits 2   torch.Size([1, 65])\n",
            "tensor([[-0.7910, -1.2310,  0.9097,  2.3525,  1.2958,  0.2410,  0.5483,  0.3403,\n",
            "         -0.8423, -1.6776, -0.5752,  0.1749, -0.4009,  1.0710, -1.5919, -0.1944,\n",
            "          0.2893, -1.1849, -0.4577, -0.6060,  0.9608, -0.9320,  0.7746, -0.9588,\n",
            "          0.4150, -0.3560,  0.5466, -0.0942, -0.8303,  0.2199, -0.8431,  0.6587,\n",
            "         -1.1154,  0.3968, -0.4045,  0.3512,  0.4033,  0.5496,  0.8030, -1.6678,\n",
            "          0.9049, -0.0758,  0.4827, -0.4166, -0.1461, -0.2807,  1.6233,  1.1839,\n",
            "          0.6007,  0.8107, -0.7018,  0.2012, -0.0604,  0.8498, -0.4860,  0.6853,\n",
            "         -0.2351, -0.7959, -0.1019,  2.4729,  0.6720, -1.1681,  0.5378, -0.2308,\n",
            "          2.9356]], grad_fn=<SliceBackward0>)\n",
            "\n",
            "probs   torch.Size([1, 65])\n",
            "tensor([[0.0038, 0.0024, 0.0206, 0.0872, 0.0303, 0.0106, 0.0144, 0.0117, 0.0036,\n",
            "         0.0015, 0.0047, 0.0099, 0.0056, 0.0242, 0.0017, 0.0068, 0.0111, 0.0025,\n",
            "         0.0052, 0.0045, 0.0217, 0.0033, 0.0180, 0.0032, 0.0126, 0.0058, 0.0143,\n",
            "         0.0076, 0.0036, 0.0103, 0.0036, 0.0160, 0.0027, 0.0123, 0.0055, 0.0118,\n",
            "         0.0124, 0.0144, 0.0185, 0.0016, 0.0205, 0.0077, 0.0134, 0.0055, 0.0072,\n",
            "         0.0063, 0.0421, 0.0271, 0.0151, 0.0187, 0.0041, 0.0101, 0.0078, 0.0194,\n",
            "         0.0051, 0.0165, 0.0066, 0.0037, 0.0075, 0.0984, 0.0162, 0.0026, 0.0142,\n",
            "         0.0066, 0.1562]], grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "idx_next   torch.Size([1, 1])\n",
            "tensor([[59]])\n",
            "\n",
            "idx   torch.Size([1, 5])\n",
            "tensor([[ 0, 57, 25, 60, 59]])\n",
            "\n",
            "****************************************************************************************************\n",
            "****************************************************************************************************\n",
            "logits 1   torch.Size([1, 5, 65])\n",
            "tensor([[[ 6.4022e-01, -8.3649e-01,  2.2870e+00, -6.3659e-01, -1.6597e+00,\n",
            "           9.9623e-01, -8.3188e-01,  5.2271e-01, -1.0720e+00, -6.9581e-01,\n",
            "          -1.7181e-01, -9.1379e-01,  3.7381e-01, -7.0926e-01, -2.8789e-01,\n",
            "          -1.3386e+00,  9.9227e-01, -3.6417e-01, -6.1489e-02,  2.7885e-01,\n",
            "          -1.6062e+00, -3.2556e-01,  2.2351e-01, -1.0779e+00, -7.9957e-01,\n",
            "          -8.6830e-03,  5.0956e-01,  2.0606e+00, -1.7745e+00,  3.1508e-01,\n",
            "           1.4927e+00,  1.0682e-02, -4.0964e-01, -2.1083e+00,  1.7071e-01,\n",
            "           4.3387e-01,  1.4519e+00, -2.2249e+00,  7.7063e-01,  1.0032e+00,\n",
            "          -7.0616e-04, -4.3484e-01, -1.9791e-01, -1.1886e+00, -3.1788e-01,\n",
            "          -8.7637e-01, -1.1614e+00,  2.7749e-01, -5.7510e-01, -4.2678e-01,\n",
            "           7.3345e-01, -3.2024e-01,  6.5886e-01,  1.7822e-01, -8.8296e-01,\n",
            "          -1.1789e+00,  2.4352e+00,  1.2554e+00, -1.6087e+00,  2.0354e-01,\n",
            "          -8.8759e-01,  8.7902e-02, -5.1729e-01, -8.0634e-03, -2.4454e-01],\n",
            "         [-3.8277e-01, -1.4797e+00, -9.3967e-01,  7.0841e-01, -4.7449e-01,\n",
            "          -1.1234e-01, -8.3979e-03,  1.2981e+00,  1.0284e+00,  1.9532e+00,\n",
            "           9.6351e-01, -5.5102e-01, -1.8094e-02, -1.1516e+00,  9.6337e-01,\n",
            "          -1.1422e+00,  4.2531e-01,  7.8829e-01,  1.5791e-01, -6.5050e-01,\n",
            "          -1.9515e+00, -5.1075e-01, -3.1796e-01, -1.2072e+00, -4.5217e-01,\n",
            "           1.1605e+00, -4.3852e-01,  1.3751e+00, -1.5350e-01, -5.4588e-01,\n",
            "           7.3537e-01, -1.9808e-01,  1.2885e+00, -8.1364e-01, -2.5151e-01,\n",
            "           1.0707e+00, -7.2009e-01, -2.2826e-01,  5.7055e-01,  2.8824e-01,\n",
            "          -4.4158e-01, -1.0114e+00, -5.8738e-02, -8.0512e-01, -1.2166e-01,\n",
            "           9.6103e-01, -3.1628e-01,  6.9615e-01,  1.2500e+00,  3.2730e-01,\n",
            "          -1.6830e+00, -1.0868e+00,  1.7973e-01, -4.9715e-01, -8.9743e-01,\n",
            "           2.3514e+00, -1.7441e+00,  2.9130e-01,  5.1169e-01, -3.0511e-01,\n",
            "          -5.1778e-01,  1.1511e+00,  1.0217e+00, -9.4185e-01,  1.3796e+00],\n",
            "         [-1.7727e+00, -4.7958e-01, -1.1037e+00,  1.5407e+00, -1.7011e+00,\n",
            "           5.4244e-01,  2.5793e-01, -6.4687e-01, -1.3273e+00, -8.0335e-01,\n",
            "          -2.0133e+00,  7.5757e-01,  1.4304e+00, -2.5731e+00,  1.6308e-01,\n",
            "           5.0361e-01, -1.6828e+00, -6.0351e-01,  3.2846e-01, -4.8570e-01,\n",
            "          -2.2001e-01,  7.2255e-01, -6.9377e-01, -5.0921e-01, -1.5880e-01,\n",
            "          -1.5865e-01, -1.0233e+00,  7.9247e-01, -2.2431e-01, -3.7015e-01,\n",
            "          -6.8938e-02,  7.1754e-01,  2.0767e-01,  5.7276e-01, -5.7819e-02,\n",
            "           1.9290e-02, -6.9943e-01, -1.6268e+00, -2.0428e+00,  4.1387e-01,\n",
            "           8.4179e-01,  6.8693e-01,  1.2607e-01, -1.0933e+00,  6.2169e-01,\n",
            "           4.7553e-01, -7.4559e-01, -8.4729e-01, -1.6935e-01, -7.9273e-01,\n",
            "           5.8127e-01,  3.8010e-01, -8.6380e-01,  4.6584e-01,  4.1352e-01,\n",
            "           1.2824e+00, -3.3220e-01,  2.2574e+00,  7.1990e-02, -7.0376e-03,\n",
            "           1.7284e+00, -1.2540e-01, -1.2134e+00,  1.2877e+00, -5.9620e-01],\n",
            "         [-7.9098e-01, -1.2310e+00,  9.0967e-01,  2.3525e+00,  1.2958e+00,\n",
            "           2.4102e-01,  5.4828e-01,  3.4034e-01, -8.4229e-01, -1.6776e+00,\n",
            "          -5.7521e-01,  1.7487e-01, -4.0092e-01,  1.0710e+00, -1.5919e+00,\n",
            "          -1.9438e-01,  2.8927e-01, -1.1849e+00, -4.5773e-01, -6.0599e-01,\n",
            "           9.6084e-01, -9.3197e-01,  7.7462e-01, -9.5885e-01,  4.1500e-01,\n",
            "          -3.5605e-01,  5.4656e-01, -9.4210e-02, -8.3026e-01,  2.1989e-01,\n",
            "          -8.4314e-01,  6.5872e-01, -1.1154e+00,  3.9677e-01, -4.0450e-01,\n",
            "           3.5118e-01,  4.0325e-01,  5.4963e-01,  8.0297e-01, -1.6678e+00,\n",
            "           9.0490e-01, -7.5773e-02,  4.8272e-01, -4.1663e-01, -1.4605e-01,\n",
            "          -2.8069e-01,  1.6233e+00,  1.1839e+00,  6.0073e-01,  8.1070e-01,\n",
            "          -7.0183e-01,  2.0120e-01, -6.0440e-02,  8.4979e-01, -4.8596e-01,\n",
            "           6.8533e-01, -2.3514e-01, -7.9586e-01, -1.0194e-01,  2.4729e+00,\n",
            "           6.7204e-01, -1.1681e+00,  5.3780e-01, -2.3080e-01,  2.9356e+00],\n",
            "         [-8.2487e-01, -1.1262e+00, -1.9905e+00, -5.7842e-01,  1.1442e+00,\n",
            "          -6.2344e-01,  2.7705e-01, -8.6612e-02, -5.5443e-01, -8.5329e-01,\n",
            "           1.0496e+00, -7.6195e-02, -2.3980e-01,  9.9222e-01,  2.1765e-01,\n",
            "          -3.4155e-01, -8.3250e-01,  6.1567e-01, -1.2990e+00, -5.5579e-01,\n",
            "          -6.0035e-01,  2.2196e-02, -1.9621e-02,  4.4595e-01,  1.4080e+00,\n",
            "           1.8696e+00,  6.0174e-02, -4.2096e-01, -1.9413e+00, -5.4189e-01,\n",
            "           1.3994e+00, -1.3803e+00,  9.5743e-01,  1.5859e+00, -3.0421e-01,\n",
            "          -1.1166e+00, -1.2672e+00,  2.1693e-01,  9.2169e-01,  1.1649e-01,\n",
            "          -1.3277e+00, -9.3043e-01, -3.1011e-01, -9.5287e-01, -1.1202e+00,\n",
            "           1.4757e-01,  5.4540e-01,  8.2831e-01,  7.9593e-01, -4.9837e-01,\n",
            "           1.3721e+00,  2.9394e-01, -4.0992e-01,  5.1450e-01,  7.6212e-01,\n",
            "          -5.5108e-01,  3.8882e-01,  6.6684e-01,  9.8516e-01,  4.4573e-01,\n",
            "           2.7061e-01,  1.8889e-01,  3.6972e-01,  1.4927e-01,  6.1661e-01]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "loss \n",
            "None\n",
            "\n",
            "logits 2   torch.Size([1, 65])\n",
            "tensor([[-0.8249, -1.1262, -1.9905, -0.5784,  1.1442, -0.6234,  0.2771, -0.0866,\n",
            "         -0.5544, -0.8533,  1.0496, -0.0762, -0.2398,  0.9922,  0.2177, -0.3416,\n",
            "         -0.8325,  0.6157, -1.2990, -0.5558, -0.6003,  0.0222, -0.0196,  0.4460,\n",
            "          1.4080,  1.8696,  0.0602, -0.4210, -1.9413, -0.5419,  1.3994, -1.3803,\n",
            "          0.9574,  1.5859, -0.3042, -1.1166, -1.2672,  0.2169,  0.9217,  0.1165,\n",
            "         -1.3277, -0.9304, -0.3101, -0.9529, -1.1202,  0.1476,  0.5454,  0.8283,\n",
            "          0.7959, -0.4984,  1.3721,  0.2939, -0.4099,  0.5145,  0.7621, -0.5511,\n",
            "          0.3888,  0.6668,  0.9852,  0.4457,  0.2706,  0.1889,  0.3697,  0.1493,\n",
            "          0.6166]], grad_fn=<SliceBackward0>)\n",
            "\n",
            "probs   torch.Size([1, 65])\n",
            "tensor([[0.0048, 0.0036, 0.0015, 0.0062, 0.0345, 0.0059, 0.0145, 0.0101, 0.0063,\n",
            "         0.0047, 0.0314, 0.0102, 0.0086, 0.0297, 0.0137, 0.0078, 0.0048, 0.0203,\n",
            "         0.0030, 0.0063, 0.0060, 0.0112, 0.0108, 0.0172, 0.0449, 0.0713, 0.0117,\n",
            "         0.0072, 0.0016, 0.0064, 0.0446, 0.0028, 0.0286, 0.0537, 0.0081, 0.0036,\n",
            "         0.0031, 0.0137, 0.0276, 0.0124, 0.0029, 0.0043, 0.0081, 0.0042, 0.0036,\n",
            "         0.0127, 0.0190, 0.0252, 0.0244, 0.0067, 0.0434, 0.0148, 0.0073, 0.0184,\n",
            "         0.0236, 0.0063, 0.0162, 0.0214, 0.0294, 0.0172, 0.0144, 0.0133, 0.0159,\n",
            "         0.0128, 0.0204]], grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "idx_next   torch.Size([1, 1])\n",
            "tensor([[63]])\n",
            "\n",
            "idx   torch.Size([1, 6])\n",
            "tensor([[ 0, 57, 25, 60, 59, 63]])\n",
            "\n",
            "****************************************************************************************************\n",
            "****************************************************************************************************\n",
            "logits 1   torch.Size([1, 6, 65])\n",
            "tensor([[[ 6.4022e-01, -8.3649e-01,  2.2870e+00, -6.3659e-01, -1.6597e+00,\n",
            "           9.9623e-01, -8.3188e-01,  5.2271e-01, -1.0720e+00, -6.9581e-01,\n",
            "          -1.7181e-01, -9.1379e-01,  3.7381e-01, -7.0926e-01, -2.8789e-01,\n",
            "          -1.3386e+00,  9.9227e-01, -3.6417e-01, -6.1489e-02,  2.7885e-01,\n",
            "          -1.6062e+00, -3.2556e-01,  2.2351e-01, -1.0779e+00, -7.9957e-01,\n",
            "          -8.6830e-03,  5.0956e-01,  2.0606e+00, -1.7745e+00,  3.1508e-01,\n",
            "           1.4927e+00,  1.0682e-02, -4.0964e-01, -2.1083e+00,  1.7071e-01,\n",
            "           4.3387e-01,  1.4519e+00, -2.2249e+00,  7.7063e-01,  1.0032e+00,\n",
            "          -7.0616e-04, -4.3484e-01, -1.9791e-01, -1.1886e+00, -3.1788e-01,\n",
            "          -8.7637e-01, -1.1614e+00,  2.7749e-01, -5.7510e-01, -4.2678e-01,\n",
            "           7.3345e-01, -3.2024e-01,  6.5886e-01,  1.7822e-01, -8.8296e-01,\n",
            "          -1.1789e+00,  2.4352e+00,  1.2554e+00, -1.6087e+00,  2.0354e-01,\n",
            "          -8.8759e-01,  8.7902e-02, -5.1729e-01, -8.0634e-03, -2.4454e-01],\n",
            "         [-3.8277e-01, -1.4797e+00, -9.3967e-01,  7.0841e-01, -4.7449e-01,\n",
            "          -1.1234e-01, -8.3979e-03,  1.2981e+00,  1.0284e+00,  1.9532e+00,\n",
            "           9.6351e-01, -5.5102e-01, -1.8094e-02, -1.1516e+00,  9.6337e-01,\n",
            "          -1.1422e+00,  4.2531e-01,  7.8829e-01,  1.5791e-01, -6.5050e-01,\n",
            "          -1.9515e+00, -5.1075e-01, -3.1796e-01, -1.2072e+00, -4.5217e-01,\n",
            "           1.1605e+00, -4.3852e-01,  1.3751e+00, -1.5350e-01, -5.4588e-01,\n",
            "           7.3537e-01, -1.9808e-01,  1.2885e+00, -8.1364e-01, -2.5151e-01,\n",
            "           1.0707e+00, -7.2009e-01, -2.2826e-01,  5.7055e-01,  2.8824e-01,\n",
            "          -4.4158e-01, -1.0114e+00, -5.8738e-02, -8.0512e-01, -1.2166e-01,\n",
            "           9.6103e-01, -3.1628e-01,  6.9615e-01,  1.2500e+00,  3.2730e-01,\n",
            "          -1.6830e+00, -1.0868e+00,  1.7973e-01, -4.9715e-01, -8.9743e-01,\n",
            "           2.3514e+00, -1.7441e+00,  2.9130e-01,  5.1169e-01, -3.0511e-01,\n",
            "          -5.1778e-01,  1.1511e+00,  1.0217e+00, -9.4185e-01,  1.3796e+00],\n",
            "         [-1.7727e+00, -4.7958e-01, -1.1037e+00,  1.5407e+00, -1.7011e+00,\n",
            "           5.4244e-01,  2.5793e-01, -6.4687e-01, -1.3273e+00, -8.0335e-01,\n",
            "          -2.0133e+00,  7.5757e-01,  1.4304e+00, -2.5731e+00,  1.6308e-01,\n",
            "           5.0361e-01, -1.6828e+00, -6.0351e-01,  3.2846e-01, -4.8570e-01,\n",
            "          -2.2001e-01,  7.2255e-01, -6.9377e-01, -5.0921e-01, -1.5880e-01,\n",
            "          -1.5865e-01, -1.0233e+00,  7.9247e-01, -2.2431e-01, -3.7015e-01,\n",
            "          -6.8938e-02,  7.1754e-01,  2.0767e-01,  5.7276e-01, -5.7819e-02,\n",
            "           1.9290e-02, -6.9943e-01, -1.6268e+00, -2.0428e+00,  4.1387e-01,\n",
            "           8.4179e-01,  6.8693e-01,  1.2607e-01, -1.0933e+00,  6.2169e-01,\n",
            "           4.7553e-01, -7.4559e-01, -8.4729e-01, -1.6935e-01, -7.9273e-01,\n",
            "           5.8127e-01,  3.8010e-01, -8.6380e-01,  4.6584e-01,  4.1352e-01,\n",
            "           1.2824e+00, -3.3220e-01,  2.2574e+00,  7.1990e-02, -7.0376e-03,\n",
            "           1.7284e+00, -1.2540e-01, -1.2134e+00,  1.2877e+00, -5.9620e-01],\n",
            "         [-7.9098e-01, -1.2310e+00,  9.0967e-01,  2.3525e+00,  1.2958e+00,\n",
            "           2.4102e-01,  5.4828e-01,  3.4034e-01, -8.4229e-01, -1.6776e+00,\n",
            "          -5.7521e-01,  1.7487e-01, -4.0092e-01,  1.0710e+00, -1.5919e+00,\n",
            "          -1.9438e-01,  2.8927e-01, -1.1849e+00, -4.5773e-01, -6.0599e-01,\n",
            "           9.6084e-01, -9.3197e-01,  7.7462e-01, -9.5885e-01,  4.1500e-01,\n",
            "          -3.5605e-01,  5.4656e-01, -9.4210e-02, -8.3026e-01,  2.1989e-01,\n",
            "          -8.4314e-01,  6.5872e-01, -1.1154e+00,  3.9677e-01, -4.0450e-01,\n",
            "           3.5118e-01,  4.0325e-01,  5.4963e-01,  8.0297e-01, -1.6678e+00,\n",
            "           9.0490e-01, -7.5773e-02,  4.8272e-01, -4.1663e-01, -1.4605e-01,\n",
            "          -2.8069e-01,  1.6233e+00,  1.1839e+00,  6.0073e-01,  8.1070e-01,\n",
            "          -7.0183e-01,  2.0120e-01, -6.0440e-02,  8.4979e-01, -4.8596e-01,\n",
            "           6.8533e-01, -2.3514e-01, -7.9586e-01, -1.0194e-01,  2.4729e+00,\n",
            "           6.7204e-01, -1.1681e+00,  5.3780e-01, -2.3080e-01,  2.9356e+00],\n",
            "         [-8.2487e-01, -1.1262e+00, -1.9905e+00, -5.7842e-01,  1.1442e+00,\n",
            "          -6.2344e-01,  2.7705e-01, -8.6612e-02, -5.5443e-01, -8.5329e-01,\n",
            "           1.0496e+00, -7.6195e-02, -2.3980e-01,  9.9222e-01,  2.1765e-01,\n",
            "          -3.4155e-01, -8.3250e-01,  6.1567e-01, -1.2990e+00, -5.5579e-01,\n",
            "          -6.0035e-01,  2.2196e-02, -1.9621e-02,  4.4595e-01,  1.4080e+00,\n",
            "           1.8696e+00,  6.0174e-02, -4.2096e-01, -1.9413e+00, -5.4189e-01,\n",
            "           1.3994e+00, -1.3803e+00,  9.5743e-01,  1.5859e+00, -3.0421e-01,\n",
            "          -1.1166e+00, -1.2672e+00,  2.1693e-01,  9.2169e-01,  1.1649e-01,\n",
            "          -1.3277e+00, -9.3043e-01, -3.1011e-01, -9.5287e-01, -1.1202e+00,\n",
            "           1.4757e-01,  5.4540e-01,  8.2831e-01,  7.9593e-01, -4.9837e-01,\n",
            "           1.3721e+00,  2.9394e-01, -4.0992e-01,  5.1450e-01,  7.6212e-01,\n",
            "          -5.5108e-01,  3.8882e-01,  6.6684e-01,  9.8516e-01,  4.4573e-01,\n",
            "           2.7061e-01,  1.8889e-01,  3.6972e-01,  1.4927e-01,  6.1661e-01],\n",
            "         [-2.4758e-01, -1.2174e+00, -8.4225e-01,  7.3758e-02, -2.0748e-01,\n",
            "           5.3647e-01,  1.1809e+00,  1.6795e+00,  1.5323e-01, -1.8717e+00,\n",
            "           2.2346e+00, -6.8693e-01, -2.1119e-01, -1.4324e+00,  8.9226e-01,\n",
            "           1.4940e+00,  1.2984e-01, -5.8113e-01, -2.9294e-01, -1.8348e+00,\n",
            "          -2.4149e-01, -2.4143e-01,  4.4863e-01, -5.1218e-01,  8.4404e-01,\n",
            "          -1.3255e-01, -3.8060e-01,  1.6357e+00,  1.4380e+00,  7.6055e-01,\n",
            "           1.8211e-01,  4.9161e-01, -1.7092e+00,  6.3299e-01, -9.2653e-01,\n",
            "          -1.1860e+00,  8.1085e-01, -6.2301e-01, -6.9638e-01,  2.8001e-01,\n",
            "          -5.3669e-01,  7.3155e-01, -3.5016e-01,  4.5811e-01,  1.0904e-01,\n",
            "          -7.0566e-01,  6.9015e-01,  1.8711e+00, -2.0947e+00,  1.2399e-01,\n",
            "           1.2801e+00,  1.0601e-01,  1.9418e+00,  1.3580e+00, -1.2633e+00,\n",
            "          -7.0420e-01, -4.3899e-01,  5.0483e-01,  1.0862e+00,  3.4550e-01,\n",
            "          -6.0370e-01, -2.9158e-01, -6.2283e-01, -1.3244e-01, -2.6001e+00]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "loss \n",
            "None\n",
            "\n",
            "logits 2   torch.Size([1, 65])\n",
            "tensor([[-0.2476, -1.2174, -0.8423,  0.0738, -0.2075,  0.5365,  1.1809,  1.6795,\n",
            "          0.1532, -1.8717,  2.2346, -0.6869, -0.2112, -1.4324,  0.8923,  1.4940,\n",
            "          0.1298, -0.5811, -0.2929, -1.8348, -0.2415, -0.2414,  0.4486, -0.5122,\n",
            "          0.8440, -0.1325, -0.3806,  1.6357,  1.4380,  0.7605,  0.1821,  0.4916,\n",
            "         -1.7092,  0.6330, -0.9265, -1.1860,  0.8108, -0.6230, -0.6964,  0.2800,\n",
            "         -0.5367,  0.7315, -0.3502,  0.4581,  0.1090, -0.7057,  0.6901,  1.8711,\n",
            "         -2.0947,  0.1240,  1.2801,  0.1060,  1.9418,  1.3580, -1.2633, -0.7042,\n",
            "         -0.4390,  0.5048,  1.0862,  0.3455, -0.6037, -0.2916, -0.6228, -0.1324,\n",
            "         -2.6001]], grad_fn=<SliceBackward0>)\n",
            "\n",
            "probs   torch.Size([1, 65])\n",
            "tensor([[0.0073, 0.0028, 0.0040, 0.0101, 0.0076, 0.0160, 0.0305, 0.0502, 0.0109,\n",
            "         0.0014, 0.0874, 0.0047, 0.0076, 0.0022, 0.0228, 0.0417, 0.0107, 0.0052,\n",
            "         0.0070, 0.0015, 0.0074, 0.0074, 0.0147, 0.0056, 0.0218, 0.0082, 0.0064,\n",
            "         0.0480, 0.0394, 0.0200, 0.0112, 0.0153, 0.0017, 0.0176, 0.0037, 0.0029,\n",
            "         0.0211, 0.0050, 0.0047, 0.0124, 0.0055, 0.0194, 0.0066, 0.0148, 0.0104,\n",
            "         0.0046, 0.0187, 0.0608, 0.0012, 0.0106, 0.0337, 0.0104, 0.0652, 0.0364,\n",
            "         0.0026, 0.0046, 0.0060, 0.0155, 0.0277, 0.0132, 0.0051, 0.0070, 0.0050,\n",
            "         0.0082, 0.0007]], grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "idx_next   torch.Size([1, 1])\n",
            "tensor([[10]])\n",
            "\n",
            "idx   torch.Size([1, 7])\n",
            "tensor([[ 0, 57, 25, 60, 59, 63, 10]])\n",
            "\n",
            "****************************************************************************************************\n",
            "****************************************************************************************************\n",
            "logits 1   torch.Size([1, 7, 65])\n",
            "tensor([[[ 6.4022e-01, -8.3649e-01,  2.2870e+00, -6.3659e-01, -1.6597e+00,\n",
            "           9.9623e-01, -8.3188e-01,  5.2271e-01, -1.0720e+00, -6.9581e-01,\n",
            "          -1.7181e-01, -9.1379e-01,  3.7381e-01, -7.0926e-01, -2.8789e-01,\n",
            "          -1.3386e+00,  9.9227e-01, -3.6417e-01, -6.1489e-02,  2.7885e-01,\n",
            "          -1.6062e+00, -3.2556e-01,  2.2351e-01, -1.0779e+00, -7.9957e-01,\n",
            "          -8.6830e-03,  5.0956e-01,  2.0606e+00, -1.7745e+00,  3.1508e-01,\n",
            "           1.4927e+00,  1.0682e-02, -4.0964e-01, -2.1083e+00,  1.7071e-01,\n",
            "           4.3387e-01,  1.4519e+00, -2.2249e+00,  7.7063e-01,  1.0032e+00,\n",
            "          -7.0616e-04, -4.3484e-01, -1.9791e-01, -1.1886e+00, -3.1788e-01,\n",
            "          -8.7637e-01, -1.1614e+00,  2.7749e-01, -5.7510e-01, -4.2678e-01,\n",
            "           7.3345e-01, -3.2024e-01,  6.5886e-01,  1.7822e-01, -8.8296e-01,\n",
            "          -1.1789e+00,  2.4352e+00,  1.2554e+00, -1.6087e+00,  2.0354e-01,\n",
            "          -8.8759e-01,  8.7902e-02, -5.1729e-01, -8.0634e-03, -2.4454e-01],\n",
            "         [-3.8277e-01, -1.4797e+00, -9.3967e-01,  7.0841e-01, -4.7449e-01,\n",
            "          -1.1234e-01, -8.3979e-03,  1.2981e+00,  1.0284e+00,  1.9532e+00,\n",
            "           9.6351e-01, -5.5102e-01, -1.8094e-02, -1.1516e+00,  9.6337e-01,\n",
            "          -1.1422e+00,  4.2531e-01,  7.8829e-01,  1.5791e-01, -6.5050e-01,\n",
            "          -1.9515e+00, -5.1075e-01, -3.1796e-01, -1.2072e+00, -4.5217e-01,\n",
            "           1.1605e+00, -4.3852e-01,  1.3751e+00, -1.5350e-01, -5.4588e-01,\n",
            "           7.3537e-01, -1.9808e-01,  1.2885e+00, -8.1364e-01, -2.5151e-01,\n",
            "           1.0707e+00, -7.2009e-01, -2.2826e-01,  5.7055e-01,  2.8824e-01,\n",
            "          -4.4158e-01, -1.0114e+00, -5.8738e-02, -8.0512e-01, -1.2166e-01,\n",
            "           9.6103e-01, -3.1628e-01,  6.9615e-01,  1.2500e+00,  3.2730e-01,\n",
            "          -1.6830e+00, -1.0868e+00,  1.7973e-01, -4.9715e-01, -8.9743e-01,\n",
            "           2.3514e+00, -1.7441e+00,  2.9130e-01,  5.1169e-01, -3.0511e-01,\n",
            "          -5.1778e-01,  1.1511e+00,  1.0217e+00, -9.4185e-01,  1.3796e+00],\n",
            "         [-1.7727e+00, -4.7958e-01, -1.1037e+00,  1.5407e+00, -1.7011e+00,\n",
            "           5.4244e-01,  2.5793e-01, -6.4687e-01, -1.3273e+00, -8.0335e-01,\n",
            "          -2.0133e+00,  7.5757e-01,  1.4304e+00, -2.5731e+00,  1.6308e-01,\n",
            "           5.0361e-01, -1.6828e+00, -6.0351e-01,  3.2846e-01, -4.8570e-01,\n",
            "          -2.2001e-01,  7.2255e-01, -6.9377e-01, -5.0921e-01, -1.5880e-01,\n",
            "          -1.5865e-01, -1.0233e+00,  7.9247e-01, -2.2431e-01, -3.7015e-01,\n",
            "          -6.8938e-02,  7.1754e-01,  2.0767e-01,  5.7276e-01, -5.7819e-02,\n",
            "           1.9290e-02, -6.9943e-01, -1.6268e+00, -2.0428e+00,  4.1387e-01,\n",
            "           8.4179e-01,  6.8693e-01,  1.2607e-01, -1.0933e+00,  6.2169e-01,\n",
            "           4.7553e-01, -7.4559e-01, -8.4729e-01, -1.6935e-01, -7.9273e-01,\n",
            "           5.8127e-01,  3.8010e-01, -8.6380e-01,  4.6584e-01,  4.1352e-01,\n",
            "           1.2824e+00, -3.3220e-01,  2.2574e+00,  7.1990e-02, -7.0376e-03,\n",
            "           1.7284e+00, -1.2540e-01, -1.2134e+00,  1.2877e+00, -5.9620e-01],\n",
            "         [-7.9098e-01, -1.2310e+00,  9.0967e-01,  2.3525e+00,  1.2958e+00,\n",
            "           2.4102e-01,  5.4828e-01,  3.4034e-01, -8.4229e-01, -1.6776e+00,\n",
            "          -5.7521e-01,  1.7487e-01, -4.0092e-01,  1.0710e+00, -1.5919e+00,\n",
            "          -1.9438e-01,  2.8927e-01, -1.1849e+00, -4.5773e-01, -6.0599e-01,\n",
            "           9.6084e-01, -9.3197e-01,  7.7462e-01, -9.5885e-01,  4.1500e-01,\n",
            "          -3.5605e-01,  5.4656e-01, -9.4210e-02, -8.3026e-01,  2.1989e-01,\n",
            "          -8.4314e-01,  6.5872e-01, -1.1154e+00,  3.9677e-01, -4.0450e-01,\n",
            "           3.5118e-01,  4.0325e-01,  5.4963e-01,  8.0297e-01, -1.6678e+00,\n",
            "           9.0490e-01, -7.5773e-02,  4.8272e-01, -4.1663e-01, -1.4605e-01,\n",
            "          -2.8069e-01,  1.6233e+00,  1.1839e+00,  6.0073e-01,  8.1070e-01,\n",
            "          -7.0183e-01,  2.0120e-01, -6.0440e-02,  8.4979e-01, -4.8596e-01,\n",
            "           6.8533e-01, -2.3514e-01, -7.9586e-01, -1.0194e-01,  2.4729e+00,\n",
            "           6.7204e-01, -1.1681e+00,  5.3780e-01, -2.3080e-01,  2.9356e+00],\n",
            "         [-8.2487e-01, -1.1262e+00, -1.9905e+00, -5.7842e-01,  1.1442e+00,\n",
            "          -6.2344e-01,  2.7705e-01, -8.6612e-02, -5.5443e-01, -8.5329e-01,\n",
            "           1.0496e+00, -7.6195e-02, -2.3980e-01,  9.9222e-01,  2.1765e-01,\n",
            "          -3.4155e-01, -8.3250e-01,  6.1567e-01, -1.2990e+00, -5.5579e-01,\n",
            "          -6.0035e-01,  2.2196e-02, -1.9621e-02,  4.4595e-01,  1.4080e+00,\n",
            "           1.8696e+00,  6.0174e-02, -4.2096e-01, -1.9413e+00, -5.4189e-01,\n",
            "           1.3994e+00, -1.3803e+00,  9.5743e-01,  1.5859e+00, -3.0421e-01,\n",
            "          -1.1166e+00, -1.2672e+00,  2.1693e-01,  9.2169e-01,  1.1649e-01,\n",
            "          -1.3277e+00, -9.3043e-01, -3.1011e-01, -9.5287e-01, -1.1202e+00,\n",
            "           1.4757e-01,  5.4540e-01,  8.2831e-01,  7.9593e-01, -4.9837e-01,\n",
            "           1.3721e+00,  2.9394e-01, -4.0992e-01,  5.1450e-01,  7.6212e-01,\n",
            "          -5.5108e-01,  3.8882e-01,  6.6684e-01,  9.8516e-01,  4.4573e-01,\n",
            "           2.7061e-01,  1.8889e-01,  3.6972e-01,  1.4927e-01,  6.1661e-01],\n",
            "         [-2.4758e-01, -1.2174e+00, -8.4225e-01,  7.3758e-02, -2.0748e-01,\n",
            "           5.3647e-01,  1.1809e+00,  1.6795e+00,  1.5323e-01, -1.8717e+00,\n",
            "           2.2346e+00, -6.8693e-01, -2.1119e-01, -1.4324e+00,  8.9226e-01,\n",
            "           1.4940e+00,  1.2984e-01, -5.8113e-01, -2.9294e-01, -1.8348e+00,\n",
            "          -2.4149e-01, -2.4143e-01,  4.4863e-01, -5.1218e-01,  8.4404e-01,\n",
            "          -1.3255e-01, -3.8060e-01,  1.6357e+00,  1.4380e+00,  7.6055e-01,\n",
            "           1.8211e-01,  4.9161e-01, -1.7092e+00,  6.3299e-01, -9.2653e-01,\n",
            "          -1.1860e+00,  8.1085e-01, -6.2301e-01, -6.9638e-01,  2.8001e-01,\n",
            "          -5.3669e-01,  7.3155e-01, -3.5016e-01,  4.5811e-01,  1.0904e-01,\n",
            "          -7.0566e-01,  6.9015e-01,  1.8711e+00, -2.0947e+00,  1.2399e-01,\n",
            "           1.2801e+00,  1.0601e-01,  1.9418e+00,  1.3580e+00, -1.2633e+00,\n",
            "          -7.0420e-01, -4.3899e-01,  5.0483e-01,  1.0862e+00,  3.4550e-01,\n",
            "          -6.0370e-01, -2.9158e-01, -6.2283e-01, -1.3244e-01, -2.6001e+00],\n",
            "         [ 8.2353e-01,  5.5253e-01,  5.0815e-01,  1.3908e+00,  8.8032e-01,\n",
            "           3.1026e-01, -3.5374e-01,  8.1965e-04, -3.5488e-01, -3.5108e-01,\n",
            "          -1.1202e+00, -1.2050e+00,  1.1561e+00, -3.2138e-02,  2.0929e-02,\n",
            "           2.0213e+00,  1.0372e+00,  9.6509e-01,  1.1448e-01, -5.7509e-01,\n",
            "          -8.5201e-01,  4.9056e-01, -1.2741e+00, -1.2460e+00,  1.7684e+00,\n",
            "          -5.6799e-01, -2.0801e+00,  9.2900e-01, -6.1813e-02,  6.0497e-01,\n",
            "          -2.6942e-01,  1.9213e-01, -6.1834e-01, -7.7854e-01,  7.6267e-01,\n",
            "           8.5272e-01, -2.8700e-01, -1.9288e-01,  9.6486e-01, -6.9171e-01,\n",
            "           2.4486e-01, -4.8010e-01,  2.1298e-01,  4.2678e-01, -9.9015e-01,\n",
            "           3.3416e-01, -2.7164e-01,  7.0549e-01,  2.8121e+00,  4.1596e-01,\n",
            "          -3.2185e-01,  3.0605e-01, -1.0644e+00, -6.5521e-01, -1.4503e+00,\n",
            "          -7.0973e-01,  1.9785e+00,  1.1422e+00,  4.5719e-01,  1.0731e-01,\n",
            "          -7.8540e-01,  3.0176e-01,  1.9352e+00, -8.6118e-01,  1.3755e+00]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "loss \n",
            "None\n",
            "\n",
            "logits 2   torch.Size([1, 65])\n",
            "tensor([[ 8.2353e-01,  5.5253e-01,  5.0815e-01,  1.3908e+00,  8.8032e-01,\n",
            "          3.1026e-01, -3.5374e-01,  8.1965e-04, -3.5488e-01, -3.5108e-01,\n",
            "         -1.1202e+00, -1.2050e+00,  1.1561e+00, -3.2138e-02,  2.0929e-02,\n",
            "          2.0213e+00,  1.0372e+00,  9.6509e-01,  1.1448e-01, -5.7509e-01,\n",
            "         -8.5201e-01,  4.9056e-01, -1.2741e+00, -1.2460e+00,  1.7684e+00,\n",
            "         -5.6799e-01, -2.0801e+00,  9.2900e-01, -6.1813e-02,  6.0497e-01,\n",
            "         -2.6942e-01,  1.9213e-01, -6.1834e-01, -7.7854e-01,  7.6267e-01,\n",
            "          8.5272e-01, -2.8700e-01, -1.9288e-01,  9.6486e-01, -6.9171e-01,\n",
            "          2.4486e-01, -4.8010e-01,  2.1298e-01,  4.2678e-01, -9.9015e-01,\n",
            "          3.3416e-01, -2.7164e-01,  7.0549e-01,  2.8121e+00,  4.1596e-01,\n",
            "         -3.2185e-01,  3.0605e-01, -1.0644e+00, -6.5521e-01, -1.4503e+00,\n",
            "         -7.0973e-01,  1.9785e+00,  1.1422e+00,  4.5719e-01,  1.0731e-01,\n",
            "         -7.8540e-01,  3.0176e-01,  1.9352e+00, -8.6118e-01,  1.3755e+00]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "probs   torch.Size([1, 65])\n",
            "tensor([[0.0188, 0.0144, 0.0137, 0.0332, 0.0199, 0.0113, 0.0058, 0.0083, 0.0058,\n",
            "         0.0058, 0.0027, 0.0025, 0.0263, 0.0080, 0.0084, 0.0624, 0.0233, 0.0217,\n",
            "         0.0093, 0.0047, 0.0035, 0.0135, 0.0023, 0.0024, 0.0485, 0.0047, 0.0010,\n",
            "         0.0209, 0.0078, 0.0151, 0.0063, 0.0100, 0.0045, 0.0038, 0.0177, 0.0194,\n",
            "         0.0062, 0.0068, 0.0217, 0.0041, 0.0106, 0.0051, 0.0102, 0.0127, 0.0031,\n",
            "         0.0116, 0.0063, 0.0167, 0.1376, 0.0125, 0.0060, 0.0112, 0.0029, 0.0043,\n",
            "         0.0019, 0.0041, 0.0598, 0.0259, 0.0131, 0.0092, 0.0038, 0.0112, 0.0573,\n",
            "         0.0035, 0.0327]], grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "idx_next   torch.Size([1, 1])\n",
            "tensor([[48]])\n",
            "\n",
            "idx   torch.Size([1, 8])\n",
            "tensor([[ 0, 57, 25, 60, 59, 63, 10, 48]])\n",
            "\n",
            "****************************************************************************************************\n",
            "****************************************************************************************************\n",
            "logits 1   torch.Size([1, 8, 65])\n",
            "tensor([[[ 6.4022e-01, -8.3649e-01,  2.2870e+00, -6.3659e-01, -1.6597e+00,\n",
            "           9.9623e-01, -8.3188e-01,  5.2271e-01, -1.0720e+00, -6.9581e-01,\n",
            "          -1.7181e-01, -9.1379e-01,  3.7381e-01, -7.0926e-01, -2.8789e-01,\n",
            "          -1.3386e+00,  9.9227e-01, -3.6417e-01, -6.1489e-02,  2.7885e-01,\n",
            "          -1.6062e+00, -3.2556e-01,  2.2351e-01, -1.0779e+00, -7.9957e-01,\n",
            "          -8.6830e-03,  5.0956e-01,  2.0606e+00, -1.7745e+00,  3.1508e-01,\n",
            "           1.4927e+00,  1.0682e-02, -4.0964e-01, -2.1083e+00,  1.7071e-01,\n",
            "           4.3387e-01,  1.4519e+00, -2.2249e+00,  7.7063e-01,  1.0032e+00,\n",
            "          -7.0616e-04, -4.3484e-01, -1.9791e-01, -1.1886e+00, -3.1788e-01,\n",
            "          -8.7637e-01, -1.1614e+00,  2.7749e-01, -5.7510e-01, -4.2678e-01,\n",
            "           7.3345e-01, -3.2024e-01,  6.5886e-01,  1.7822e-01, -8.8296e-01,\n",
            "          -1.1789e+00,  2.4352e+00,  1.2554e+00, -1.6087e+00,  2.0354e-01,\n",
            "          -8.8759e-01,  8.7902e-02, -5.1729e-01, -8.0634e-03, -2.4454e-01],\n",
            "         [-3.8277e-01, -1.4797e+00, -9.3967e-01,  7.0841e-01, -4.7449e-01,\n",
            "          -1.1234e-01, -8.3979e-03,  1.2981e+00,  1.0284e+00,  1.9532e+00,\n",
            "           9.6351e-01, -5.5102e-01, -1.8094e-02, -1.1516e+00,  9.6337e-01,\n",
            "          -1.1422e+00,  4.2531e-01,  7.8829e-01,  1.5791e-01, -6.5050e-01,\n",
            "          -1.9515e+00, -5.1075e-01, -3.1796e-01, -1.2072e+00, -4.5217e-01,\n",
            "           1.1605e+00, -4.3852e-01,  1.3751e+00, -1.5350e-01, -5.4588e-01,\n",
            "           7.3537e-01, -1.9808e-01,  1.2885e+00, -8.1364e-01, -2.5151e-01,\n",
            "           1.0707e+00, -7.2009e-01, -2.2826e-01,  5.7055e-01,  2.8824e-01,\n",
            "          -4.4158e-01, -1.0114e+00, -5.8738e-02, -8.0512e-01, -1.2166e-01,\n",
            "           9.6103e-01, -3.1628e-01,  6.9615e-01,  1.2500e+00,  3.2730e-01,\n",
            "          -1.6830e+00, -1.0868e+00,  1.7973e-01, -4.9715e-01, -8.9743e-01,\n",
            "           2.3514e+00, -1.7441e+00,  2.9130e-01,  5.1169e-01, -3.0511e-01,\n",
            "          -5.1778e-01,  1.1511e+00,  1.0217e+00, -9.4185e-01,  1.3796e+00],\n",
            "         [-1.7727e+00, -4.7958e-01, -1.1037e+00,  1.5407e+00, -1.7011e+00,\n",
            "           5.4244e-01,  2.5793e-01, -6.4687e-01, -1.3273e+00, -8.0335e-01,\n",
            "          -2.0133e+00,  7.5757e-01,  1.4304e+00, -2.5731e+00,  1.6308e-01,\n",
            "           5.0361e-01, -1.6828e+00, -6.0351e-01,  3.2846e-01, -4.8570e-01,\n",
            "          -2.2001e-01,  7.2255e-01, -6.9377e-01, -5.0921e-01, -1.5880e-01,\n",
            "          -1.5865e-01, -1.0233e+00,  7.9247e-01, -2.2431e-01, -3.7015e-01,\n",
            "          -6.8938e-02,  7.1754e-01,  2.0767e-01,  5.7276e-01, -5.7819e-02,\n",
            "           1.9290e-02, -6.9943e-01, -1.6268e+00, -2.0428e+00,  4.1387e-01,\n",
            "           8.4179e-01,  6.8693e-01,  1.2607e-01, -1.0933e+00,  6.2169e-01,\n",
            "           4.7553e-01, -7.4559e-01, -8.4729e-01, -1.6935e-01, -7.9273e-01,\n",
            "           5.8127e-01,  3.8010e-01, -8.6380e-01,  4.6584e-01,  4.1352e-01,\n",
            "           1.2824e+00, -3.3220e-01,  2.2574e+00,  7.1990e-02, -7.0376e-03,\n",
            "           1.7284e+00, -1.2540e-01, -1.2134e+00,  1.2877e+00, -5.9620e-01],\n",
            "         [-7.9098e-01, -1.2310e+00,  9.0967e-01,  2.3525e+00,  1.2958e+00,\n",
            "           2.4102e-01,  5.4828e-01,  3.4034e-01, -8.4229e-01, -1.6776e+00,\n",
            "          -5.7521e-01,  1.7487e-01, -4.0092e-01,  1.0710e+00, -1.5919e+00,\n",
            "          -1.9438e-01,  2.8927e-01, -1.1849e+00, -4.5773e-01, -6.0599e-01,\n",
            "           9.6084e-01, -9.3197e-01,  7.7462e-01, -9.5885e-01,  4.1500e-01,\n",
            "          -3.5605e-01,  5.4656e-01, -9.4210e-02, -8.3026e-01,  2.1989e-01,\n",
            "          -8.4314e-01,  6.5872e-01, -1.1154e+00,  3.9677e-01, -4.0450e-01,\n",
            "           3.5118e-01,  4.0325e-01,  5.4963e-01,  8.0297e-01, -1.6678e+00,\n",
            "           9.0490e-01, -7.5773e-02,  4.8272e-01, -4.1663e-01, -1.4605e-01,\n",
            "          -2.8069e-01,  1.6233e+00,  1.1839e+00,  6.0073e-01,  8.1070e-01,\n",
            "          -7.0183e-01,  2.0120e-01, -6.0440e-02,  8.4979e-01, -4.8596e-01,\n",
            "           6.8533e-01, -2.3514e-01, -7.9586e-01, -1.0194e-01,  2.4729e+00,\n",
            "           6.7204e-01, -1.1681e+00,  5.3780e-01, -2.3080e-01,  2.9356e+00],\n",
            "         [-8.2487e-01, -1.1262e+00, -1.9905e+00, -5.7842e-01,  1.1442e+00,\n",
            "          -6.2344e-01,  2.7705e-01, -8.6612e-02, -5.5443e-01, -8.5329e-01,\n",
            "           1.0496e+00, -7.6195e-02, -2.3980e-01,  9.9222e-01,  2.1765e-01,\n",
            "          -3.4155e-01, -8.3250e-01,  6.1567e-01, -1.2990e+00, -5.5579e-01,\n",
            "          -6.0035e-01,  2.2196e-02, -1.9621e-02,  4.4595e-01,  1.4080e+00,\n",
            "           1.8696e+00,  6.0174e-02, -4.2096e-01, -1.9413e+00, -5.4189e-01,\n",
            "           1.3994e+00, -1.3803e+00,  9.5743e-01,  1.5859e+00, -3.0421e-01,\n",
            "          -1.1166e+00, -1.2672e+00,  2.1693e-01,  9.2169e-01,  1.1649e-01,\n",
            "          -1.3277e+00, -9.3043e-01, -3.1011e-01, -9.5287e-01, -1.1202e+00,\n",
            "           1.4757e-01,  5.4540e-01,  8.2831e-01,  7.9593e-01, -4.9837e-01,\n",
            "           1.3721e+00,  2.9394e-01, -4.0992e-01,  5.1450e-01,  7.6212e-01,\n",
            "          -5.5108e-01,  3.8882e-01,  6.6684e-01,  9.8516e-01,  4.4573e-01,\n",
            "           2.7061e-01,  1.8889e-01,  3.6972e-01,  1.4927e-01,  6.1661e-01],\n",
            "         [-2.4758e-01, -1.2174e+00, -8.4225e-01,  7.3758e-02, -2.0748e-01,\n",
            "           5.3647e-01,  1.1809e+00,  1.6795e+00,  1.5323e-01, -1.8717e+00,\n",
            "           2.2346e+00, -6.8693e-01, -2.1119e-01, -1.4324e+00,  8.9226e-01,\n",
            "           1.4940e+00,  1.2984e-01, -5.8113e-01, -2.9294e-01, -1.8348e+00,\n",
            "          -2.4149e-01, -2.4143e-01,  4.4863e-01, -5.1218e-01,  8.4404e-01,\n",
            "          -1.3255e-01, -3.8060e-01,  1.6357e+00,  1.4380e+00,  7.6055e-01,\n",
            "           1.8211e-01,  4.9161e-01, -1.7092e+00,  6.3299e-01, -9.2653e-01,\n",
            "          -1.1860e+00,  8.1085e-01, -6.2301e-01, -6.9638e-01,  2.8001e-01,\n",
            "          -5.3669e-01,  7.3155e-01, -3.5016e-01,  4.5811e-01,  1.0904e-01,\n",
            "          -7.0566e-01,  6.9015e-01,  1.8711e+00, -2.0947e+00,  1.2399e-01,\n",
            "           1.2801e+00,  1.0601e-01,  1.9418e+00,  1.3580e+00, -1.2633e+00,\n",
            "          -7.0420e-01, -4.3899e-01,  5.0483e-01,  1.0862e+00,  3.4550e-01,\n",
            "          -6.0370e-01, -2.9158e-01, -6.2283e-01, -1.3244e-01, -2.6001e+00],\n",
            "         [ 8.2353e-01,  5.5253e-01,  5.0815e-01,  1.3908e+00,  8.8032e-01,\n",
            "           3.1026e-01, -3.5374e-01,  8.1965e-04, -3.5488e-01, -3.5108e-01,\n",
            "          -1.1202e+00, -1.2050e+00,  1.1561e+00, -3.2138e-02,  2.0929e-02,\n",
            "           2.0213e+00,  1.0372e+00,  9.6509e-01,  1.1448e-01, -5.7509e-01,\n",
            "          -8.5201e-01,  4.9056e-01, -1.2741e+00, -1.2460e+00,  1.7684e+00,\n",
            "          -5.6799e-01, -2.0801e+00,  9.2900e-01, -6.1813e-02,  6.0497e-01,\n",
            "          -2.6942e-01,  1.9213e-01, -6.1834e-01, -7.7854e-01,  7.6267e-01,\n",
            "           8.5272e-01, -2.8700e-01, -1.9288e-01,  9.6486e-01, -6.9171e-01,\n",
            "           2.4486e-01, -4.8010e-01,  2.1298e-01,  4.2678e-01, -9.9015e-01,\n",
            "           3.3416e-01, -2.7164e-01,  7.0549e-01,  2.8121e+00,  4.1596e-01,\n",
            "          -3.2185e-01,  3.0605e-01, -1.0644e+00, -6.5521e-01, -1.4503e+00,\n",
            "          -7.0973e-01,  1.9785e+00,  1.1422e+00,  4.5719e-01,  1.0731e-01,\n",
            "          -7.8540e-01,  3.0176e-01,  1.9352e+00, -8.6118e-01,  1.3755e+00],\n",
            "         [-5.7075e-01, -3.8165e-01, -8.9146e-01, -6.8085e-01, -1.2459e+00,\n",
            "           7.2844e-01,  1.0182e+00,  1.5140e-02,  2.0107e-01, -1.1911e+00,\n",
            "          -5.5425e-01, -3.2881e-01,  1.8214e+00,  1.9537e+00, -8.4760e-02,\n",
            "           6.7746e-01,  4.7121e-01, -1.4204e+00,  5.8264e-01, -1.2894e+00,\n",
            "           1.5084e-02, -5.6652e-01,  4.3420e-02, -1.5474e+00, -4.9463e-02,\n",
            "           9.9222e-02,  8.3802e-01, -1.6525e-02, -1.4293e+00,  3.6902e-01,\n",
            "          -3.3776e-01,  5.6823e-01, -4.7986e-01, -6.4255e-02,  4.5620e-01,\n",
            "          -4.4511e-01, -4.5951e-01, -7.2753e-01,  4.2803e-01,  1.5092e+00,\n",
            "           6.1540e-02, -7.6555e-01,  3.1808e-01,  8.4000e-01,  8.6496e-01,\n",
            "           1.5870e+00,  3.1308e-01,  6.7870e-02,  1.3656e+00,  1.6123e+00,\n",
            "           1.6907e+00, -8.0375e-01, -4.3442e-01,  8.9316e-01, -2.9539e-01,\n",
            "           4.3919e-01,  8.0265e-01,  1.2316e+00, -4.7660e-02,  7.2945e-01,\n",
            "           2.1887e+00,  4.0312e-01, -2.6475e-01,  2.1325e-01, -1.0569e-01]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "loss \n",
            "None\n",
            "\n",
            "logits 2   torch.Size([1, 65])\n",
            "tensor([[-0.5707, -0.3817, -0.8915, -0.6808, -1.2459,  0.7284,  1.0182,  0.0151,\n",
            "          0.2011, -1.1911, -0.5543, -0.3288,  1.8214,  1.9537, -0.0848,  0.6775,\n",
            "          0.4712, -1.4204,  0.5826, -1.2894,  0.0151, -0.5665,  0.0434, -1.5474,\n",
            "         -0.0495,  0.0992,  0.8380, -0.0165, -1.4293,  0.3690, -0.3378,  0.5682,\n",
            "         -0.4799, -0.0643,  0.4562, -0.4451, -0.4595, -0.7275,  0.4280,  1.5092,\n",
            "          0.0615, -0.7655,  0.3181,  0.8400,  0.8650,  1.5870,  0.3131,  0.0679,\n",
            "          1.3656,  1.6123,  1.6907, -0.8037, -0.4344,  0.8932, -0.2954,  0.4392,\n",
            "          0.8026,  1.2316, -0.0477,  0.7294,  2.1887,  0.4031, -0.2648,  0.2133,\n",
            "         -0.1057]], grad_fn=<SliceBackward0>)\n",
            "\n",
            "probs   torch.Size([1, 65])\n",
            "tensor([[0.0051, 0.0061, 0.0037, 0.0045, 0.0026, 0.0186, 0.0248, 0.0091, 0.0110,\n",
            "         0.0027, 0.0051, 0.0065, 0.0554, 0.0632, 0.0082, 0.0176, 0.0144, 0.0022,\n",
            "         0.0160, 0.0025, 0.0091, 0.0051, 0.0094, 0.0019, 0.0085, 0.0099, 0.0207,\n",
            "         0.0088, 0.0021, 0.0130, 0.0064, 0.0158, 0.0055, 0.0084, 0.0141, 0.0057,\n",
            "         0.0057, 0.0043, 0.0137, 0.0405, 0.0095, 0.0042, 0.0123, 0.0208, 0.0213,\n",
            "         0.0438, 0.0123, 0.0096, 0.0351, 0.0449, 0.0486, 0.0040, 0.0058, 0.0219,\n",
            "         0.0067, 0.0139, 0.0200, 0.0307, 0.0085, 0.0186, 0.0800, 0.0134, 0.0069,\n",
            "         0.0111, 0.0081]], grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "idx_next   torch.Size([1, 1])\n",
            "tensor([[62]])\n",
            "\n",
            "idx   torch.Size([1, 9])\n",
            "tensor([[ 0, 57, 25, 60, 59, 63, 10, 48, 62]])\n",
            "\n",
            "****************************************************************************************************\n",
            "****************************************************************************************************\n",
            "logits 1   torch.Size([1, 9, 65])\n",
            "tensor([[[ 6.4022e-01, -8.3649e-01,  2.2870e+00, -6.3659e-01, -1.6597e+00,\n",
            "           9.9623e-01, -8.3188e-01,  5.2271e-01, -1.0720e+00, -6.9581e-01,\n",
            "          -1.7181e-01, -9.1379e-01,  3.7381e-01, -7.0926e-01, -2.8789e-01,\n",
            "          -1.3386e+00,  9.9227e-01, -3.6417e-01, -6.1489e-02,  2.7885e-01,\n",
            "          -1.6062e+00, -3.2556e-01,  2.2351e-01, -1.0779e+00, -7.9957e-01,\n",
            "          -8.6830e-03,  5.0956e-01,  2.0606e+00, -1.7745e+00,  3.1508e-01,\n",
            "           1.4927e+00,  1.0682e-02, -4.0964e-01, -2.1083e+00,  1.7071e-01,\n",
            "           4.3387e-01,  1.4519e+00, -2.2249e+00,  7.7063e-01,  1.0032e+00,\n",
            "          -7.0616e-04, -4.3484e-01, -1.9791e-01, -1.1886e+00, -3.1788e-01,\n",
            "          -8.7637e-01, -1.1614e+00,  2.7749e-01, -5.7510e-01, -4.2678e-01,\n",
            "           7.3345e-01, -3.2024e-01,  6.5886e-01,  1.7822e-01, -8.8296e-01,\n",
            "          -1.1789e+00,  2.4352e+00,  1.2554e+00, -1.6087e+00,  2.0354e-01,\n",
            "          -8.8759e-01,  8.7902e-02, -5.1729e-01, -8.0634e-03, -2.4454e-01],\n",
            "         [-3.8277e-01, -1.4797e+00, -9.3967e-01,  7.0841e-01, -4.7449e-01,\n",
            "          -1.1234e-01, -8.3979e-03,  1.2981e+00,  1.0284e+00,  1.9532e+00,\n",
            "           9.6351e-01, -5.5102e-01, -1.8094e-02, -1.1516e+00,  9.6337e-01,\n",
            "          -1.1422e+00,  4.2531e-01,  7.8829e-01,  1.5791e-01, -6.5050e-01,\n",
            "          -1.9515e+00, -5.1075e-01, -3.1796e-01, -1.2072e+00, -4.5217e-01,\n",
            "           1.1605e+00, -4.3852e-01,  1.3751e+00, -1.5350e-01, -5.4588e-01,\n",
            "           7.3537e-01, -1.9808e-01,  1.2885e+00, -8.1364e-01, -2.5151e-01,\n",
            "           1.0707e+00, -7.2009e-01, -2.2826e-01,  5.7055e-01,  2.8824e-01,\n",
            "          -4.4158e-01, -1.0114e+00, -5.8738e-02, -8.0512e-01, -1.2166e-01,\n",
            "           9.6103e-01, -3.1628e-01,  6.9615e-01,  1.2500e+00,  3.2730e-01,\n",
            "          -1.6830e+00, -1.0868e+00,  1.7973e-01, -4.9715e-01, -8.9743e-01,\n",
            "           2.3514e+00, -1.7441e+00,  2.9130e-01,  5.1169e-01, -3.0511e-01,\n",
            "          -5.1778e-01,  1.1511e+00,  1.0217e+00, -9.4185e-01,  1.3796e+00],\n",
            "         [-1.7727e+00, -4.7958e-01, -1.1037e+00,  1.5407e+00, -1.7011e+00,\n",
            "           5.4244e-01,  2.5793e-01, -6.4687e-01, -1.3273e+00, -8.0335e-01,\n",
            "          -2.0133e+00,  7.5757e-01,  1.4304e+00, -2.5731e+00,  1.6308e-01,\n",
            "           5.0361e-01, -1.6828e+00, -6.0351e-01,  3.2846e-01, -4.8570e-01,\n",
            "          -2.2001e-01,  7.2255e-01, -6.9377e-01, -5.0921e-01, -1.5880e-01,\n",
            "          -1.5865e-01, -1.0233e+00,  7.9247e-01, -2.2431e-01, -3.7015e-01,\n",
            "          -6.8938e-02,  7.1754e-01,  2.0767e-01,  5.7276e-01, -5.7819e-02,\n",
            "           1.9290e-02, -6.9943e-01, -1.6268e+00, -2.0428e+00,  4.1387e-01,\n",
            "           8.4179e-01,  6.8693e-01,  1.2607e-01, -1.0933e+00,  6.2169e-01,\n",
            "           4.7553e-01, -7.4559e-01, -8.4729e-01, -1.6935e-01, -7.9273e-01,\n",
            "           5.8127e-01,  3.8010e-01, -8.6380e-01,  4.6584e-01,  4.1352e-01,\n",
            "           1.2824e+00, -3.3220e-01,  2.2574e+00,  7.1990e-02, -7.0376e-03,\n",
            "           1.7284e+00, -1.2540e-01, -1.2134e+00,  1.2877e+00, -5.9620e-01],\n",
            "         [-7.9098e-01, -1.2310e+00,  9.0967e-01,  2.3525e+00,  1.2958e+00,\n",
            "           2.4102e-01,  5.4828e-01,  3.4034e-01, -8.4229e-01, -1.6776e+00,\n",
            "          -5.7521e-01,  1.7487e-01, -4.0092e-01,  1.0710e+00, -1.5919e+00,\n",
            "          -1.9438e-01,  2.8927e-01, -1.1849e+00, -4.5773e-01, -6.0599e-01,\n",
            "           9.6084e-01, -9.3197e-01,  7.7462e-01, -9.5885e-01,  4.1500e-01,\n",
            "          -3.5605e-01,  5.4656e-01, -9.4210e-02, -8.3026e-01,  2.1989e-01,\n",
            "          -8.4314e-01,  6.5872e-01, -1.1154e+00,  3.9677e-01, -4.0450e-01,\n",
            "           3.5118e-01,  4.0325e-01,  5.4963e-01,  8.0297e-01, -1.6678e+00,\n",
            "           9.0490e-01, -7.5773e-02,  4.8272e-01, -4.1663e-01, -1.4605e-01,\n",
            "          -2.8069e-01,  1.6233e+00,  1.1839e+00,  6.0073e-01,  8.1070e-01,\n",
            "          -7.0183e-01,  2.0120e-01, -6.0440e-02,  8.4979e-01, -4.8596e-01,\n",
            "           6.8533e-01, -2.3514e-01, -7.9586e-01, -1.0194e-01,  2.4729e+00,\n",
            "           6.7204e-01, -1.1681e+00,  5.3780e-01, -2.3080e-01,  2.9356e+00],\n",
            "         [-8.2487e-01, -1.1262e+00, -1.9905e+00, -5.7842e-01,  1.1442e+00,\n",
            "          -6.2344e-01,  2.7705e-01, -8.6612e-02, -5.5443e-01, -8.5329e-01,\n",
            "           1.0496e+00, -7.6195e-02, -2.3980e-01,  9.9222e-01,  2.1765e-01,\n",
            "          -3.4155e-01, -8.3250e-01,  6.1567e-01, -1.2990e+00, -5.5579e-01,\n",
            "          -6.0035e-01,  2.2196e-02, -1.9621e-02,  4.4595e-01,  1.4080e+00,\n",
            "           1.8696e+00,  6.0174e-02, -4.2096e-01, -1.9413e+00, -5.4189e-01,\n",
            "           1.3994e+00, -1.3803e+00,  9.5743e-01,  1.5859e+00, -3.0421e-01,\n",
            "          -1.1166e+00, -1.2672e+00,  2.1693e-01,  9.2169e-01,  1.1649e-01,\n",
            "          -1.3277e+00, -9.3043e-01, -3.1011e-01, -9.5287e-01, -1.1202e+00,\n",
            "           1.4757e-01,  5.4540e-01,  8.2831e-01,  7.9593e-01, -4.9837e-01,\n",
            "           1.3721e+00,  2.9394e-01, -4.0992e-01,  5.1450e-01,  7.6212e-01,\n",
            "          -5.5108e-01,  3.8882e-01,  6.6684e-01,  9.8516e-01,  4.4573e-01,\n",
            "           2.7061e-01,  1.8889e-01,  3.6972e-01,  1.4927e-01,  6.1661e-01],\n",
            "         [-2.4758e-01, -1.2174e+00, -8.4225e-01,  7.3758e-02, -2.0748e-01,\n",
            "           5.3647e-01,  1.1809e+00,  1.6795e+00,  1.5323e-01, -1.8717e+00,\n",
            "           2.2346e+00, -6.8693e-01, -2.1119e-01, -1.4324e+00,  8.9226e-01,\n",
            "           1.4940e+00,  1.2984e-01, -5.8113e-01, -2.9294e-01, -1.8348e+00,\n",
            "          -2.4149e-01, -2.4143e-01,  4.4863e-01, -5.1218e-01,  8.4404e-01,\n",
            "          -1.3255e-01, -3.8060e-01,  1.6357e+00,  1.4380e+00,  7.6055e-01,\n",
            "           1.8211e-01,  4.9161e-01, -1.7092e+00,  6.3299e-01, -9.2653e-01,\n",
            "          -1.1860e+00,  8.1085e-01, -6.2301e-01, -6.9638e-01,  2.8001e-01,\n",
            "          -5.3669e-01,  7.3155e-01, -3.5016e-01,  4.5811e-01,  1.0904e-01,\n",
            "          -7.0566e-01,  6.9015e-01,  1.8711e+00, -2.0947e+00,  1.2399e-01,\n",
            "           1.2801e+00,  1.0601e-01,  1.9418e+00,  1.3580e+00, -1.2633e+00,\n",
            "          -7.0420e-01, -4.3899e-01,  5.0483e-01,  1.0862e+00,  3.4550e-01,\n",
            "          -6.0370e-01, -2.9158e-01, -6.2283e-01, -1.3244e-01, -2.6001e+00],\n",
            "         [ 8.2353e-01,  5.5253e-01,  5.0815e-01,  1.3908e+00,  8.8032e-01,\n",
            "           3.1026e-01, -3.5374e-01,  8.1965e-04, -3.5488e-01, -3.5108e-01,\n",
            "          -1.1202e+00, -1.2050e+00,  1.1561e+00, -3.2138e-02,  2.0929e-02,\n",
            "           2.0213e+00,  1.0372e+00,  9.6509e-01,  1.1448e-01, -5.7509e-01,\n",
            "          -8.5201e-01,  4.9056e-01, -1.2741e+00, -1.2460e+00,  1.7684e+00,\n",
            "          -5.6799e-01, -2.0801e+00,  9.2900e-01, -6.1813e-02,  6.0497e-01,\n",
            "          -2.6942e-01,  1.9213e-01, -6.1834e-01, -7.7854e-01,  7.6267e-01,\n",
            "           8.5272e-01, -2.8700e-01, -1.9288e-01,  9.6486e-01, -6.9171e-01,\n",
            "           2.4486e-01, -4.8010e-01,  2.1298e-01,  4.2678e-01, -9.9015e-01,\n",
            "           3.3416e-01, -2.7164e-01,  7.0549e-01,  2.8121e+00,  4.1596e-01,\n",
            "          -3.2185e-01,  3.0605e-01, -1.0644e+00, -6.5521e-01, -1.4503e+00,\n",
            "          -7.0973e-01,  1.9785e+00,  1.1422e+00,  4.5719e-01,  1.0731e-01,\n",
            "          -7.8540e-01,  3.0176e-01,  1.9352e+00, -8.6118e-01,  1.3755e+00],\n",
            "         [-5.7075e-01, -3.8165e-01, -8.9146e-01, -6.8085e-01, -1.2459e+00,\n",
            "           7.2844e-01,  1.0182e+00,  1.5140e-02,  2.0107e-01, -1.1911e+00,\n",
            "          -5.5425e-01, -3.2881e-01,  1.8214e+00,  1.9537e+00, -8.4760e-02,\n",
            "           6.7746e-01,  4.7121e-01, -1.4204e+00,  5.8264e-01, -1.2894e+00,\n",
            "           1.5084e-02, -5.6652e-01,  4.3420e-02, -1.5474e+00, -4.9463e-02,\n",
            "           9.9222e-02,  8.3802e-01, -1.6525e-02, -1.4293e+00,  3.6902e-01,\n",
            "          -3.3776e-01,  5.6823e-01, -4.7986e-01, -6.4255e-02,  4.5620e-01,\n",
            "          -4.4511e-01, -4.5951e-01, -7.2753e-01,  4.2803e-01,  1.5092e+00,\n",
            "           6.1540e-02, -7.6555e-01,  3.1808e-01,  8.4000e-01,  8.6496e-01,\n",
            "           1.5870e+00,  3.1308e-01,  6.7870e-02,  1.3656e+00,  1.6123e+00,\n",
            "           1.6907e+00, -8.0375e-01, -4.3442e-01,  8.9316e-01, -2.9539e-01,\n",
            "           4.3919e-01,  8.0265e-01,  1.2316e+00, -4.7660e-02,  7.2945e-01,\n",
            "           2.1887e+00,  4.0312e-01, -2.6475e-01,  2.1325e-01, -1.0569e-01],\n",
            "         [ 1.2572e+00, -3.8367e-01,  6.9216e-01, -3.3078e-01, -1.1041e+00,\n",
            "           3.9833e-01, -4.6716e-01,  4.3577e-01, -1.8893e+00,  1.6173e+00,\n",
            "           8.0077e-02,  4.7049e-01, -2.0437e-04, -2.8543e-01, -5.5473e-01,\n",
            "           8.4905e-01, -1.8782e+00,  7.5913e-01, -6.1835e-01,  2.0207e+00,\n",
            "           7.7716e-01,  3.0027e-01,  2.4747e-01,  1.7616e-01, -2.7771e+00,\n",
            "          -5.4470e-01,  1.0700e+00, -1.3042e+00, -6.6271e-01, -1.7451e+00,\n",
            "           1.5205e-01, -3.2699e-01,  2.7362e-01,  8.2461e-01,  5.0803e-01,\n",
            "           1.5821e+00, -1.9175e+00,  7.5143e-02, -1.0865e+00,  4.9196e-01,\n",
            "          -1.3577e+00,  5.5525e-01, -2.0710e+00, -3.4214e-01,  5.9237e-01,\n",
            "          -2.8906e+00, -1.5135e+00, -1.8888e+00, -1.6783e+00, -2.0536e+00,\n",
            "           1.1536e+00,  7.2961e-02,  8.5445e-02,  1.2386e+00,  6.1932e-01,\n",
            "          -2.3964e+00, -1.7924e+00,  1.7268e-01,  3.0910e-01,  1.4828e+00,\n",
            "          -8.1263e-01,  3.7025e-01, -5.7155e-01, -1.8450e+00, -1.6734e+00]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "loss \n",
            "None\n",
            "\n",
            "logits 2   torch.Size([1, 65])\n",
            "tensor([[ 1.2572e+00, -3.8367e-01,  6.9216e-01, -3.3078e-01, -1.1041e+00,\n",
            "          3.9833e-01, -4.6716e-01,  4.3577e-01, -1.8893e+00,  1.6173e+00,\n",
            "          8.0077e-02,  4.7049e-01, -2.0437e-04, -2.8543e-01, -5.5473e-01,\n",
            "          8.4905e-01, -1.8782e+00,  7.5913e-01, -6.1835e-01,  2.0207e+00,\n",
            "          7.7716e-01,  3.0027e-01,  2.4747e-01,  1.7616e-01, -2.7771e+00,\n",
            "         -5.4470e-01,  1.0700e+00, -1.3042e+00, -6.6271e-01, -1.7451e+00,\n",
            "          1.5205e-01, -3.2699e-01,  2.7362e-01,  8.2461e-01,  5.0803e-01,\n",
            "          1.5821e+00, -1.9175e+00,  7.5143e-02, -1.0865e+00,  4.9196e-01,\n",
            "         -1.3577e+00,  5.5525e-01, -2.0710e+00, -3.4214e-01,  5.9237e-01,\n",
            "         -2.8906e+00, -1.5135e+00, -1.8888e+00, -1.6783e+00, -2.0536e+00,\n",
            "          1.1536e+00,  7.2961e-02,  8.5445e-02,  1.2386e+00,  6.1932e-01,\n",
            "         -2.3964e+00, -1.7924e+00,  1.7268e-01,  3.0910e-01,  1.4828e+00,\n",
            "         -8.1263e-01,  3.7025e-01, -5.7155e-01, -1.8450e+00, -1.6734e+00]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "probs   torch.Size([1, 65])\n",
            "tensor([[0.0411, 0.0080, 0.0233, 0.0084, 0.0039, 0.0174, 0.0073, 0.0181, 0.0018,\n",
            "         0.0589, 0.0127, 0.0187, 0.0117, 0.0088, 0.0067, 0.0273, 0.0018, 0.0250,\n",
            "         0.0063, 0.0881, 0.0254, 0.0158, 0.0150, 0.0139, 0.0007, 0.0068, 0.0341,\n",
            "         0.0032, 0.0060, 0.0020, 0.0136, 0.0084, 0.0154, 0.0267, 0.0194, 0.0568,\n",
            "         0.0017, 0.0126, 0.0039, 0.0191, 0.0030, 0.0204, 0.0015, 0.0083, 0.0211,\n",
            "         0.0006, 0.0026, 0.0018, 0.0022, 0.0015, 0.0370, 0.0126, 0.0127, 0.0403,\n",
            "         0.0217, 0.0011, 0.0019, 0.0139, 0.0159, 0.0515, 0.0052, 0.0169, 0.0066,\n",
            "         0.0018, 0.0022]], grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "idx_next   torch.Size([1, 1])\n",
            "tensor([[56]])\n",
            "\n",
            "idx   torch.Size([1, 10])\n",
            "tensor([[ 0, 57, 25, 60, 59, 63, 10, 48, 62, 56]])\n",
            "\n",
            "****************************************************************************************************\n",
            "****************************************************************************************************\n",
            "logits 1   torch.Size([1, 10, 65])\n",
            "tensor([[[ 6.4022e-01, -8.3649e-01,  2.2870e+00, -6.3659e-01, -1.6597e+00,\n",
            "           9.9623e-01, -8.3188e-01,  5.2271e-01, -1.0720e+00, -6.9581e-01,\n",
            "          -1.7181e-01, -9.1379e-01,  3.7381e-01, -7.0926e-01, -2.8789e-01,\n",
            "          -1.3386e+00,  9.9227e-01, -3.6417e-01, -6.1489e-02,  2.7885e-01,\n",
            "          -1.6062e+00, -3.2556e-01,  2.2351e-01, -1.0779e+00, -7.9957e-01,\n",
            "          -8.6830e-03,  5.0956e-01,  2.0606e+00, -1.7745e+00,  3.1508e-01,\n",
            "           1.4927e+00,  1.0682e-02, -4.0964e-01, -2.1083e+00,  1.7071e-01,\n",
            "           4.3387e-01,  1.4519e+00, -2.2249e+00,  7.7063e-01,  1.0032e+00,\n",
            "          -7.0616e-04, -4.3484e-01, -1.9791e-01, -1.1886e+00, -3.1788e-01,\n",
            "          -8.7637e-01, -1.1614e+00,  2.7749e-01, -5.7510e-01, -4.2678e-01,\n",
            "           7.3345e-01, -3.2024e-01,  6.5886e-01,  1.7822e-01, -8.8296e-01,\n",
            "          -1.1789e+00,  2.4352e+00,  1.2554e+00, -1.6087e+00,  2.0354e-01,\n",
            "          -8.8759e-01,  8.7902e-02, -5.1729e-01, -8.0634e-03, -2.4454e-01],\n",
            "         [-3.8277e-01, -1.4797e+00, -9.3967e-01,  7.0841e-01, -4.7449e-01,\n",
            "          -1.1234e-01, -8.3979e-03,  1.2981e+00,  1.0284e+00,  1.9532e+00,\n",
            "           9.6351e-01, -5.5102e-01, -1.8094e-02, -1.1516e+00,  9.6337e-01,\n",
            "          -1.1422e+00,  4.2531e-01,  7.8829e-01,  1.5791e-01, -6.5050e-01,\n",
            "          -1.9515e+00, -5.1075e-01, -3.1796e-01, -1.2072e+00, -4.5217e-01,\n",
            "           1.1605e+00, -4.3852e-01,  1.3751e+00, -1.5350e-01, -5.4588e-01,\n",
            "           7.3537e-01, -1.9808e-01,  1.2885e+00, -8.1364e-01, -2.5151e-01,\n",
            "           1.0707e+00, -7.2009e-01, -2.2826e-01,  5.7055e-01,  2.8824e-01,\n",
            "          -4.4158e-01, -1.0114e+00, -5.8738e-02, -8.0512e-01, -1.2166e-01,\n",
            "           9.6103e-01, -3.1628e-01,  6.9615e-01,  1.2500e+00,  3.2730e-01,\n",
            "          -1.6830e+00, -1.0868e+00,  1.7973e-01, -4.9715e-01, -8.9743e-01,\n",
            "           2.3514e+00, -1.7441e+00,  2.9130e-01,  5.1169e-01, -3.0511e-01,\n",
            "          -5.1778e-01,  1.1511e+00,  1.0217e+00, -9.4185e-01,  1.3796e+00],\n",
            "         [-1.7727e+00, -4.7958e-01, -1.1037e+00,  1.5407e+00, -1.7011e+00,\n",
            "           5.4244e-01,  2.5793e-01, -6.4687e-01, -1.3273e+00, -8.0335e-01,\n",
            "          -2.0133e+00,  7.5757e-01,  1.4304e+00, -2.5731e+00,  1.6308e-01,\n",
            "           5.0361e-01, -1.6828e+00, -6.0351e-01,  3.2846e-01, -4.8570e-01,\n",
            "          -2.2001e-01,  7.2255e-01, -6.9377e-01, -5.0921e-01, -1.5880e-01,\n",
            "          -1.5865e-01, -1.0233e+00,  7.9247e-01, -2.2431e-01, -3.7015e-01,\n",
            "          -6.8938e-02,  7.1754e-01,  2.0767e-01,  5.7276e-01, -5.7819e-02,\n",
            "           1.9290e-02, -6.9943e-01, -1.6268e+00, -2.0428e+00,  4.1387e-01,\n",
            "           8.4179e-01,  6.8693e-01,  1.2607e-01, -1.0933e+00,  6.2169e-01,\n",
            "           4.7553e-01, -7.4559e-01, -8.4729e-01, -1.6935e-01, -7.9273e-01,\n",
            "           5.8127e-01,  3.8010e-01, -8.6380e-01,  4.6584e-01,  4.1352e-01,\n",
            "           1.2824e+00, -3.3220e-01,  2.2574e+00,  7.1990e-02, -7.0376e-03,\n",
            "           1.7284e+00, -1.2540e-01, -1.2134e+00,  1.2877e+00, -5.9620e-01],\n",
            "         [-7.9098e-01, -1.2310e+00,  9.0967e-01,  2.3525e+00,  1.2958e+00,\n",
            "           2.4102e-01,  5.4828e-01,  3.4034e-01, -8.4229e-01, -1.6776e+00,\n",
            "          -5.7521e-01,  1.7487e-01, -4.0092e-01,  1.0710e+00, -1.5919e+00,\n",
            "          -1.9438e-01,  2.8927e-01, -1.1849e+00, -4.5773e-01, -6.0599e-01,\n",
            "           9.6084e-01, -9.3197e-01,  7.7462e-01, -9.5885e-01,  4.1500e-01,\n",
            "          -3.5605e-01,  5.4656e-01, -9.4210e-02, -8.3026e-01,  2.1989e-01,\n",
            "          -8.4314e-01,  6.5872e-01, -1.1154e+00,  3.9677e-01, -4.0450e-01,\n",
            "           3.5118e-01,  4.0325e-01,  5.4963e-01,  8.0297e-01, -1.6678e+00,\n",
            "           9.0490e-01, -7.5773e-02,  4.8272e-01, -4.1663e-01, -1.4605e-01,\n",
            "          -2.8069e-01,  1.6233e+00,  1.1839e+00,  6.0073e-01,  8.1070e-01,\n",
            "          -7.0183e-01,  2.0120e-01, -6.0440e-02,  8.4979e-01, -4.8596e-01,\n",
            "           6.8533e-01, -2.3514e-01, -7.9586e-01, -1.0194e-01,  2.4729e+00,\n",
            "           6.7204e-01, -1.1681e+00,  5.3780e-01, -2.3080e-01,  2.9356e+00],\n",
            "         [-8.2487e-01, -1.1262e+00, -1.9905e+00, -5.7842e-01,  1.1442e+00,\n",
            "          -6.2344e-01,  2.7705e-01, -8.6612e-02, -5.5443e-01, -8.5329e-01,\n",
            "           1.0496e+00, -7.6195e-02, -2.3980e-01,  9.9222e-01,  2.1765e-01,\n",
            "          -3.4155e-01, -8.3250e-01,  6.1567e-01, -1.2990e+00, -5.5579e-01,\n",
            "          -6.0035e-01,  2.2196e-02, -1.9621e-02,  4.4595e-01,  1.4080e+00,\n",
            "           1.8696e+00,  6.0174e-02, -4.2096e-01, -1.9413e+00, -5.4189e-01,\n",
            "           1.3994e+00, -1.3803e+00,  9.5743e-01,  1.5859e+00, -3.0421e-01,\n",
            "          -1.1166e+00, -1.2672e+00,  2.1693e-01,  9.2169e-01,  1.1649e-01,\n",
            "          -1.3277e+00, -9.3043e-01, -3.1011e-01, -9.5287e-01, -1.1202e+00,\n",
            "           1.4757e-01,  5.4540e-01,  8.2831e-01,  7.9593e-01, -4.9837e-01,\n",
            "           1.3721e+00,  2.9394e-01, -4.0992e-01,  5.1450e-01,  7.6212e-01,\n",
            "          -5.5108e-01,  3.8882e-01,  6.6684e-01,  9.8516e-01,  4.4573e-01,\n",
            "           2.7061e-01,  1.8889e-01,  3.6972e-01,  1.4927e-01,  6.1661e-01],\n",
            "         [-2.4758e-01, -1.2174e+00, -8.4225e-01,  7.3758e-02, -2.0748e-01,\n",
            "           5.3647e-01,  1.1809e+00,  1.6795e+00,  1.5323e-01, -1.8717e+00,\n",
            "           2.2346e+00, -6.8693e-01, -2.1119e-01, -1.4324e+00,  8.9226e-01,\n",
            "           1.4940e+00,  1.2984e-01, -5.8113e-01, -2.9294e-01, -1.8348e+00,\n",
            "          -2.4149e-01, -2.4143e-01,  4.4863e-01, -5.1218e-01,  8.4404e-01,\n",
            "          -1.3255e-01, -3.8060e-01,  1.6357e+00,  1.4380e+00,  7.6055e-01,\n",
            "           1.8211e-01,  4.9161e-01, -1.7092e+00,  6.3299e-01, -9.2653e-01,\n",
            "          -1.1860e+00,  8.1085e-01, -6.2301e-01, -6.9638e-01,  2.8001e-01,\n",
            "          -5.3669e-01,  7.3155e-01, -3.5016e-01,  4.5811e-01,  1.0904e-01,\n",
            "          -7.0566e-01,  6.9015e-01,  1.8711e+00, -2.0947e+00,  1.2399e-01,\n",
            "           1.2801e+00,  1.0601e-01,  1.9418e+00,  1.3580e+00, -1.2633e+00,\n",
            "          -7.0420e-01, -4.3899e-01,  5.0483e-01,  1.0862e+00,  3.4550e-01,\n",
            "          -6.0370e-01, -2.9158e-01, -6.2283e-01, -1.3244e-01, -2.6001e+00],\n",
            "         [ 8.2353e-01,  5.5253e-01,  5.0815e-01,  1.3908e+00,  8.8032e-01,\n",
            "           3.1026e-01, -3.5374e-01,  8.1965e-04, -3.5488e-01, -3.5108e-01,\n",
            "          -1.1202e+00, -1.2050e+00,  1.1561e+00, -3.2138e-02,  2.0929e-02,\n",
            "           2.0213e+00,  1.0372e+00,  9.6509e-01,  1.1448e-01, -5.7509e-01,\n",
            "          -8.5201e-01,  4.9056e-01, -1.2741e+00, -1.2460e+00,  1.7684e+00,\n",
            "          -5.6799e-01, -2.0801e+00,  9.2900e-01, -6.1813e-02,  6.0497e-01,\n",
            "          -2.6942e-01,  1.9213e-01, -6.1834e-01, -7.7854e-01,  7.6267e-01,\n",
            "           8.5272e-01, -2.8700e-01, -1.9288e-01,  9.6486e-01, -6.9171e-01,\n",
            "           2.4486e-01, -4.8010e-01,  2.1298e-01,  4.2678e-01, -9.9015e-01,\n",
            "           3.3416e-01, -2.7164e-01,  7.0549e-01,  2.8121e+00,  4.1596e-01,\n",
            "          -3.2185e-01,  3.0605e-01, -1.0644e+00, -6.5521e-01, -1.4503e+00,\n",
            "          -7.0973e-01,  1.9785e+00,  1.1422e+00,  4.5719e-01,  1.0731e-01,\n",
            "          -7.8540e-01,  3.0176e-01,  1.9352e+00, -8.6118e-01,  1.3755e+00],\n",
            "         [-5.7075e-01, -3.8165e-01, -8.9146e-01, -6.8085e-01, -1.2459e+00,\n",
            "           7.2844e-01,  1.0182e+00,  1.5140e-02,  2.0107e-01, -1.1911e+00,\n",
            "          -5.5425e-01, -3.2881e-01,  1.8214e+00,  1.9537e+00, -8.4760e-02,\n",
            "           6.7746e-01,  4.7121e-01, -1.4204e+00,  5.8264e-01, -1.2894e+00,\n",
            "           1.5084e-02, -5.6652e-01,  4.3420e-02, -1.5474e+00, -4.9463e-02,\n",
            "           9.9222e-02,  8.3802e-01, -1.6525e-02, -1.4293e+00,  3.6902e-01,\n",
            "          -3.3776e-01,  5.6823e-01, -4.7986e-01, -6.4255e-02,  4.5620e-01,\n",
            "          -4.4511e-01, -4.5951e-01, -7.2753e-01,  4.2803e-01,  1.5092e+00,\n",
            "           6.1540e-02, -7.6555e-01,  3.1808e-01,  8.4000e-01,  8.6496e-01,\n",
            "           1.5870e+00,  3.1308e-01,  6.7870e-02,  1.3656e+00,  1.6123e+00,\n",
            "           1.6907e+00, -8.0375e-01, -4.3442e-01,  8.9316e-01, -2.9539e-01,\n",
            "           4.3919e-01,  8.0265e-01,  1.2316e+00, -4.7660e-02,  7.2945e-01,\n",
            "           2.1887e+00,  4.0312e-01, -2.6475e-01,  2.1325e-01, -1.0569e-01],\n",
            "         [ 1.2572e+00, -3.8367e-01,  6.9216e-01, -3.3078e-01, -1.1041e+00,\n",
            "           3.9833e-01, -4.6716e-01,  4.3577e-01, -1.8893e+00,  1.6173e+00,\n",
            "           8.0077e-02,  4.7049e-01, -2.0437e-04, -2.8543e-01, -5.5473e-01,\n",
            "           8.4905e-01, -1.8782e+00,  7.5913e-01, -6.1835e-01,  2.0207e+00,\n",
            "           7.7716e-01,  3.0027e-01,  2.4747e-01,  1.7616e-01, -2.7771e+00,\n",
            "          -5.4470e-01,  1.0700e+00, -1.3042e+00, -6.6271e-01, -1.7451e+00,\n",
            "           1.5205e-01, -3.2699e-01,  2.7362e-01,  8.2461e-01,  5.0803e-01,\n",
            "           1.5821e+00, -1.9175e+00,  7.5143e-02, -1.0865e+00,  4.9196e-01,\n",
            "          -1.3577e+00,  5.5525e-01, -2.0710e+00, -3.4214e-01,  5.9237e-01,\n",
            "          -2.8906e+00, -1.5135e+00, -1.8888e+00, -1.6783e+00, -2.0536e+00,\n",
            "           1.1536e+00,  7.2961e-02,  8.5445e-02,  1.2386e+00,  6.1932e-01,\n",
            "          -2.3964e+00, -1.7924e+00,  1.7268e-01,  3.0910e-01,  1.4828e+00,\n",
            "          -8.1263e-01,  3.7025e-01, -5.7155e-01, -1.8450e+00, -1.6734e+00],\n",
            "         [ 8.1236e-01, -9.0303e-01,  7.0732e-01,  1.3868e+00,  1.7853e+00,\n",
            "          -1.6545e+00, -2.1584e-02, -1.2649e+00, -3.8548e-01, -2.2693e-01,\n",
            "          -1.5295e-01,  1.6237e-02,  6.7723e-01, -3.4118e-01,  2.0235e-01,\n",
            "          -4.4506e-02,  2.4989e+00, -1.6165e+00,  1.0505e-01,  6.3336e-02,\n",
            "           7.6769e-01, -2.7113e+00,  7.4527e-01, -4.9782e-01, -1.4638e+00,\n",
            "          -6.7248e-01, -4.3625e-01,  4.8807e-01,  1.3430e+00,  1.2452e+00,\n",
            "          -3.3302e-01, -4.2465e-01,  1.9389e+00, -1.3836e+00,  1.7344e+00,\n",
            "          -5.1806e-01,  5.0265e-01, -3.7196e-01, -6.3209e-01,  1.4872e-01,\n",
            "           6.6954e-02, -9.2893e-01,  4.3017e-01,  2.9278e-03,  7.8687e-01,\n",
            "           6.5758e-01,  2.5873e-01,  1.1657e+00, -4.8920e-01,  1.9071e+00,\n",
            "           3.8216e-01, -1.3708e+00,  8.2542e-01,  7.7518e-01, -1.6801e+00,\n",
            "           1.8479e-01,  1.6269e+00, -4.5915e-01, -3.0237e-01,  1.4325e+00,\n",
            "          -8.8403e-01, -4.4780e-01, -1.6898e+00,  2.1231e-01,  9.0816e-01]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "loss \n",
            "None\n",
            "\n",
            "logits 2   torch.Size([1, 65])\n",
            "tensor([[ 0.8124, -0.9030,  0.7073,  1.3868,  1.7853, -1.6545, -0.0216, -1.2649,\n",
            "         -0.3855, -0.2269, -0.1529,  0.0162,  0.6772, -0.3412,  0.2024, -0.0445,\n",
            "          2.4989, -1.6165,  0.1050,  0.0633,  0.7677, -2.7113,  0.7453, -0.4978,\n",
            "         -1.4638, -0.6725, -0.4363,  0.4881,  1.3430,  1.2452, -0.3330, -0.4246,\n",
            "          1.9389, -1.3836,  1.7344, -0.5181,  0.5027, -0.3720, -0.6321,  0.1487,\n",
            "          0.0670, -0.9289,  0.4302,  0.0029,  0.7869,  0.6576,  0.2587,  1.1657,\n",
            "         -0.4892,  1.9071,  0.3822, -1.3708,  0.8254,  0.7752, -1.6801,  0.1848,\n",
            "          1.6269, -0.4591, -0.3024,  1.4325, -0.8840, -0.4478, -1.6898,  0.2123,\n",
            "          0.9082]], grad_fn=<SliceBackward0>)\n",
            "\n",
            "probs   torch.Size([1, 65])\n",
            "tensor([[0.0194, 0.0035, 0.0175, 0.0345, 0.0513, 0.0016, 0.0084, 0.0024, 0.0059,\n",
            "         0.0069, 0.0074, 0.0088, 0.0170, 0.0061, 0.0105, 0.0082, 0.1048, 0.0017,\n",
            "         0.0096, 0.0092, 0.0186, 0.0006, 0.0181, 0.0052, 0.0020, 0.0044, 0.0056,\n",
            "         0.0140, 0.0330, 0.0299, 0.0062, 0.0056, 0.0599, 0.0022, 0.0488, 0.0051,\n",
            "         0.0142, 0.0059, 0.0046, 0.0100, 0.0092, 0.0034, 0.0132, 0.0086, 0.0189,\n",
            "         0.0166, 0.0112, 0.0276, 0.0053, 0.0580, 0.0126, 0.0022, 0.0197, 0.0187,\n",
            "         0.0016, 0.0104, 0.0438, 0.0054, 0.0064, 0.0361, 0.0036, 0.0055, 0.0016,\n",
            "         0.0106, 0.0214]], grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "idx_next   torch.Size([1, 1])\n",
            "tensor([[16]])\n",
            "\n",
            "idx   torch.Size([1, 11])\n",
            "tensor([[ 0, 57, 25, 60, 59, 63, 10, 48, 62, 56, 16]])\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "sMvuy:jxrD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[0.0091, 0.0071, 0.0053, 0.0030, 0.0141, 0.0078, 0.0197, 0.0081, 0.0109,\n",
        "         0.0243, 0.0020, 0.0045, 0.0096, 0.0060, 0.0030, 0.0354, 0.0292, 0.0066,\n",
        "         0.0101, 0.0199, 0.0010, 0.0124, 0.0335, 0.0137, 0.0086, 0.0016, 0.0024,\n",
        "         0.0054, 0.0118, 0.0034, 0.0347, 0.0930, 0.0039, 0.0059, 0.0208, 0.0085,\n",
        "         0.0089, 0.0235, 0.0024, 0.0056, 0.0046, 0.0030, 0.0131, 0.0017, 0.0023,\n",
        "         0.0134, 0.0042, 0.0038, 0.0392, 0.0034, 0.0292, 0.0057, 0.0017, 0.0621,\n",
        "         0.1199, 0.0013, 0.0323, 0.0017, 0.0172, 0.0061, 0.0165, 0.0351, 0.0379,\n",
        "         0.0051, 0.0033]])\n",
        "\n",
        "\n",
        "b = torch.argmax(a)\n"
      ],
      "metadata": {
        "id": "Sfd4FDkawrzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b1 = torch.unsqueeze(b, dim=0)\n",
        "\n",
        "print(b1)\n",
        "\n",
        "b2 = torch.unsqueeze(b1, dim=0)\n",
        "\n",
        "print(b2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iACH8CvQcjTg",
        "outputId": "8c12a0b2-7652-4e57-ca03-81f51b3bf063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([54])\n",
            "tensor([[54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input = torch.randn(4, 10)\n",
        "target = torch.LongTensor([2, 5, 1, 9])\n",
        "\n",
        "\n",
        "# input_data = torch.randn(4, 10)  # 4 samples, 10 classes\n",
        "# labels = torch.LongTensor([2, 5, 1, 9])\n",
        "\n",
        "print(\"input.shape \" ,input.shape )\n",
        "print(input)\n",
        "\n",
        "\n",
        "print(\"target.shape \" ,target.shape )\n",
        "print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qve_ggU79_iL",
        "outputId": "6f2876e1-667f-441b-953d-d371f850ed29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input.shape  torch.Size([4, 10])\n",
            "tensor([[ 0.6230,  0.7585,  0.8772, -0.6974, -0.2518, -0.3087, -0.7422,  0.5597,\n",
            "         -0.0612, -0.3581],\n",
            "        [ 0.2352,  0.8811,  1.3976, -0.0397,  1.3781, -0.4115, -0.1419, -0.7477,\n",
            "          1.4577,  0.6484],\n",
            "        [-0.1150,  1.8377,  1.5220,  0.2469, -0.9484,  0.2979,  0.3340, -0.1535,\n",
            "         -0.1813,  0.2081],\n",
            "        [-1.2114, -0.9645,  1.9652,  2.8789,  0.7197,  0.6127,  1.2042,  2.2287,\n",
            "          0.7241, -0.6630]])\n",
            "target.shape  torch.Size([4])\n",
            "tensor([2, 5, 1, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embedding = nn.Embedding(10, 3)\n",
        "\n",
        "input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9],[0,1,2,4]])\n",
        "a = embedding(input)\n",
        "print(a.shape)\n",
        "print(a)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mWTIL2S4z_v",
        "outputId": "80c12e5c-cf0e-4864-ce1a-bed7738388a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 4, 3])\n",
            "tensor([[[-8.8771e-01,  1.0885e+00, -9.6822e-02],\n",
            "         [ 1.8109e-01,  6.9053e-01,  5.4397e-01],\n",
            "         [-4.1890e-01,  1.0217e+00,  8.6555e-01],\n",
            "         [ 4.3020e-01,  6.5274e-04,  3.9139e-01]],\n",
            "\n",
            "        [[-4.1890e-01,  1.0217e+00,  8.6555e-01],\n",
            "         [ 3.7042e-01, -7.0267e-01,  2.2366e-01],\n",
            "         [ 1.8109e-01,  6.9053e-01,  5.4397e-01],\n",
            "         [-5.4336e-01,  1.4790e-02, -1.5209e+00]],\n",
            "\n",
            "        [[ 1.5694e-01, -4.2831e-02,  6.1647e-02],\n",
            "         [-8.8771e-01,  1.0885e+00, -9.6822e-02],\n",
            "         [ 1.8109e-01,  6.9053e-01,  5.4397e-01],\n",
            "         [-4.1890e-01,  1.0217e+00,  8.6555e-01]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "    def generate2(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "\n",
        "            b = torch.argmax(probs)\n",
        "\n",
        "            b1 = torch.unsqueeze(b, dim=0)\n",
        "\n",
        "\n",
        "            idx_next = torch.unsqueeze(b1, dim=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "# print(logits.shape)\n",
        "# print(loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=10)[0].tolist()))\n",
        "\n",
        "print(decode(m.generate2(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=10)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7gtGdQuaYNQ",
        "outputId": "01483fe6-f1a6-4bb8-aebe-70f04ed33691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ";oD b:hVgy\n",
            "\n",
            "$AVgNQT Oz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = BigramLanguageModel(vocab_size)\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "batch_size = 32\n",
        "for steps in tqdm(range(10000)): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "print()\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "ab03c386-bc4c-4530-e018-2f0cb584bd2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:17<00:00, 576.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2.4973154067993164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "e37e63c6-3c1a-4c90-f64e-65857ad20655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "INCin:\n",
            "MXPisk'blis f n pok f HASueerumputse n V:\n",
            "\n",
            "\n",
            "AD:\n",
            "Pat cofooramoomy w tl, saf ss fes mege omer p NTe qAnt tet 'd p th oucouthe, f ad s fat for, mef:\n",
            "OLASARDETUESatis\n",
            "\n",
            "Tor carope:\n",
            "Arun pe K:\n",
            "ONCo whoo?\n",
            "A:\n",
            "W, ENorand tonaltond jme, SI d steloue gr\n",
            "GAMPOFRhor ghert then be ond araindirber prss\n",
            "IIfousthe Veat hin\n",
            "oor ouramomyom' pelorivy hiK:\n",
            "\n",
            "e corevig, ft ll st abeke ativeome hthenthimaju, min wilis?Xck.\n",
            "S:\n",
            "Cor se ARAnid tasiot g blamy, ar at nthinde buthe bearo in eangu.\n",
            "ALBELI'd?\n",
            "Whoushetons\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = decode(m.generate2(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist())"
      ],
      "metadata": {
        "id": "rRI0AJYHf3_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "kAj61nrnh3dT",
        "outputId": "3aea6f3d-aaa5-4647-b06d-f17aa2788fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "XinV8nmAnmKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "6a7e7200-b4f0-4593-cbd7-bb74ca887012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "621709aa-eb60-41f3-b142-8ac1fd261d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ],
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xbow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbcNjdEmm8TA",
        "outputId": "e5f059aa-2551-46c1-f142-19578b981b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.8173,  0.4127],\n",
              "         [-0.1342,  0.4395],\n",
              "         [ 0.2711,  0.4774],\n",
              "         [ 0.2421,  0.0694],\n",
              "         [ 0.0084,  0.0020],\n",
              "         [ 0.0712, -0.1128],\n",
              "         [ 0.2527,  0.2149]],\n",
              "\n",
              "        [[-0.6631, -0.2513],\n",
              "         [ 0.1735, -0.0649],\n",
              "         [ 0.1685,  0.3348],\n",
              "         [-0.1621,  0.1765],\n",
              "         [-0.2312, -0.0436],\n",
              "         [-0.1015, -0.2855],\n",
              "         [-0.2593, -0.1630],\n",
              "         [-0.3015, -0.2293]],\n",
              "\n",
              "        [[ 1.6455, -0.8030],\n",
              "         [ 1.4985, -0.5395],\n",
              "         [ 0.4954,  0.3420],\n",
              "         [ 1.0623, -0.1802],\n",
              "         [ 1.1401, -0.4462],\n",
              "         [ 1.0870, -0.4071],\n",
              "         [ 1.0430, -0.1299],\n",
              "         [ 1.1138, -0.1641]]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "print(\"wei \" , wei.shape)\n",
        "print(wei)\n",
        "print()\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "print(\"wei \" , wei.shape)\n",
        "print(wei)\n",
        "print()\n",
        "\n",
        "\n",
        "print(\"x \" , x.shape)\n",
        "print(x)\n",
        "print()\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "print(\"xbow2 \" , xbow2.shape)\n",
        "print(xbow2)\n",
        "print()\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "08bb49fd-a7f7-424a-9e34-1040f6a06440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei  torch.Size([8, 8])\n",
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "\n",
            "wei  torch.Size([8, 8])\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "\n",
            "x  torch.Size([4, 8, 2])\n",
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.3596, -0.9152],\n",
            "         [ 0.6258,  0.0255],\n",
            "         [ 0.9545,  0.0643],\n",
            "         [ 0.3612,  1.1679],\n",
            "         [-1.3499, -0.5102],\n",
            "         [ 0.2360, -0.2398],\n",
            "         [-0.9211,  1.5433]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.2858,  0.9651],\n",
            "         [-2.0371,  0.4931],\n",
            "         [ 1.4870,  0.5910],\n",
            "         [ 0.1260, -1.5627],\n",
            "         [-1.1601, -0.3348],\n",
            "         [ 0.4478, -0.8016],\n",
            "         [ 1.5236,  2.5086]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 1.0101,  0.1215],\n",
            "         [ 0.1584,  1.1340],\n",
            "         [-1.1539, -0.2984],\n",
            "         [-0.5075, -0.9239],\n",
            "         [ 0.5467, -1.4948],\n",
            "         [-1.2057,  0.5718],\n",
            "         [-0.5974, -0.6937]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.3514, -0.2759],\n",
            "         [-1.5108,  2.1048],\n",
            "         [ 2.7630, -1.7465],\n",
            "         [ 1.4516, -1.5103],\n",
            "         [ 0.8212, -0.2115],\n",
            "         [ 0.7789,  1.5333],\n",
            "         [ 1.6097, -0.4032]]])\n",
            "\n",
            "xbow2  torch.Size([4, 8, 2])\n",
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "print(\"tril.shape \" , tril.shape)\n",
        "print(tril)\n",
        "print()\n",
        "\n",
        "wei = torch.zeros((T,T))\n",
        "print(\"wei.shape \" , wei.shape)\n",
        "print(wei)\n",
        "print()\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "print(\"wei.shape \" , wei.shape)\n",
        "print(wei)\n",
        "print()\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(\"wei.shape \" , wei.shape)\n",
        "print(wei)\n",
        "print()\n",
        "xbow3 = wei @ x\n",
        "print(\"xbow3.shape \" , xbow3.shape)\n",
        "print(xbow3)\n",
        "print()\n",
        "torch.allclose(xbow, xbow3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "28756927-a24e-4c85-8bbc-0c403bbec433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tril.shape  torch.Size([8, 8])\n",
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "\n",
            "wei.shape  torch.Size([8, 8])\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "\n",
            "wei.shape  torch.Size([8, 8])\n",
            "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "\n",
            "wei.shape  torch.Size([8, 8])\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "\n",
            "xbow3.shape  torch.Size([4, 8, 2])\n",
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "print(\"k \" , k.shape)\n",
        "print(k)\n",
        "q = query(x) # (B, T, 16)\n",
        "print(\"q \" , q.shape)\n",
        "print(q)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "print(\"wei\",wei.shape)\n",
        "print(wei)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "print(\"tril\",tril.shape)\n",
        "print(tril)\n",
        "\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "print(\"wei\",wei.shape)\n",
        "print(wei)\n",
        "\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(\"wei\",wei.shape)\n",
        "print(wei)\n",
        "\n",
        "\n",
        "v = value(x)\n",
        "print(\"v\",v.shape)\n",
        "print(v)\n",
        "\n",
        "out = wei @ v\n",
        "print(\"out\",out.shape)\n",
        "print(out)\n",
        "\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "b3b68ad6-7b69-46da-c94b-6a0308610ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k  torch.Size([4, 8, 16])\n",
            "tensor([[[ 1.1965e-01, -3.0127e-01,  3.6293e-01,  1.1771e+00,  1.1385e+00,\n",
            "          -2.5543e-01,  1.4537e-01, -2.9437e-01, -7.0201e-01, -1.0308e+00,\n",
            "           7.4357e-01, -8.0984e-01, -6.6687e-01,  9.1233e-02, -6.0747e-03,\n",
            "           1.9833e-01],\n",
            "         [-5.4229e-01, -5.5581e-01, -7.6131e-02,  1.2929e+00,  8.6535e-01,\n",
            "          -1.1998e+00,  3.8781e-01,  1.9389e-01,  7.0235e-01, -8.2251e-01,\n",
            "           2.3484e-01, -8.4995e-01, -3.8126e-01, -2.9906e-01,  1.0242e-02,\n",
            "          -5.5449e-01],\n",
            "         [-3.7359e-01, -4.6781e-01, -2.1560e-01, -8.0344e-01, -3.7153e-01,\n",
            "          -5.4427e-01, -9.1455e-01, -5.5926e-02, -3.2903e-01, -2.1023e-01,\n",
            "           1.1665e-01, -1.7978e-01, -2.8196e-01, -3.3204e-01, -4.5963e-01,\n",
            "          -1.3255e-01],\n",
            "         [-3.1463e-01,  8.4460e-02, -1.2351e-01, -7.0577e-01, -1.8022e-01,\n",
            "           5.4922e-01, -8.9805e-01, -4.9384e-01,  6.7907e-01,  8.8270e-01,\n",
            "           4.9109e-01,  5.1903e-01,  9.0109e-01,  9.1255e-02, -1.9332e-01,\n",
            "          -6.7704e-01],\n",
            "         [ 2.3940e-02,  9.9822e-02, -1.8709e-01, -8.5960e-02, -4.8815e-01,\n",
            "          -1.6765e+00,  2.4126e-01,  7.3606e-01,  4.6080e-01, -8.7217e-01,\n",
            "          -4.2590e-01, -1.1347e+00, -1.0571e+00, -9.4006e-01,  1.3426e-01,\n",
            "          -1.5716e-02],\n",
            "         [-2.3618e-01, -7.8730e-01, -3.8019e-01,  5.8150e-01, -3.7222e-01,\n",
            "           1.2405e+00, -7.0045e-01, -1.4917e+00,  7.6784e-01,  3.5839e-01,\n",
            "           6.1200e-01, -7.9353e-02,  5.9827e-01,  2.6353e-01,  6.4905e-01,\n",
            "           7.0914e-02],\n",
            "         [-7.9413e-01, -1.6598e-01, -2.8096e-01, -1.0208e-01, -7.3521e-01,\n",
            "          -7.5183e-01, -1.2759e-01, -5.1134e-03,  3.3249e-01, -3.3738e-01,\n",
            "           1.6783e-01,  3.1048e-01,  2.2577e-01,  1.2434e-01,  4.6169e-01,\n",
            "           2.0156e-01],\n",
            "         [ 1.6513e-01, -1.5990e-01, -5.7168e-01, -3.9571e-01,  3.9301e-01,\n",
            "          -8.5665e-01,  3.3900e-01, -7.9771e-01,  2.2134e-01, -5.1612e-01,\n",
            "           1.8504e-01, -2.1048e-01,  3.7789e-01,  4.8222e-02, -4.7437e-01,\n",
            "          -5.0405e-02]],\n",
            "\n",
            "        [[-1.6977e-01, -1.5875e+00, -9.1855e-01,  6.6326e-02, -1.1497e+00,\n",
            "           2.7652e-01, -7.1052e-01, -6.0851e-01, -7.9616e-02, -1.3215e-01,\n",
            "           6.9567e-01,  6.7096e-01,  5.4679e-01,  7.6157e-01,  6.3947e-01,\n",
            "           5.8098e-01],\n",
            "         [-1.1435e-01, -3.5312e-01, -1.8434e-01,  5.2000e-01, -6.0603e-01,\n",
            "           6.3977e-01,  1.2789e-01, -8.0061e-01, -3.9588e-01,  9.8180e-01,\n",
            "          -2.7790e-01, -4.0351e-01, -6.6473e-01,  2.3659e-01,  2.4786e-01,\n",
            "           2.3966e-01],\n",
            "         [-6.3508e-01, -1.0090e+00,  4.4846e-01,  2.6102e-01,  3.0953e-01,\n",
            "           1.0269e+00, -5.0824e-01,  1.5112e-01,  4.9967e-01, -1.0242e+00,\n",
            "           3.3076e-02,  7.9948e-01,  4.7760e-01,  1.0383e-01,  2.8658e-01,\n",
            "           6.3477e-01],\n",
            "         [ 7.1183e-02,  5.7131e-01,  6.2270e-01,  2.4220e-01,  1.1163e+00,\n",
            "           5.2713e-01, -2.7616e-01, -2.8885e-01,  1.6921e-01,  1.0390e+00,\n",
            "          -1.2049e-01, -7.5153e-01,  2.8590e-01, -3.0348e-01, -3.1344e-02,\n",
            "          -6.0875e-01],\n",
            "         [-3.2904e-02,  5.3796e-01,  5.0853e-02,  1.1635e+00, -1.3198e-01,\n",
            "          -8.2809e-01,  3.2218e-01,  2.0548e-01, -1.3409e-01, -2.4342e-01,\n",
            "          -5.2483e-01, -1.0036e+00,  1.4676e-01,  6.1899e-02,  1.1584e-01,\n",
            "          -1.9803e-01],\n",
            "         [-1.5397e-01,  6.4264e-01, -1.2269e-01,  4.0754e-01,  7.2767e-02,\n",
            "          -2.1382e+00,  2.0803e+00,  1.0649e+00,  1.3110e-01, -1.7620e-01,\n",
            "          -1.1203e-02, -1.7259e+00, -7.9774e-01,  1.2679e+00, -6.2352e-03,\n",
            "          -2.9788e-02],\n",
            "         [ 7.5567e-01, -1.1675e-01, -7.9704e-01,  1.6243e-02,  8.6796e-01,\n",
            "          -2.0754e-01,  1.0132e+00, -8.4467e-01,  3.1207e-01,  3.1308e-01,\n",
            "          -4.0251e-01, -5.5028e-01, -1.1918e-02,  1.1328e-01, -1.2361e-01,\n",
            "           2.7873e-01],\n",
            "         [ 1.9565e-01,  1.5312e-01, -2.6387e-01, -9.0676e-01, -8.9970e-01,\n",
            "          -1.5432e-01,  2.9018e-01,  5.1112e-01,  3.9277e-01,  1.4502e-01,\n",
            "          -8.6059e-02,  1.0033e+00,  2.9766e-01, -4.0496e-02, -2.7407e-01,\n",
            "           6.2894e-01]],\n",
            "\n",
            "        [[ 2.1920e-01, -4.3338e-01, -1.7334e-02,  6.1086e-02, -5.0162e-01,\n",
            "          -9.1736e-01, -2.8565e-02, -2.9307e-01,  1.9116e-01,  4.5901e-01,\n",
            "          -6.4669e-01,  2.8410e-01,  7.1452e-01,  5.5001e-01,  7.2716e-02,\n",
            "           1.0264e+00],\n",
            "         [ 1.6208e-01,  4.7036e-01, -1.7571e-01, -1.4430e-01, -4.1618e-01,\n",
            "          -2.7120e-01,  1.7485e-01,  3.4478e-01,  2.0791e-03, -8.3833e-01,\n",
            "           4.8237e-01,  1.4978e-01,  2.6961e-01,  3.1957e-01,  3.1318e-01,\n",
            "           2.4300e-01],\n",
            "         [ 2.3199e-02,  9.1282e-01,  1.2309e-01,  4.3552e-01,  3.1683e-01,\n",
            "           5.4443e-01, -4.1182e-01, -3.9750e-01, -4.6773e-01,  1.4980e-01,\n",
            "          -7.6691e-04,  1.9398e-01, -5.9607e-02,  2.7678e-01,  3.8587e-01,\n",
            "           1.0099e-01],\n",
            "         [ 3.8775e-01, -7.5004e-01,  4.4353e-01,  2.0455e-01,  5.7050e-01,\n",
            "           5.2300e-01, -5.5296e-01,  3.4047e-01, -3.5511e-01, -6.9000e-01,\n",
            "           1.3859e-01, -6.1129e-01,  2.7986e-01, -1.0584e+00, -3.4378e-01,\n",
            "          -6.7254e-01],\n",
            "         [-7.4944e-01,  1.2011e+00,  4.7504e-01, -1.4175e+00, -1.1661e-01,\n",
            "          -2.0519e-01,  4.8880e-02, -5.6190e-01,  1.6865e-01, -5.8476e-01,\n",
            "           1.5643e-01,  2.0620e-01,  3.0129e-01,  3.0515e-01,  1.5822e-01,\n",
            "           1.3580e-01],\n",
            "         [ 3.2854e-03,  8.5792e-02, -3.7273e-01, -4.3263e-01,  2.7126e-01,\n",
            "           5.5295e-01, -3.3752e-01, -3.6228e-01,  3.7946e-01, -5.6956e-01,\n",
            "           3.5850e-01,  5.0297e-01,  8.3247e-01,  2.7065e-01,  2.3051e-01,\n",
            "          -3.7021e-01],\n",
            "         [ 2.6607e-01,  7.4628e-01,  9.7758e-01,  8.5964e-01,  7.2511e-01,\n",
            "          -6.1081e-01, -6.5677e-01, -5.0406e-02, -5.2642e-02,  5.8294e-01,\n",
            "          -4.5590e-02, -3.5460e-02,  9.0733e-01,  2.4781e-01, -1.8979e-01,\n",
            "          -9.3868e-01],\n",
            "         [-9.1529e-01, -9.2379e-01,  2.2234e-01, -3.1099e-01,  3.9580e-01,\n",
            "           5.2756e-01, -4.7417e-01, -2.0447e-01, -2.5679e-01,  3.5713e-01,\n",
            "           1.9908e-01,  7.3336e-02,  6.5161e-01, -2.3829e-01,  5.5460e-01,\n",
            "          -1.9587e-01]],\n",
            "\n",
            "        [[-1.5028e-01, -6.7611e-01, -9.4847e-02,  6.0556e-02, -1.2049e-01,\n",
            "           1.1210e-01,  5.8812e-01,  4.8340e-01, -4.8511e-01,  2.8539e-01,\n",
            "           1.1188e-01, -5.7574e-01,  1.4927e-01,  2.4169e-01, -1.1611e-01,\n",
            "          -1.2201e-01],\n",
            "         [ 3.1164e-01, -9.0459e-02, -2.8066e-01,  2.6897e-01,  6.4195e-01,\n",
            "          -6.5475e-01,  1.1037e+00, -4.5296e-01, -2.1339e-02,  1.6460e-01,\n",
            "           6.7715e-01,  2.3949e-01, -4.3220e-01,  9.4793e-01,  1.7489e-01,\n",
            "          -1.5304e-01],\n",
            "         [-7.9503e-01, -1.4741e+00,  1.1253e+00,  2.7440e-01, -1.4027e+00,\n",
            "           3.7211e-01, -3.9604e-01,  8.4127e-01,  3.5312e-01,  1.4552e-01,\n",
            "           4.2781e-01,  1.1326e+00,  1.9570e-01,  4.9587e-01,  1.9671e-01,\n",
            "           7.6903e-01],\n",
            "         [-3.3249e-01, -5.9558e-01, -2.0805e-01, -5.2000e-01, -1.0317e-01,\n",
            "          -1.0147e+00,  1.2277e-01,  5.6320e-01,  2.6245e-02, -2.8960e-01,\n",
            "           4.3904e-01,  2.1655e-01, -5.1625e-01,  6.9628e-01, -7.1387e-02,\n",
            "           5.5466e-01],\n",
            "         [ 7.1868e-01, -6.9756e-01,  1.7511e-01,  4.2946e-01, -6.4692e-02,\n",
            "           2.1727e-02,  1.6995e-01,  1.0254e-01,  1.7318e-02, -3.4716e-01,\n",
            "          -2.9683e-02,  2.2608e-01, -4.8018e-01, -3.1518e-01,  2.7052e-01,\n",
            "           2.4391e-01],\n",
            "         [-1.8101e-01,  9.1307e-01,  3.3671e-01,  2.0421e-01, -3.9466e-02,\n",
            "           4.7134e-01,  2.1301e-01,  8.5801e-01,  1.5041e-01, -3.7583e-01,\n",
            "          -1.2250e-01, -7.5942e-01,  6.6617e-02, -5.8663e-01, -1.5167e-02,\n",
            "           1.1934e-01],\n",
            "         [-3.1353e-02, -6.3727e-01, -5.9223e-01,  5.9708e-01,  2.5528e-01,\n",
            "          -1.6740e-01,  1.5375e-01, -1.4879e+00,  1.2765e-01,  1.8780e-01,\n",
            "           3.5408e-01, -4.7336e-02, -2.5101e-01,  9.1024e-01, -6.3272e-01,\n",
            "          -2.5878e-01],\n",
            "         [-1.2732e+00, -6.2869e-01,  5.6168e-02, -2.5593e-03, -7.3370e-01,\n",
            "          -2.7521e-01, -1.5650e-01,  3.9314e-01, -4.1830e-01, -1.7399e+00,\n",
            "           6.3731e-01, -6.3222e-01,  4.7992e-01,  1.8370e-01,  1.0338e+00,\n",
            "          -5.4454e-01]]], grad_fn=<UnsafeViewBackward0>)\n",
            "q  torch.Size([4, 8, 16])\n",
            "tensor([[[-6.5674e-01,  2.8302e-02,  9.4470e-03, -6.9949e-01, -3.6043e-01,\n",
            "           8.3760e-01, -4.4455e-01,  1.2278e-01,  6.2761e-01, -6.2222e-01,\n",
            "           3.4833e-01,  2.4108e-01,  5.4092e-01, -2.6054e-01,  3.6119e-01,\n",
            "          -4.3574e-02],\n",
            "         [-3.9319e-01,  8.2196e-01, -7.0274e-01,  9.5429e-02, -1.2218e-01,\n",
            "          -1.5182e-01, -5.0242e-01, -4.6365e-01,  1.1758e-01,  1.4282e+00,\n",
            "          -5.8116e-01,  1.4008e-01,  9.6041e-01,  4.1002e-02, -6.2136e-01,\n",
            "          -6.3472e-01],\n",
            "         [ 2.1567e-01, -3.5065e-01,  2.1671e-03,  4.2317e-01, -2.2844e-01,\n",
            "          -7.3162e-02, -3.4118e-01,  9.6471e-01, -5.1775e-01,  9.2104e-02,\n",
            "          -5.0425e-01,  8.3885e-01,  6.1487e-01, -1.0894e-02, -5.5692e-01,\n",
            "           5.8197e-01],\n",
            "         [ 8.9999e-01, -1.2723e-01,  5.4581e-01,  4.2544e-01, -4.5128e-01,\n",
            "          -2.1242e-02,  1.7111e-01,  2.5990e-01, -9.9782e-01,  4.8897e-01,\n",
            "           1.7374e-01, -6.9986e-02, -3.1131e-01,  3.7479e-01, -1.8482e-01,\n",
            "          -6.3789e-01],\n",
            "         [ 3.3199e-02,  5.8858e-01, -4.4368e-01,  3.7748e-01, -6.8257e-01,\n",
            "          -2.7749e-01,  4.6726e-01, -1.2956e+00,  6.6032e-01,  1.6333e-01,\n",
            "          -1.7573e+00, -6.5818e-01, -2.3023e-01, -8.6169e-02, -5.9972e-03,\n",
            "           7.5729e-01],\n",
            "         [ 2.0985e-01,  4.3915e-02, -7.0198e-02,  7.2701e-02, -2.0124e-01,\n",
            "          -1.7539e+00,  1.0369e+00,  1.1635e-01,  2.9557e-01,  3.2307e-01,\n",
            "           5.0523e-01,  7.0110e-01, -2.8444e-01, -7.8443e-01,  4.7822e-01,\n",
            "          -5.1704e-01],\n",
            "         [ 6.1001e-01, -3.2841e-01, -8.5571e-01,  8.5427e-01,  7.8055e-01,\n",
            "          -4.0234e-01, -8.1832e-01, -5.5446e-02,  1.8732e-01,  2.7065e-01,\n",
            "          -7.0659e-01, -8.6369e-01,  6.9979e-01, -6.6958e-02,  2.5508e-01,\n",
            "           2.1492e-01],\n",
            "         [ 1.4591e-01,  1.3493e-01, -2.3353e-01, -4.1732e-02,  2.9277e-01,\n",
            "          -5.0801e-01,  1.1770e-01,  1.8610e-01,  1.4554e-01,  2.9240e-02,\n",
            "          -8.4698e-01,  6.1163e-01,  1.2445e+00,  1.9087e-01,  3.6944e-01,\n",
            "          -2.7448e-03]],\n",
            "\n",
            "        [[ 1.1104e+00, -8.7192e-01,  7.0978e-01,  3.6331e-01,  2.0670e-01,\n",
            "          -3.5486e-02, -3.1695e-02,  6.9234e-01, -4.1590e-01, -1.6547e+00,\n",
            "           4.3214e-01, -1.1557e+00,  7.1400e-02, -6.7659e-01,  6.0415e-01,\n",
            "          -5.9200e-01],\n",
            "         [ 3.2561e-01,  5.7866e-01,  5.4575e-01, -7.2274e-01,  1.2343e+00,\n",
            "          -1.5586e-01,  6.8699e-01, -6.3906e-01,  6.1569e-01,  2.1342e-01,\n",
            "          -9.3616e-01,  2.7811e-01,  9.5776e-01,  1.7266e-01, -1.6889e-01,\n",
            "          -1.7047e-02],\n",
            "         [-1.5634e-02, -5.4639e-01,  3.0958e-01,  3.5532e-01,  5.9885e-01,\n",
            "          -8.2791e-01, -5.9326e-01,  7.3282e-01, -4.5197e-01, -8.4692e-01,\n",
            "           5.1515e-01, -1.0304e-02, -2.4767e-01, -6.7420e-02,  1.9623e-03,\n",
            "          -8.3188e-01],\n",
            "         [-2.4959e-01,  2.7492e-01,  2.6894e-01, -3.6563e-01, -3.2585e-01,\n",
            "           3.7158e-01, -8.7898e-01,  1.5132e-01,  3.0180e-02,  3.2213e-01,\n",
            "           3.9398e-01,  6.9950e-01,  9.7176e-02,  8.0347e-02, -1.1911e-02,\n",
            "           3.9823e-01],\n",
            "         [ 3.9181e-01,  5.7756e-01,  1.3630e-01, -3.3129e-01,  3.4955e-01,\n",
            "           3.3893e-01,  2.8573e-01, -3.3917e-01,  6.8701e-01,  3.2722e-01,\n",
            "          -1.0067e+00, -5.3265e-01,  1.0750e+00,  2.7662e-01, -5.8393e-01,\n",
            "          -3.2861e-01],\n",
            "         [ 6.6613e-01,  2.1817e+00, -4.7026e-01,  5.5768e-02, -8.0701e-01,\n",
            "           6.1819e-01,  1.8163e-01, -5.4206e-01,  8.6598e-01,  9.1274e-01,\n",
            "          -1.1465e+00,  1.2842e+00,  2.2156e+00,  8.1063e-01, -1.0830e+00,\n",
            "          -1.6162e-01],\n",
            "         [-5.6865e-01,  4.0198e-01, -5.5940e-01,  2.4041e-01,  2.5784e-02,\n",
            "          -4.5127e-01,  2.0618e-01, -1.1354e-01, -5.3368e-01,  9.9677e-01,\n",
            "          -3.4785e-01,  3.3627e-02,  1.3022e-01, -3.5643e-01,  4.5948e-01,\n",
            "          -3.3823e-01],\n",
            "         [ 5.9567e-01,  2.7697e-01, -5.3694e-01,  3.8806e-01, -5.2068e-01,\n",
            "           6.3736e-02, -4.6341e-01,  1.6976e-01, -6.2182e-01, -8.5360e-01,\n",
            "           1.5969e-02, -4.1913e-01,  7.5529e-01,  3.6444e-01,  4.0385e-01,\n",
            "          -1.9791e-01]],\n",
            "\n",
            "        [[ 1.3326e+00,  1.0350e+00, -1.3503e-02, -9.2348e-01,  1.0694e+00,\n",
            "          -1.4107e-01,  4.7608e-01, -2.5034e-01, -2.9666e-02, -4.9094e-01,\n",
            "          -6.6426e-01,  9.3041e-02,  1.4563e+00,  1.4807e-01, -6.1347e-01,\n",
            "          -1.0926e+00],\n",
            "         [ 5.4338e-01, -1.9188e-01, -5.3040e-01,  6.8131e-01,  6.2352e-02,\n",
            "          -2.8209e-01, -1.6728e-02,  5.4435e-01, -3.0124e-01, -1.1855e-01,\n",
            "          -1.8417e-01, -4.8132e-01, -2.1184e-01,  2.4121e-01,  2.1798e-02,\n",
            "          -6.9178e-02],\n",
            "         [-3.3528e-01, -5.6466e-01,  6.6484e-01, -1.5190e-01,  3.6304e-01,\n",
            "           6.2404e-01, -2.4197e-01,  1.1305e+00,  1.7005e-01,  3.6276e-01,\n",
            "           8.0700e-01, -1.0924e-01,  1.9538e-01, -4.2642e-01, -6.4841e-01,\n",
            "          -2.0725e-01],\n",
            "         [-1.0337e-01,  8.0347e-02, -3.1804e-01, -1.0417e+00,  6.4425e-02,\n",
            "          -4.2446e-01,  4.8210e-01,  2.6664e-01, -5.0266e-01, -1.4787e+00,\n",
            "           1.0776e+00,  2.1790e-01, -8.7924e-01,  1.0318e-01,  1.1044e-01,\n",
            "          -8.6890e-01],\n",
            "         [ 7.3975e-01, -5.6316e-01,  6.8825e-01,  6.8188e-01,  9.2013e-01,\n",
            "          -6.6580e-01, -2.3015e-01,  3.4094e-01,  4.4395e-01,  6.7518e-01,\n",
            "           2.6042e-01,  1.4252e+00,  7.6539e-01,  2.5805e-01, -7.9161e-01,\n",
            "           7.3077e-01],\n",
            "         [ 4.9907e-02, -4.8685e-01,  2.1094e-01, -3.5810e-01,  1.2313e-01,\n",
            "           1.5951e-01,  1.7245e-01,  2.8349e-01, -2.1648e-01, -5.0059e-01,\n",
            "          -2.0210e-01,  2.4838e-01,  1.3610e-02,  1.0566e+00,  2.7065e-01,\n",
            "          -4.7702e-01],\n",
            "         [-6.4012e-01,  4.7867e-02, -2.7659e-02, -5.3705e-01,  4.5048e-01,\n",
            "           2.2384e-01, -1.1379e+00,  5.9782e-01,  1.0890e-02, -4.5584e-01,\n",
            "           7.9715e-01,  3.0061e-01,  7.8801e-01, -2.9773e-01, -1.7181e-01,\n",
            "          -7.1184e-01],\n",
            "         [ 4.1349e-02, -6.6965e-01, -4.0473e-01, -8.1760e-01, -1.4332e-01,\n",
            "          -2.2694e-01,  8.5180e-02,  3.4696e-01,  2.9030e-02,  2.2824e-01,\n",
            "           4.9848e-01,  3.6049e-01, -4.2176e-01, -5.3471e-01, -5.0211e-02,\n",
            "           1.8603e-01]],\n",
            "\n",
            "        [[ 6.2212e-04,  3.1138e-01, -7.1241e-01, -5.4445e-01,  8.2328e-01,\n",
            "          -1.6868e-01,  2.2658e-01,  4.8862e-01, -7.2207e-01,  3.6705e-01,\n",
            "          -1.3507e-01,  3.5477e-02, -4.4308e-01, -4.5602e-01, -9.0744e-01,\n",
            "          -2.2787e-01],\n",
            "         [-8.5366e-01, -1.1014e-02, -1.6077e-01,  1.7789e-03, -1.3390e-01,\n",
            "          -4.7289e-01, -2.1686e-01,  4.3677e-01,  8.0428e-01,  1.0344e+00,\n",
            "           8.8352e-03,  2.7911e-01, -7.6171e-02, -7.9940e-01,  4.4846e-01,\n",
            "           7.0971e-01],\n",
            "         [ 1.4088e+00, -4.4377e-02, -2.5436e-03,  1.2366e-01,  5.7979e-01,\n",
            "          -7.1957e-01, -5.0969e-01, -8.0928e-01, -8.9076e-02, -7.4968e-02,\n",
            "          -8.5395e-01, -1.5098e+00, -1.1381e+00, -6.0501e-02,  1.5464e-01,\n",
            "           3.2477e-01],\n",
            "         [ 1.9741e-01, -2.5031e-01, -1.2182e-01,  4.9976e-01,  2.4592e-01,\n",
            "           6.9912e-01, -4.1537e-01, -1.3993e+00,  5.9324e-01,  7.7563e-02,\n",
            "          -1.0144e+00, -8.3186e-01,  1.7011e-01, -3.2685e-01,  6.3889e-01,\n",
            "          -9.1289e-03],\n",
            "         [-1.1387e-02,  8.6315e-01,  2.5427e-01, -3.2685e-02, -1.1675e-02,\n",
            "          -6.4872e-01,  1.3967e-01, -1.2100e-01, -3.6965e-01, -3.9830e-01,\n",
            "           2.0917e-01, -4.2211e-02,  3.5471e-01, -1.4781e-02,  1.0395e-01,\n",
            "          -8.2862e-01],\n",
            "         [-3.7612e-01, -7.3518e-02, -1.1904e+00,  7.2211e-01,  2.6136e-01,\n",
            "          -3.6523e-01,  1.0752e+00, -4.8674e-01, -4.3567e-01,  1.3338e-01,\n",
            "           6.0010e-01, -3.8806e-01, -1.5267e+00, -3.4049e-01,  3.2029e-01,\n",
            "          -3.5348e-01],\n",
            "         [-3.8176e-01,  7.8970e-01,  8.1802e-01,  8.8146e-01,  7.0618e-01,\n",
            "          -6.2861e-01, -6.7371e-01, -1.7663e-01,  5.2108e-01,  6.6437e-01,\n",
            "          -8.8697e-01,  5.0958e-02,  6.5746e-01, -6.3669e-01, -8.9697e-02,\n",
            "           3.2022e-01],\n",
            "         [-3.0550e-01,  8.9354e-02, -2.3808e-01,  1.0563e+00,  3.4164e-01,\n",
            "          -9.2939e-01,  8.5246e-01,  2.3477e-02,  1.6643e-01, -1.2088e+00,\n",
            "          -2.5446e-01,  6.6724e-01, -1.5612e-01, -1.7337e-01,  1.2187e-01,\n",
            "           2.0050e-01]]], grad_fn=<UnsafeViewBackward0>)\n",
            "wei torch.Size([4, 8, 8])\n",
            "tensor([[[-1.7629e+00, -1.3011e+00,  5.6516e-01,  2.1616e+00, -1.0674e+00,\n",
            "           1.9632e+00,  1.0765e+00, -4.5295e-01],\n",
            "         [-3.3334e+00, -1.6556e+00,  1.0405e-01,  3.3782e+00, -2.1825e+00,\n",
            "           1.0415e+00, -5.5714e-02,  2.9273e-01],\n",
            "         [-1.0226e+00, -1.2606e+00,  7.6228e-02, -3.8125e-01, -9.8430e-01,\n",
            "          -1.4303e+00,  7.4921e-02, -9.5465e-01],\n",
            "         [ 7.8359e-01, -8.0143e-01, -3.3680e-01, -8.4963e-01, -5.6023e-01,\n",
            "          -1.1701e+00, -1.2927e+00, -1.0260e+00],\n",
            "         [-1.2566e+00,  1.8719e-02, -7.8797e-01, -1.3204e+00,  2.0363e+00,\n",
            "           8.6381e-01,  3.7188e-01,  9.2577e-01],\n",
            "         [-3.1262e-01,  2.4152e+00, -1.1058e-01, -9.9305e-01,  3.3449e+00,\n",
            "          -2.5229e+00,  1.4187e+00,  1.2196e+00],\n",
            "         [ 1.0876e+00,  1.9652e+00, -2.6213e-01, -3.1579e-01,  6.0905e-01,\n",
            "           1.2616e+00, -5.4841e-01,  8.0485e-01],\n",
            "         [-1.8044e+00, -4.1260e-01, -8.3061e-01,  5.8985e-01, -7.9869e-01,\n",
            "          -5.8560e-01,  6.4332e-01,  6.3028e-01]],\n",
            "\n",
            "        [[-7.3529e-01, -1.7807e+00,  1.0745e+00, -2.7429e-01,  1.6347e+00,\n",
            "           1.4177e+00, -5.5213e-01, -2.3580e+00],\n",
            "         [-3.0892e+00, -1.4943e+00, -2.6167e-01,  2.2760e+00, -2.4364e-01,\n",
            "           1.6198e-01,  2.5783e+00,  3.9591e-01],\n",
            "         [-5.0206e-01, -2.0745e+00,  5.3785e-01, -4.0494e-01,  8.3292e-01,\n",
            "           1.3570e+00, -1.5621e+00, -1.6490e+00],\n",
            "         [ 1.3810e+00, -1.4713e-01,  1.2181e+00, -2.2266e-01, -1.8247e+00,\n",
            "          -3.7044e+00, -2.1321e+00,  1.3178e+00],\n",
            "         [-2.3568e+00, -4.6170e-01, -8.8196e-01,  2.3700e+00,  6.7828e-01,\n",
            "           1.6262e-01,  1.9379e+00,  1.0397e-01],\n",
            "         [-9.2435e-01, -6.2351e-01, -1.3938e+00,  1.3336e+00, -8.9731e-03,\n",
            "          -3.1789e+00,  9.0259e-01,  3.6256e+00],\n",
            "         [-6.5522e-01,  1.0991e+00, -2.1399e+00,  9.6468e-01,  9.9463e-01,\n",
            "           9.3899e-01,  4.6799e-01, -3.5870e-01],\n",
            "         [ 1.5463e+00, -4.9438e-01, -1.4180e-02, -9.7428e-01,  1.3779e+00,\n",
            "           7.8648e-03, -5.3590e-01, -4.5531e-01]],\n",
            "\n",
            "        [[-3.7898e-01,  5.1592e-01,  3.0332e-01,  1.1303e+00,  2.0511e+00,\n",
            "           2.2323e+00,  3.1239e+00, -1.2231e+00],\n",
            "         [ 1.0377e-01,  1.7584e-01, -1.6369e-01,  5.2328e-01, -2.2172e+00,\n",
            "          -8.7770e-01,  1.7020e-01, -1.0842e+00],\n",
            "         [-1.6373e+00, -6.5557e-01, -8.5031e-01,  2.3457e+00, -9.9497e-01,\n",
            "          -4.9228e-02,  5.5157e-01,  1.5285e+00],\n",
            "         [-2.7155e+00,  1.9022e+00, -8.4620e-01,  5.9058e-01,  2.1122e+00,\n",
            "           8.8971e-01, -2.0679e+00, -7.4249e-01],\n",
            "         [ 2.5044e+00, -4.9691e-01, -2.6300e-01, -1.6288e-01, -1.7459e+00,\n",
            "           8.6298e-02,  2.7739e+00, -2.4952e-02],\n",
            "         [-4.8634e-02,  4.9620e-01, -2.0859e-01, -8.4632e-02,  3.6811e-01,\n",
            "           7.8713e-01, -1.9678e-01,  4.1090e-01],\n",
            "         [-1.7485e+00,  4.6233e-01,  3.8654e-03,  2.1114e+00,  1.2731e+00,\n",
            "           2.1582e+00,  1.3125e+00,  2.0600e+00],\n",
            "         [-8.5500e-02, -1.5414e-02, -1.3915e+00,  6.3086e-02, -2.4530e-01,\n",
            "          -2.0677e-01, -2.2102e+00,  4.4531e-01]],\n",
            "\n",
            "        [[ 4.5165e-01,  3.2148e-01, -3.1926e+00,  3.0765e-01, -6.1612e-01,\n",
            "           2.5626e-01, -2.9891e-01, -2.1917e+00],\n",
            "         [-4.0009e-01, -9.6205e-01,  1.9568e+00,  6.6612e-01, -3.2630e-01,\n",
            "           2.6258e-01, -1.3973e+00, -8.9450e-01],\n",
            "         [-4.6199e-01,  5.8600e-01, -4.6738e+00, -3.2178e-01,  1.2684e+00,\n",
            "          -1.7402e-01,  1.2461e+00, -2.2283e+00],\n",
            "         [-7.1746e-01, -1.0279e+00, -2.0509e+00, -2.7234e+00,  3.1231e-01,\n",
            "          -1.6416e-01,  1.5162e+00, -7.7670e-01],\n",
            "         [-4.0388e-01,  5.1597e-01, -2.0697e+00, -4.0982e-01, -8.0534e-01,\n",
            "           5.2210e-01, -4.1242e-01,  1.3377e+00],\n",
            "         [ 8.2322e-01,  3.0237e+00, -3.0655e+00,  7.0404e-01,  6.7207e-01,\n",
            "          -4.6692e-01,  2.3746e+00,  3.1181e-01],\n",
            "         [-1.4141e+00, -1.4241e+00, -8.0387e-01, -1.7450e+00, -7.4035e-01,\n",
            "           9.8188e-01, -9.0056e-01, -2.3158e+00],\n",
            "         [-5.0277e-01,  1.6844e+00, -4.1847e-01,  1.0239e+00,  1.0275e+00,\n",
            "           1.3980e-01,  4.8822e-01,  1.5573e+00]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "tril torch.Size([8, 8])\n",
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "wei torch.Size([4, 8, 8])\n",
            "tensor([[[-1.7629e+00,        -inf,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-3.3334e+00, -1.6556e+00,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-1.0226e+00, -1.2606e+00,  7.6228e-02,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [ 7.8359e-01, -8.0143e-01, -3.3680e-01, -8.4963e-01,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-1.2566e+00,  1.8719e-02, -7.8797e-01, -1.3204e+00,  2.0363e+00,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-3.1262e-01,  2.4152e+00, -1.1058e-01, -9.9305e-01,  3.3449e+00,\n",
            "          -2.5229e+00,        -inf,        -inf],\n",
            "         [ 1.0876e+00,  1.9652e+00, -2.6213e-01, -3.1579e-01,  6.0905e-01,\n",
            "           1.2616e+00, -5.4841e-01,        -inf],\n",
            "         [-1.8044e+00, -4.1260e-01, -8.3061e-01,  5.8985e-01, -7.9869e-01,\n",
            "          -5.8560e-01,  6.4332e-01,  6.3028e-01]],\n",
            "\n",
            "        [[-7.3529e-01,        -inf,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-3.0892e+00, -1.4943e+00,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-5.0206e-01, -2.0745e+00,  5.3785e-01,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [ 1.3810e+00, -1.4713e-01,  1.2181e+00, -2.2266e-01,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-2.3568e+00, -4.6170e-01, -8.8196e-01,  2.3700e+00,  6.7828e-01,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-9.2435e-01, -6.2351e-01, -1.3938e+00,  1.3336e+00, -8.9731e-03,\n",
            "          -3.1789e+00,        -inf,        -inf],\n",
            "         [-6.5522e-01,  1.0991e+00, -2.1399e+00,  9.6468e-01,  9.9463e-01,\n",
            "           9.3899e-01,  4.6799e-01,        -inf],\n",
            "         [ 1.5463e+00, -4.9438e-01, -1.4180e-02, -9.7428e-01,  1.3779e+00,\n",
            "           7.8648e-03, -5.3590e-01, -4.5531e-01]],\n",
            "\n",
            "        [[-3.7898e-01,        -inf,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [ 1.0377e-01,  1.7584e-01,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-1.6373e+00, -6.5557e-01, -8.5031e-01,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-2.7155e+00,  1.9022e+00, -8.4620e-01,  5.9058e-01,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [ 2.5044e+00, -4.9691e-01, -2.6300e-01, -1.6288e-01, -1.7459e+00,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-4.8634e-02,  4.9620e-01, -2.0859e-01, -8.4632e-02,  3.6811e-01,\n",
            "           7.8713e-01,        -inf,        -inf],\n",
            "         [-1.7485e+00,  4.6233e-01,  3.8654e-03,  2.1114e+00,  1.2731e+00,\n",
            "           2.1582e+00,  1.3125e+00,        -inf],\n",
            "         [-8.5500e-02, -1.5414e-02, -1.3915e+00,  6.3086e-02, -2.4530e-01,\n",
            "          -2.0677e-01, -2.2102e+00,  4.4531e-01]],\n",
            "\n",
            "        [[ 4.5165e-01,        -inf,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-4.0009e-01, -9.6205e-01,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-4.6199e-01,  5.8600e-01, -4.6738e+00,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-7.1746e-01, -1.0279e+00, -2.0509e+00, -2.7234e+00,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-4.0388e-01,  5.1597e-01, -2.0697e+00, -4.0982e-01, -8.0534e-01,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [ 8.2322e-01,  3.0237e+00, -3.0655e+00,  7.0404e-01,  6.7207e-01,\n",
            "          -4.6692e-01,        -inf,        -inf],\n",
            "         [-1.4141e+00, -1.4241e+00, -8.0387e-01, -1.7450e+00, -7.4035e-01,\n",
            "           9.8188e-01, -9.0056e-01,        -inf],\n",
            "         [-5.0277e-01,  1.6844e+00, -4.1847e-01,  1.0239e+00,  1.0275e+00,\n",
            "           1.3980e-01,  4.8822e-01,  1.5573e+00]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "wei torch.Size([4, 8, 8])\n",
            "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
            "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
            "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
            "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
            "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
            "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
            "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
            "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
            "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "v torch.Size([4, 8, 16])\n",
            "tensor([[[-1.5713e-01,  8.8009e-01,  1.6152e-01, -7.8239e-01, -1.4289e-01,\n",
            "           7.4676e-01,  1.0068e-01, -5.2395e-01, -8.8726e-01,  1.9068e-01,\n",
            "           1.7616e-01, -5.9426e-01, -4.8124e-01, -4.8599e-01,  2.8623e-01,\n",
            "           5.7099e-01],\n",
            "         [ 8.3212e-01, -8.1437e-01, -3.2425e-01,  5.1913e-01, -1.2520e-01,\n",
            "          -4.8982e-01, -5.2867e-01, -3.1393e-02,  1.0723e-01,  8.2692e-01,\n",
            "           8.1322e-01, -2.7132e-02,  4.7754e-01,  4.9801e-01, -1.3769e-01,\n",
            "           1.4025e+00],\n",
            "         [ 6.0346e-01, -2.4995e-01, -6.1588e-01,  4.0678e-01,  3.3283e-01,\n",
            "          -3.9097e-01,  1.3119e-01,  2.1718e-01, -1.2991e-01, -8.8281e-01,\n",
            "           1.7242e-01,  4.6522e-01, -4.2710e-01, -7.6754e-02, -2.8524e-01,\n",
            "           1.3875e+00],\n",
            "         [ 6.6568e-01, -7.0960e-01, -6.0986e-01,  4.3484e-01,  8.9754e-01,\n",
            "          -9.2983e-01,  6.8325e-02,  1.8632e-01,  5.4002e-01,  2.4271e-01,\n",
            "          -6.9225e-01,  4.9774e-01,  4.8503e-01,  6.6076e-01,  8.7671e-01,\n",
            "           7.4566e-02],\n",
            "         [ 1.5357e-01,  1.0439e+00,  8.4574e-01,  2.3882e-01,  3.0046e-01,\n",
            "           1.0516e+00,  7.6373e-01,  4.5166e-01, -7.4263e-01, -1.4395e+00,\n",
            "          -4.9412e-01, -3.7087e-01, -1.1819e+00,  1.0001e-01, -1.8065e-01,\n",
            "           5.1291e-01],\n",
            "         [-8.9198e-01,  5.7820e-02, -3.3504e-01,  8.4768e-01,  3.8764e-01,\n",
            "           1.6644e-01, -4.5871e-01, -5.9737e-01,  4.9612e-01,  6.5476e-01,\n",
            "           5.4789e-02,  9.4680e-01,  4.5108e-01,  1.1999e-01,  1.0573e+00,\n",
            "          -2.2570e-01],\n",
            "         [-4.8492e-01,  1.6553e-01, -2.2215e-01, -1.3454e-01, -8.6441e-02,\n",
            "          -6.6281e-01, -9.3596e-02,  1.0496e-01, -2.6121e-01,  1.8538e-01,\n",
            "           3.1711e-01, -1.3927e-01,  5.4862e-01, -4.0864e-01, -3.8507e-01,\n",
            "           7.1057e-01],\n",
            "         [ 2.0424e-01,  3.7717e-01, -1.1255e+00,  3.9950e-01,  1.4892e-01,\n",
            "           3.5902e-01, -1.7912e-01,  1.3732e+00,  1.5880e-01, -2.3202e-01,\n",
            "           1.6507e-01,  7.6043e-01,  3.5211e-01, -1.0864e+00, -7.9393e-01,\n",
            "          -3.0253e-01]],\n",
            "\n",
            "        [[-1.3254e+00,  1.1236e+00,  2.2927e-01, -2.9970e-01, -7.6267e-03,\n",
            "           7.9364e-01,  8.9581e-01,  3.9650e-01, -6.6613e-01, -2.1844e-01,\n",
            "          -1.3539e+00,  4.1245e-01,  9.6011e-01, -1.0805e+00, -3.9751e-01,\n",
            "          -4.4439e-01],\n",
            "         [-1.9221e-01, -4.6449e-01,  5.9880e-02,  2.8408e-01, -1.0312e-01,\n",
            "          -1.7967e-03,  1.8920e-01, -3.7337e-01, -9.8137e-02,  2.3116e-02,\n",
            "           8.5743e-01,  5.6841e-01, -2.1939e-01, -2.9158e-01, -2.0158e-01,\n",
            "          -4.6876e-01],\n",
            "         [-1.1012e+00,  9.8266e-02,  5.8596e-01, -5.6413e-03,  3.7330e-01,\n",
            "          -6.1363e-02,  2.8833e-02,  2.6230e-01,  6.4099e-01,  7.1003e-02,\n",
            "           3.6877e-01,  5.0011e-01,  7.3872e-01,  1.1909e-01,  5.4246e-01,\n",
            "           6.8950e-02],\n",
            "         [ 4.9074e-01, -2.9978e-01,  1.0949e+00,  1.0131e+00,  3.5883e-01,\n",
            "           9.5771e-01, -1.8349e-01,  1.4002e-01,  1.4243e-01,  8.0787e-01,\n",
            "          -2.4476e-01,  1.3392e-01,  2.6700e-01,  3.2605e-01,  2.0296e-01,\n",
            "           1.4967e-01],\n",
            "         [ 4.5700e-02,  1.0993e+00,  4.6545e-01, -1.5803e-01, -7.2921e-01,\n",
            "           5.8145e-01,  4.0171e-01,  1.3040e+00, -2.2263e-02,  3.9847e-01,\n",
            "           6.3218e-01, -1.4205e-01,  5.0596e-01, -2.9585e-01, -3.5306e-02,\n",
            "          -7.2087e-01],\n",
            "         [ 3.6249e-01,  3.1444e-01,  3.7844e-01, -3.3100e-01, -1.1213e+00,\n",
            "          -6.8686e-01, -6.5431e-01, -2.1805e-01, -2.6552e-01,  6.7712e-01,\n",
            "           3.9176e-01, -1.3338e+00,  3.7350e-01, -1.1095e+00,  3.7270e-01,\n",
            "          -9.3442e-01],\n",
            "         [-2.0881e-01, -7.6620e-02, -1.5674e-01,  1.4457e-01,  8.7035e-01,\n",
            "           2.1136e-01, -4.8995e-01,  2.4986e-01,  5.1811e-01,  6.6507e-01,\n",
            "           3.2814e-01,  4.6015e-01,  9.2321e-01, -4.5579e-01, -4.8577e-01,\n",
            "          -2.7199e-01],\n",
            "         [-1.8408e-01,  1.7347e-01,  1.4034e-02, -4.8221e-01, -5.2118e-01,\n",
            "          -2.6668e-01, -1.0874e-01,  2.0809e-01,  3.0165e-01,  5.3594e-02,\n",
            "          -3.7746e-01, -7.4163e-01,  8.8692e-04, -1.2250e+00,  3.0022e-01,\n",
            "          -5.0357e-01]],\n",
            "\n",
            "        [[ 6.8925e-02,  1.2248e+00, -4.1194e-01, -1.7046e-01, -6.9224e-01,\n",
            "          -2.9201e-01,  1.2704e+00, -6.8596e-01,  4.3798e-01, -2.6366e-01,\n",
            "           1.1528e-01,  1.1676e+00, -7.2138e-01, -1.2308e+00,  8.3821e-01,\n",
            "          -5.5987e-01],\n",
            "         [-9.5939e-01,  9.2166e-02,  7.7470e-02, -9.8325e-02, -5.0263e-01,\n",
            "          -7.0076e-01, -7.3248e-01,  1.8081e-02,  4.7626e-01, -1.1356e-01,\n",
            "           2.6368e-01, -3.6124e-01, -2.1905e-02, -3.4626e-01, -1.0357e-01,\n",
            "           6.5548e-01],\n",
            "         [-5.7584e-01, -3.0022e-01, -6.9503e-02, -9.9645e-02, -2.8187e-01,\n",
            "          -6.7841e-01, -1.4310e-01, -3.7591e-01,  5.7496e-01,  4.6758e-04,\n",
            "           9.1726e-01,  1.6101e-01, -4.4098e-01,  5.3701e-03,  7.9788e-01,\n",
            "           5.6693e-01],\n",
            "         [ 3.4514e-01,  3.0841e-01,  1.0998e-01, -2.6316e-01,  1.0666e+00,\n",
            "          -5.6067e-02, -6.9560e-01,  3.0091e-01, -2.7254e-01,  8.2122e-01,\n",
            "          -8.6185e-01,  6.1082e-02, -1.2083e-01,  4.1112e-01, -1.0277e-01,\n",
            "          -2.9790e-01],\n",
            "         [-1.8289e+00, -8.6379e-01, -7.9821e-01,  2.4173e-01, -5.0344e-01,\n",
            "          -1.0447e+00,  8.7287e-01,  5.0584e-01,  5.6657e-02, -3.1938e-01,\n",
            "           1.0980e+00,  1.1729e+00, -5.4148e-01, -1.0805e+00,  7.3217e-02,\n",
            "          -2.8329e-01],\n",
            "         [-3.5718e-01, -3.2740e-01, -6.9867e-01,  7.8014e-01,  4.2778e-01,\n",
            "           3.3665e-01,  5.5142e-02,  5.9465e-01,  6.4841e-01, -8.7773e-02,\n",
            "          -4.3907e-02,  6.5681e-01,  1.2646e-01,  2.5969e-01,  6.7423e-01,\n",
            "          -7.6637e-01],\n",
            "         [ 7.6206e-01,  4.9035e-01,  8.2749e-01,  3.7294e-01, -7.1975e-01,\n",
            "          -3.3127e-01, -8.6443e-01, -1.6571e-03, -5.9054e-01,  6.3868e-01,\n",
            "           2.2889e-01, -5.5488e-02,  2.9504e-01,  5.3679e-01, -7.7014e-01,\n",
            "           4.9259e-01],\n",
            "         [ 4.3940e-01,  2.4456e-01, -6.1958e-01,  5.1417e-01,  8.1137e-01,\n",
            "           2.7439e-01,  1.6661e-01,  5.0555e-02,  9.1574e-02,  8.9894e-01,\n",
            "          -1.0681e-01,  3.1970e-01, -7.3390e-02,  3.0807e-01,  7.9702e-01,\n",
            "           7.5018e-01]],\n",
            "\n",
            "        [[ 9.7183e-02,  5.7301e-02, -1.0468e-01, -4.6654e-02, -1.4006e-01,\n",
            "          -8.4126e-01, -1.3625e-01, -6.7465e-01, -2.1541e-01,  1.0993e+00,\n",
            "           2.3427e-01,  3.2605e-02, -1.8521e-01,  1.4780e-01, -6.1045e-01,\n",
            "           1.5391e+00],\n",
            "         [ 3.6123e-01, -6.7973e-01, -7.7090e-01,  6.4828e-01, -2.4451e-01,\n",
            "          -5.7902e-01, -1.5354e+00, -7.2195e-01, -1.8834e-01,  1.0884e-02,\n",
            "           2.3991e-01, -5.4473e-02, -1.4373e-01,  4.9291e-02, -8.8639e-01,\n",
            "           7.2397e-01],\n",
            "         [-1.0977e-01,  8.0600e-01,  8.1140e-01, -3.4001e-01, -4.5837e-01,\n",
            "           5.4328e-03,  1.3075e+00, -7.7781e-01, -6.2820e-01,  7.4216e-02,\n",
            "          -2.1868e-01,  1.8126e-01, -2.0854e-01,  6.7201e-01,  6.9363e-02,\n",
            "           9.8662e-01],\n",
            "         [ 3.0428e-01,  1.1563e+00,  1.3803e-01, -2.0818e+00, -1.0470e-01,\n",
            "           5.2292e-01,  1.2301e+00,  5.3652e-01, -9.0009e-01, -1.0794e+00,\n",
            "          -2.4331e-01,  9.7983e-04,  2.4827e-01,  4.4169e-02, -6.7854e-01,\n",
            "          -3.3345e-01],\n",
            "         [-5.3004e-01, -9.2135e-01,  3.7915e-01, -2.0732e-02,  3.7330e-01,\n",
            "          -1.6131e-01, -7.0930e-01,  4.2039e-02,  1.6151e-01,  1.6618e-01,\n",
            "           5.6694e-01,  5.5056e-01, -7.1126e-02, -5.5536e-01, -1.2077e-01,\n",
            "          -4.5284e-01],\n",
            "         [-6.9652e-01,  4.4457e-01,  8.0947e-01, -6.0359e-01,  4.7886e-02,\n",
            "          -4.6401e-01, -2.0967e-01,  5.5984e-01,  5.7196e-01,  3.6429e-01,\n",
            "           5.9383e-02, -1.3565e+00,  6.8667e-01,  5.4511e-01, -6.7370e-01,\n",
            "           6.3525e-01],\n",
            "         [ 3.5459e-01,  1.1575e-01, -4.2291e-01, -4.7040e-01, -2.2670e-01,\n",
            "           1.5671e-01, -2.1000e-01, -1.0505e+00, -1.0665e+00, -8.3185e-01,\n",
            "           1.9891e-01,  9.0778e-01,  3.5189e-01,  5.6643e-02, -6.4876e-01,\n",
            "           5.5124e-02],\n",
            "         [-1.7223e+00,  5.1077e-01,  2.9681e-01,  2.3290e-01,  2.4183e-01,\n",
            "           3.3723e-01, -2.5232e-01,  6.4762e-01, -1.4068e+00, -6.4379e-01,\n",
            "           7.4489e-02, -5.8730e-01,  1.2959e-01, -2.1585e-01, -7.5063e-01,\n",
            "           3.2311e-01]]], grad_fn=<UnsafeViewBackward0>)\n",
            "out torch.Size([4, 8, 16])\n",
            "tensor([[[-1.5713e-01,  8.8009e-01,  1.6152e-01, -7.8239e-01, -1.4289e-01,\n",
            "           7.4676e-01,  1.0068e-01, -5.2395e-01, -8.8726e-01,  1.9068e-01,\n",
            "           1.7616e-01, -5.9426e-01, -4.8124e-01, -4.8599e-01,  2.8623e-01,\n",
            "           5.7099e-01],\n",
            "         [ 6.7643e-01, -5.4770e-01, -2.4780e-01,  3.1430e-01, -1.2799e-01,\n",
            "          -2.9521e-01, -4.2962e-01, -1.0891e-01, -4.9282e-02,  7.2679e-01,\n",
            "           7.1296e-01, -1.1639e-01,  3.2665e-01,  3.4315e-01, -7.0975e-02,\n",
            "           1.2716e+00],\n",
            "         [ 4.8227e-01, -1.0688e-01, -4.0555e-01,  1.7696e-01,  1.5811e-01,\n",
            "          -1.6967e-01,  1.6217e-02,  2.1509e-02, -2.4903e-01, -3.7725e-01,\n",
            "           2.7867e-01,  1.6295e-01, -2.8951e-01, -6.7610e-02, -1.4162e-01,\n",
            "           1.2194e+00],\n",
            "         [ 1.9708e-01,  2.8561e-01, -1.3028e-01, -2.6552e-01,  6.6781e-02,\n",
            "           1.9535e-01,  2.8073e-02, -2.4511e-01, -4.6466e-01,  6.9287e-02,\n",
            "           1.5284e-01, -2.0324e-01, -2.4789e-01, -1.6213e-01,  1.9474e-01,\n",
            "           7.6778e-01],\n",
            "         [ 2.5104e-01,  7.3457e-01,  5.9385e-01,  2.5159e-01,  2.6064e-01,\n",
            "           7.5820e-01,  5.5947e-01,  3.5387e-01, -5.9338e-01, -1.0807e+00,\n",
            "          -3.1110e-01, -2.7809e-01, -9.0541e-01,  1.3181e-01, -1.3818e-01,\n",
            "           6.3715e-01],\n",
            "         [ 3.4277e-01,  4.9605e-01,  4.7248e-01,  3.0277e-01,  1.8440e-01,\n",
            "           5.8144e-01,  3.8245e-01,  2.9521e-01, -4.8969e-01, -7.7051e-01,\n",
            "          -1.1721e-01, -2.5412e-01, -6.8921e-01,  1.9795e-01, -1.5135e-01,\n",
            "           7.6659e-01],\n",
            "         [ 1.8658e-01, -9.6351e-02, -1.4300e-01,  3.0587e-01,  8.3441e-02,\n",
            "          -6.8646e-03, -2.0472e-01, -1.5350e-01, -7.6250e-02,  3.2689e-01,\n",
            "           3.0896e-01,  7.6626e-02,  9.9243e-02,  1.6560e-01,  1.9745e-01,\n",
            "           7.6248e-01],\n",
            "         [ 1.3013e-01, -3.2832e-02, -4.9645e-01,  2.8652e-01,  2.7042e-01,\n",
            "          -2.6357e-01, -7.3756e-02,  3.7857e-01,  7.4580e-02,  3.3827e-02,\n",
            "           1.4695e-02,  3.1937e-01,  2.9926e-01, -1.6530e-01, -3.8630e-02,\n",
            "           3.3748e-01]],\n",
            "\n",
            "        [[-1.3254e+00,  1.1236e+00,  2.2927e-01, -2.9970e-01, -7.6267e-03,\n",
            "           7.9364e-01,  8.9581e-01,  3.9650e-01, -6.6613e-01, -2.1844e-01,\n",
            "          -1.3539e+00,  4.1245e-01,  9.6011e-01, -1.0805e+00, -3.9751e-01,\n",
            "          -4.4439e-01],\n",
            "         [-3.8338e-01, -1.9659e-01,  8.8455e-02,  1.8560e-01, -8.7010e-02,\n",
            "           1.3239e-01,  3.0841e-01, -2.4350e-01, -1.9396e-01, -1.7634e-02,\n",
            "           4.8439e-01,  5.4210e-01, -2.0407e-02, -4.2467e-01, -2.3463e-01,\n",
            "          -4.6465e-01],\n",
            "         [-1.1100e+00,  3.2334e-01,  4.7054e-01, -6.3595e-02,  2.5443e-01,\n",
            "           1.5352e-01,  2.5186e-01,  2.6286e-01,  2.7916e-01, -3.1662e-03,\n",
            "          -3.2880e-02,  4.8191e-01,  7.4431e-01, -1.9921e-01,  2.7134e-01,\n",
            "          -8.5871e-02],\n",
            "         [-9.7190e-01,  4.6124e-01,  4.2349e-01, -1.7230e-02,  1.5847e-01,\n",
            "           4.1175e-01,  4.0764e-01,  2.4982e-01, -5.0322e-02,  4.1514e-03,\n",
            "          -3.9853e-01,  4.3551e-01,  7.0285e-01, -4.3081e-01,  2.6684e-02,\n",
            "          -2.0169e-01],\n",
            "         [ 3.3586e-01, -8.5915e-02,  9.3660e-01,  7.7311e-01,  1.8037e-01,\n",
            "           8.2853e-01, -6.9183e-02,  2.8814e-01,  1.1734e-01,  6.8448e-01,\n",
            "          -5.8500e-02,  1.2726e-01,  2.9780e-01,  1.9324e-01,  1.5655e-01,\n",
            "          -9.3005e-03],\n",
            "         [ 1.6984e-01,  3.0993e-02,  8.1557e-01,  6.1679e-01,  1.0429e-01,\n",
            "           7.4573e-01,  2.3072e-02,  3.0572e-01,  5.8163e-02,  5.7122e-01,\n",
            "          -4.5275e-02,  1.5051e-01,  3.2901e-01,  5.6984e-02,  1.0311e-01,\n",
            "          -9.9174e-02],\n",
            "         [ 4.6497e-02,  1.5765e-01,  3.9760e-01,  1.7619e-01, -2.1168e-01,\n",
            "           2.3365e-01, -6.2083e-02,  2.1726e-01, -7.8725e-03,  4.5389e-01,\n",
            "           3.4349e-01, -5.5631e-02,  3.3726e-01, -3.7591e-01, -1.0140e-02,\n",
            "          -4.5806e-01],\n",
            "         [-5.3896e-01,  7.5555e-01,  3.3034e-01, -1.5849e-01, -2.6740e-01,\n",
            "           4.3495e-01,  3.7772e-01,  5.5794e-01, -1.8369e-01,  1.5938e-01,\n",
            "          -2.1042e-01,  5.5790e-02,  6.3184e-01, -6.4884e-01, -9.6084e-02,\n",
            "          -5.0751e-01]],\n",
            "\n",
            "        [[ 6.8925e-02,  1.2248e+00, -4.1194e-01, -1.7046e-01, -6.9224e-01,\n",
            "          -2.9201e-01,  1.2704e+00, -6.8596e-01,  4.3798e-01, -2.6366e-01,\n",
            "           1.1528e-01,  1.1676e+00, -7.2138e-01, -1.2308e+00,  8.3821e-01,\n",
            "          -5.5987e-01],\n",
            "         [-4.6375e-01,  6.3807e-01, -1.5842e-01, -1.3309e-01, -5.9402e-01,\n",
            "          -5.0374e-01,  2.3289e-01, -3.2126e-01,  4.5781e-01, -1.8590e-01,\n",
            "           1.9215e-01,  3.7566e-01, -3.5905e-01, -7.7262e-01,  3.5036e-01,\n",
            "           6.9694e-02],\n",
            "         [-6.4044e-01,  1.3831e-01, -6.1007e-02, -1.1112e-01, -4.5228e-01,\n",
            "          -6.2271e-01, -1.7030e-01, -2.4949e-01,  5.0670e-01, -9.6444e-02,\n",
            "           4.8315e-01,  9.4986e-02, -2.9810e-01, -3.6538e-01,  3.9458e-01,\n",
            "           4.1512e-01],\n",
            "         [-6.7193e-01,  1.2516e-01,  7.3386e-02, -1.3198e-01, -1.7880e-01,\n",
            "          -5.6740e-01, -6.8226e-01,  5.0844e-02,  3.3051e-01,  7.8242e-02,\n",
            "           6.8022e-02, -2.4041e-01, -6.6864e-02, -1.8411e-01, -5.3514e-02,\n",
            "           4.5113e-01],\n",
            "         [-1.4270e-02,  1.0195e+00, -3.4792e-01, -1.6421e-01, -5.5846e-01,\n",
            "          -3.2457e-01,  9.9404e-01, -5.6891e-01,  4.0097e-01, -1.8123e-01,\n",
            "           1.1856e-01,  9.8704e-01, -6.4057e-01, -1.0320e+00,  7.3320e-01,\n",
            "          -4.3167e-01],\n",
            "         [-6.3858e-01, -7.6533e-02, -3.6510e-01,  1.7782e-01, -6.5426e-02,\n",
            "          -3.5158e-01,  7.9591e-02,  1.7384e-01,  3.6676e-01, -4.2302e-02,\n",
            "           2.4923e-01,  4.8239e-01, -2.1295e-01, -2.9492e-01,  3.4749e-01,\n",
            "          -1.7111e-01],\n",
            "         [-2.2366e-01, -5.5317e-02, -1.8296e-01,  2.4258e-01,  2.5357e-01,\n",
            "          -1.6154e-01, -2.3908e-01,  3.3243e-01,  1.0304e-01,  2.6067e-01,\n",
            "          -5.0670e-02,  3.6947e-01, -4.9856e-02,  1.1197e-01,  1.1752e-01,\n",
            "          -2.5078e-01],\n",
            "         [-2.4821e-01,  1.4845e-01, -3.5033e-01,  1.7102e-01,  1.6613e-01,\n",
            "          -2.0643e-01,  8.6633e-02,  8.8414e-02,  2.1188e-01,  2.5805e-01,\n",
            "           5.5145e-02,  4.2668e-01, -2.0443e-01, -1.7372e-01,  3.8899e-01,\n",
            "           5.1725e-02]],\n",
            "\n",
            "        [[ 9.7183e-02,  5.7301e-02, -1.0468e-01, -4.6654e-02, -1.4006e-01,\n",
            "          -8.4126e-01, -1.3625e-01, -6.7465e-01, -2.1541e-01,  1.0993e+00,\n",
            "           2.3427e-01,  3.2605e-02, -1.8521e-01,  1.4780e-01, -6.1045e-01,\n",
            "           1.5391e+00],\n",
            "         [ 1.9305e-01, -2.1031e-01, -3.4658e-01,  2.0567e-01, -1.7799e-01,\n",
            "          -7.4604e-01, -6.4427e-01, -6.9183e-01, -2.0558e-01,  7.0413e-01,\n",
            "           2.3632e-01,  9.8800e-04, -1.7015e-01,  1.1203e-01, -7.1064e-01,\n",
            "           1.2431e+00],\n",
            "         [ 2.9114e-01, -4.8343e-01, -5.9254e-01,  4.6477e-01, -2.1832e-01,\n",
            "          -6.4460e-01, -1.1627e+00, -7.0993e-01, -1.9703e-01,  2.9262e-01,\n",
            "           2.3669e-01, -3.1050e-02, -1.5471e-01,  7.7153e-02, -8.1137e-01,\n",
            "           9.3578e-01],\n",
            "         [ 1.7549e-01, -3.4260e-02, -2.0523e-01,  2.7644e-02, -2.1312e-01,\n",
            "          -5.6022e-01, -3.5273e-01, -6.2722e-01, -3.0037e-01,  4.6061e-01,\n",
            "           1.5004e-01,  1.9040e-02, -1.4646e-01,  1.7220e-01, -6.2559e-01,\n",
            "           1.0722e+00],\n",
            "         [ 1.7354e-01, -1.7962e-01, -2.7874e-01, -1.0590e-01, -1.2952e-01,\n",
            "          -3.5086e-01, -5.5830e-01, -3.8638e-01, -2.9719e-01,  3.3368e-02,\n",
            "           1.7392e-01,  5.5898e-02, -7.2007e-02,  1.3182e-02, -6.6710e-01,\n",
            "           5.4229e-01],\n",
            "         [ 2.4678e-01, -4.7274e-01, -5.2827e-01,  3.1212e-01, -1.7528e-01,\n",
            "          -4.8636e-01, -1.1223e+00, -5.4196e-01, -2.0142e-01,  4.0103e-02,\n",
            "           2.2231e-01, -2.9380e-02, -9.4354e-02,  2.6374e-02, -7.8726e-01,\n",
            "           6.2836e-01],\n",
            "         [-3.9784e-01,  2.5915e-01,  5.0358e-01, -4.6864e-01, -2.2024e-02,\n",
            "          -3.2242e-01, -1.2578e-01,  1.0634e-01,  1.3618e-01,  1.7780e-01,\n",
            "           1.0391e-01, -6.2540e-01,  3.8904e-01,  3.3690e-01, -5.5140e-01,\n",
            "           5.2246e-01],\n",
            "         [-3.5927e-01,  3.3935e-02, -2.9863e-02, -1.5019e-01, -6.0354e-03,\n",
            "          -6.5733e-02, -3.9659e-01, -6.0435e-02, -5.7551e-01, -2.9157e-01,\n",
            "           1.4899e-01, -7.5002e-02,  7.3228e-02, -4.7413e-02, -6.4394e-01,\n",
            "           2.8560e-01]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t08svm4vWoZ",
        "outputId": "a8dd0acd-493b-459e-f12a-4e17c29f20a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=32, out_features=16, bias=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ],
      "metadata": {
        "id": "M5CvobiQ0pLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ],
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ],
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    # x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel()\n",
        "\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "\n",
        "logits, loss = m(xb, yb)\n",
        "\n",
        "\n",
        "\n",
        "# m = m.to(device)\n",
        "# print the number of parameters in the model\n",
        "# print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# # create a PyTorch optimizer\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# for iter in range(max_iters):\n",
        "\n",
        "#     # every once in a while evaluate the loss on train and val sets\n",
        "#     if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "#         losses = estimate_loss()\n",
        "#         print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "#     # sample a batch of data\n",
        "#     xb, yb = get_batch('train')\n",
        "\n",
        "#     # evaluate the loss\n",
        "#     logits, loss = model(xb, yb)\n",
        "#     optimizer.zero_grad(set_to_none=True)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "# # generate from the model\n",
        "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "# print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "hoelkOrFY8bN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])"
      ],
      "metadata": {
        "id": "fjjvMifYZf7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPeeC7R8DaLV",
        "outputId": "37565c4d-7606-4f24-d25c-feb4d3acc142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Block(\n",
              "    (sa): MultiHeadAttention(\n",
              "      (heads): ModuleList(\n",
              "        (0-3): 4 x Head(\n",
              "          (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "          (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "          (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (ffwd): FeedFoward(\n",
              "      (net): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (1): Block(\n",
              "    (sa): MultiHeadAttention(\n",
              "      (heads): ModuleList(\n",
              "        (0-3): 4 x Head(\n",
              "          (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "          (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "          (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (ffwd): FeedFoward(\n",
              "      (net): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (2): Block(\n",
              "    (sa): MultiHeadAttention(\n",
              "      (heads): ModuleList(\n",
              "        (0-3): 4 x Head(\n",
              "          (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "          (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "          (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (ffwd): FeedFoward(\n",
              "      (net): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (3): Block(\n",
              "    (sa): MultiHeadAttention(\n",
              "      (heads): ModuleList(\n",
              "        (0-3): 4 x Head(\n",
              "          (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "          (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "          (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (ffwd): FeedFoward(\n",
              "      (net): Sequential(\n",
              "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "        (3): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = nn.Sequential([Block(n_embd, n_head=n_head) for _ in range(n_layer)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "0JR80jbRDa9l",
        "outputId": "c3a93c29-49ff-49d1-f42b-aa82995ace08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-8e75ed45ada3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_head\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_item_by_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36madd_module\u001b[0;34m(self, name, module)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \"\"\"\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             raise TypeError(\"{} is not a Module subclass\".format(\n\u001b[0m\u001b[1;32m    597\u001b[0m                 torch.typename(module)))\n\u001b[1;32m    598\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list is not a Module subclass"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nmdtNsC6Hj9-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}