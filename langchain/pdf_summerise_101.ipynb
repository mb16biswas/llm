{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zIWkwjnvIVt",
        "outputId": "20d0ac25-f96c-46af-e74e-2f4f8f1278ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.16)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.34)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.45)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.49)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.23.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement glob (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for glob\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install pypdf\n",
        "!pip install glob\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain import OpenAI, PromptTemplate\n",
        "import glob\n",
        "import os\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader"
      ],
      "metadata": {
        "id": "TvJHqM2GOrX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from secret_key import openapi_key"
      ],
      "metadata": {
        "id": "Ol_GVWaQv1Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = \"\""
      ],
      "metadata": {
        "id": "OGa2cpZiSh9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdfs_folder = \"/content/f\""
      ],
      "metadata": {
        "id": "YLHDER1wPXcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pdf_file in glob.glob(pdfs_folder + \"/*.pdf\"):\n",
        "\n",
        "    print(pdf_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K_08M6gO7DX",
        "outputId": "1836275e-46be-4165-9033-f40fa58319b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/f/2304.11062.pdf\n",
            "/content/f/2109.01134.pdf\n",
            "/content/f/2304.11477.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_file = \"/content/f/2304.11062.pdf\"\n",
        "pdf_file2 = \"/content/f/2304.11477.pdf\"\n",
        "pdf_file3 = \"/content/f/2109.01134.pdf\"\n"
      ],
      "metadata": {
        "id": "tHqdq9gSPnAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.2)\n",
        "loader = PyPDFLoader(pdf_file3)\n",
        "docs = loader.load_and_split()"
      ],
      "metadata": {
        "id": "E1WeVmfdPgV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(pdf_file)\n",
        "docs = loader.load_and_split()\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "summary = chain.run(docs)"
      ],
      "metadata": {
        "id": "zbTsQpR4TJAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "fxE2VZgaT74B",
        "outputId": "2c228a58-fa43-4958-e974-a048006fed4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' This study explores the use of recurrent memory augmentation in pre-trained transformer models to extend input context length while maintaining high retrieval accuracy. The proposed approach allows for storing information in memory for sequences up to two million tokens, with potential applications in natural language understanding and large-scale context processing. The study introduces a new model, the Recurrent Memory Transformer, which shows strong performance on longer sequences and can be combined with other methods for improved efficiency. The study also discusses the use of memory-augmented neural networks in various tasks and the potential for broader applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3RymqQaQ-ME",
        "outputId": "0b14826d-ee78-4536-89ec-0b24caecb082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8JQwq8WRApp",
        "outputId": "5d8adb13-cc80-4179-e8ca-2adf60d48b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[19]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CReRjPlRF05",
        "outputId": "fa2dc74b-b442-410f-c3cc-d7cdf0afcbd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Table 1: RMT can be successfully combined with parameter-\\nefficient methods (parallel adapter, LoRA). Results for language\\nmodeling on the Arxiv dataset for Pythia-70m model.\\nMODEL LOSS\\nADAPTER ONLY 41.43\\nADAPTER + RMT-1 SEG 10.31\\nADAPTER + L ORA + RMT-1 SEG 7.30\\nADAPTER + L ORA + RMT-2 SEG 6.97\\nRMT offers the flexibility of incorporation various cost-\\nefficient training methods, which greatly enhances its prac-\\ntical applicability, especially when computational resources\\nare limited.', metadata={'source': '/content/f/2304.11062.pdf', 'page': 12})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in docs:\n",
        "\n",
        "    print(len(i.page_content))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYrVKDWbRVE5",
        "outputId": "faed1fe3-29ed-4a64-cf73-c456331a4623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4000\n",
            "975\n",
            "3953\n",
            "1586\n",
            "3971\n",
            "775\n",
            "3194\n",
            "3373\n",
            "3987\n",
            "376\n",
            "3972\n",
            "728\n",
            "3994\n",
            "1828\n",
            "3962\n",
            "1961\n",
            "819\n",
            "3542\n",
            "2578\n",
            "489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fYwbF9sVRjAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_pdfs_from_folder(pdfs_folder):\n",
        "    summaries = []\n",
        "    for pdf_file in glob.glob(pdfs_folder + \"/*.pdf\"):\n",
        "        loader = PyPDFLoader(pdf_file)\n",
        "        docs = loader.load_and_split()\n",
        "        chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "        summary = chain.run(docs)\n",
        "        print(\"Summary for: \", pdf_file)\n",
        "        print(summary)\n",
        "        print(\"\\n\")\n",
        "        summaries.append(summary)\n",
        "\n",
        "    return summaries"
      ],
      "metadata": {
        "id": "XElpCXroSmFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_summary(pdf_folder, custom_prompt):\n",
        "    summaries = []\n",
        "    for pdf_file in glob.glob(pdf_folder + \"/*.pdf\"):\n",
        "        loader = PyPDFLoader(pdf_file)\n",
        "        docs = loader.load_and_split()\n",
        "        prompt_template = custom_prompt + \"\"\"\n",
        "\n",
        "        {text}\n",
        "\n",
        "        SUMMARY:\"\"\"\n",
        "        PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
        "        chain = load_summarize_chain(llm, chain_type=\"map_reduce\",\n",
        "                                    map_prompt=PROMPT, combine_prompt=PROMPT)\n",
        "        summary_output = chain({\"input_documents\": docs},return_only_outputs=True)[\"output_text\"]\n",
        "        summaries.append(summary_output)\n",
        "\n",
        "    return summaries"
      ],
      "metadata": {
        "id": "UCf_Gmk4Sm7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = summarize_pdfs_from_folder(\"/content/f\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY1i6rutS1jF",
        "outputId": "dac7c648-9956-4fed-9482-0106dc1fddcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary for:  /content/f/2304.11062.pdf\n",
            " This study explores the use of recurrent memory augmentation in pre-trained transformer models to extend input context length while maintaining high retrieval accuracy. The proposed method allows for storing information in memory for sequences of up to two million tokens, with potential to enhance long-term dependency handling in natural language tasks. The study also discusses previous research on memory in neural architectures and introduces a new model, the Recurrent Memory Transformer (RMT), which has shown promising results in tasks such as long-range language modeling and theorem proving. The article also discusses various techniques and advancements in parameter-efficient transfer learning and the use of memory-augmented neural networks for long-range sequence modeling. The appendix provides details on the training process and compares the efficiency of RMT with full attention models. Overall, RMT has been shown to be a faster and more memory-efficient option for handling longer sequences in natural language processing tasks.\n",
            "\n",
            "\n",
            "Summary for:  /content/f/2109.01134.pdf\n",
            " This article discusses the use of pre-trained vision-language models and the proposed approach of Context Optimization (CoOp) for adapting these models to different image recognition tasks. CoOp has shown promising results in improving performance on various datasets and outperforms hand-crafted prompts. The article also highlights the potential of prompt learning in democratizing foundation models and its impact on transfer learning performance. It compares CoOp to other methods and discusses its potential for future research in efficient adaptation methods for large vision models. The article also mentions the use of language models as few-shot learners and the challenges of generalizing to new data.\n",
            "\n",
            "\n",
            "Summary for:  /content/f/2304.11477.pdf\n",
            " This paper introduces LLM+P, a framework that combines large language models and classical planners to solve planning problems in natural language. It outperforms LLMs alone and connects the model to existing tools. The article discusses the use of symbolic planners and LLMs for planning tasks, and introduces LLM+P as a method for solving these tasks. It also compares LLM+P to other methods and suggests future work. The article also mentions previous work on classical planning and the use of LLMs for planning tasks. Other topics covered include using external tools to enhance language models, and the use of LLMs in various applications such as urban driving and object rearrangement. The article also includes a collection of related research papers on the use of language models in task and motion planning.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summaries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk6vUM48TDAB",
        "outputId": "28612c59-c9c1-4dba-e513-46721bf1e5f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' This study explores the use of recurrent memory augmentation in pre-trained transformer models to extend input context length while maintaining high retrieval accuracy. The proposed method allows for storing information in memory for sequences of up to two million tokens, with potential to enhance long-term dependency handling in natural language tasks. The study also discusses previous research on memory in neural architectures and introduces a new model, the Recurrent Memory Transformer (RMT), which has shown promising results in tasks such as long-range language modeling and theorem proving. The article also discusses various techniques and advancements in parameter-efficient transfer learning and the use of memory-augmented neural networks for long-range sequence modeling. The appendix provides details on the training process and compares the efficiency of RMT with full attention models. Overall, RMT has been shown to be a faster and more memory-efficient option for handling longer sequences in natural language processing tasks.',\n",
              " ' This article discusses the use of pre-trained vision-language models and the proposed approach of Context Optimization (CoOp) for adapting these models to different image recognition tasks. CoOp has shown promising results in improving performance on various datasets and outperforms hand-crafted prompts. The article also highlights the potential of prompt learning in democratizing foundation models and its impact on transfer learning performance. It compares CoOp to other methods and discusses its potential for future research in efficient adaptation methods for large vision models. The article also mentions the use of language models as few-shot learners and the challenges of generalizing to new data.',\n",
              " ' This paper introduces LLM+P, a framework that combines large language models and classical planners to solve planning problems in natural language. It outperforms LLMs alone and connects the model to existing tools. The article discusses the use of symbolic planners and LLMs for planning tasks, and introduces LLM+P as a method for solving these tasks. It also compares LLM+P to other methods and suggests future work. The article also mentions previous work on classical planning and the use of LLMs for planning tasks. Other topics covered include using external tools to enhance language models, and the use of LLMs in various applications such as urban driving and object rearrangement. The article also includes a collection of related research papers on the use of language models in task and motion planning.']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python!\n",
        "loader = PyPDFDirectoryLoader(\"/content/f\")\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "# Create the vector store index\n",
        "index = VectorstoreIndexCreator().from_loaders([loader])"
      ],
      "metadata": {
        "id": "fx8tmFeGUBLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvBEV35cWM2d",
        "outputId": "46062a40-5380-49dc-a84f-d2add3b006bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdrhOVdDX4z2",
        "outputId": "ddabc84d-8d7e-4270-d045-c4cc8078cdac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.document_loaders.pdf.PyPDFDirectoryLoader at 0x7cbeb399c430>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYbMoD8ZWExx",
        "outputId": "9f70a42b-4ebc-4da6-d903-272640a172bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Scaling Transformer to 1M tokens and beyond with RMT\\nAydar Bulatov1, Yuri Kuratov2,1, Yermek Kapushev2, Mikhail Burtsev3\\n1Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny, Russia\\n2AIRI, Moscow, Russia\\n3London Institute for Mathematical Sciences, London, UK\\n{bulatov.as,yurii.kuratov }@phystech.edu, mb@lims.ac.uk\\nAbstract\\nA major limitation for the broader scope of problems solv-\\nable by transformers is the quadratic scaling of computational\\ncomplexity with input size. In this study, we investigate the\\nrecurrent memory augmentation of pre-trained transformer\\nmodels to extend input context length while linearly scaling\\ncompute. Our approach demonstrates the capability to store\\ninformation in memory for sequences of up to an unprece-\\ndented two million tokens while maintaining high retrieval\\naccuracy. Experiments with language modeling tasks show\\nperplexity improvement as the number of processed input\\nsegments increases. These results underscore the effective-\\nness of our method, which has significant potential to enhance\\nlong-term dependency handling in natural language under-\\nstanding and generation tasks, as well as enable large-scale\\ncontext processing for memory-intensive applications.\\nIntroduction\\nTransformer-based models show their effectiveness across\\nmultiple domains and tasks. The self-attention allows to\\ncombine information from all sequence elements into\\ncontext-aware representations. However, global and local in-\\nformation has to be stored mostly in the same element-wise\\nrepresentations. Moreover, the length of an input sequence\\nis limited by quadratic computational complexity of self-\\nattention.\\nIn this work, we propose and study a memory-augmented\\nsegment-level recurrent Transformer (Recurrent Memory\\nTransformer or RMT). Memory allows to store and process\\nlocal and global information as well as to pass information\\nbetween segments of the long sequence with the help of\\nrecurrence. We implement a memory mechanism with no\\nchanges to Transformer model by adding special memory\\ntokens to the input or output sequence. Then Transformer\\nis trained to control both memory operations and sequence\\nrepresentations processing.\\nIn this study we show that by using simple token-based\\nmemory mechanism introduced in (Bulatov, Kuratov, and\\nBurtsev 2022) can be combined with pretrained transformer\\nmodels like BERT (Devlin et al. 2019) and GPT-2 (Radford\\net al. 2019) with full attention and full precision operations.\\nCopyright © 2024, Association for the Advancement of Artificial\\nIntelligence (www.aaai.org). All rights reserved.Contributions 1. We expand application of RMT to\\nencoder-only and decoder-only pre-trained language mod-\\nels. Proposed segment wise curriculum learning allows to\\nfine-tune majority of pre-trained transformer based models\\nfor processing potentially unlimited sequences.\\n2. To benchmark generalization capabilities of RMT we\\npropose a set of novel memory acquisition and retention\\ntasks scalable to extremely long sequences of million tokens.\\n3. We demonstrate the unparalleled ability of RMT to gen-\\neralize memory operations, successfully detecting and stor-\\ning information about facts for up to two million tokens. To\\nthe best of our knowledge, this establishes a record for the\\nlongest sequence task processed by any existing deep neural\\nnetwork. Furthermore, we identify no technical limitations\\nthat prevent further scaling.\\n4. We compare computational complexity of RMT vs.\\nother transformer models and demonstrate the significant ad-\\nvantage of RMT due to its linear scaling of inference opera-\\ntions and constant memory.\\nThe code is available on GitHub1. The paper version with\\nsupplementary materials is available on arXiv2.\\nRelated Work\\nOur work revolves around the concept of memory in neural\\narchitectures. Memory has been a recurrent theme in neu-\\nral network research, dating back to early works (McCul-\\nloch and Pitts 1943; Stephen 1956) and significantly ad-\\nvancing in the 1990s with the introduction of the Back-\\npropagation Through Time learning algorithm (Werbos\\n1990) and Long-Short Term Memory (LSTM) neural ar-\\nchitecture (Hochreiter and Schmidhuber 1997). Contempo-\\nrary memory-augmented neural networks (MANNs) typi-\\ncally utilize some form of recurrent external memory sep-\\narate from the model’s parameters. Neural Turing Machines\\n(NTMs) (Graves, Wayne, and Danihelka 2014) and Memory\\nNetworks (Weston, Chopra, and Bordes 2015) are equipped\\nwith storage for vector representations accessible through an\\nattention mechanism. Memory Networks (Weston, Chopra,\\nand Bordes 2015; Sukhbaatar et al. 2015) were designed to\\nenable reasoning through sequential attention over memory\\ncontent.\\n1https://github.com/booydar/t5-experiments/tree/aaai24\\n2https://arxiv.org/abs/2304.11062arXiv:2304.11062v2  [cs.CL]  6 Feb 2024', metadata={'source': '/content/f/2304.11062.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opNjsYVzWRXI",
        "outputId": "290a5006-6d14-4c5f-e63b-9f3f4b2b0384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Autoregressive task and motion planning with llms as translators and\\ncheckers,” arXiv preprint arXiv:2306.06531 , 2023.\\n[46] K. Valmeekam, S. Sreedharan, M. Marquez, A. Olmo, and S. Kamb-\\nhampati, “On the planning abilities of large language models (a\\ncritical investigation with a proposed benchmark),” arXiv preprint\\narXiv:2302.06706 , 2023.\\n[47] T. Silver, V . Hariprasad, R. S. Shuttleworth, N. Kumar, T. Lozano-\\nP´erez, and L. P. Kaelbling, “PDDL planning with pretrained\\nlarge language models,” in NeurIPS 2022 Foundation Models\\nfor Decision Making Workshop , 2022. [Online]. Available: https:\\n//openreview.net/forum?id=1QMMUB4zfl\\n[48] V . Pallagani, B. Muppasani, K. Murugesan, F. Rossi, L. Horesh,\\nB. Srivastava, F. Fabiano, and A. Loreggia, “Plansformer: Generating\\nsymbolic plans using transformers,” arXiv preprint arXiv:2212.08681 ,\\n2022.\\n[49] D. Arora and S. Kambhampati, “Learning and leveraging verifiers to\\nimprove planning capabilities of pre-trained language models,” arXiv\\npreprint arXiv:2305.17077 , 2023.\\n[50] L. Guan, K. Valmeekam, S. Sreedharan, and S. Kambhampati,\\n“Leveraging pre-trained large language models to construct and uti-\\nlize world models for model-based task planning,” arXiv preprint\\narXiv:2305.14909 , 2023.\\n[51] T. Silver, S. Dan, K. Srinivas, J. B. Tenenbaum, L. P. Kaelbling, and\\nM. Katz, “Generalized planning in pddl domains with pretrained large\\nlanguage models,” arXiv preprint arXiv:2305.11014 , 2023.\\n[52] V . Pallagani, B. Muppasani, K. Murugesan, F. Rossi, B. Srivastava,\\nL. Horesh, F. Fabiano, and A. Loreggia, “Understanding the capabili-\\nties of large language models for automated planning,” arXiv preprint\\narXiv:2305.16151 , 2023.\\n[53] K. Valmeekam, M. Marquez, S. Sreedharan, and S. Kambhampati,\\n“On the planning abilities of large language models–a critical investi-\\ngation,” arXiv preprint arXiv:2305.15771 , 2023.\\n[54] Y . Xie, C. Yu, T. Zhu, J. Bai, Z. Gong, and H. Soh, “Translating\\nnatural language to planning goals with large-language models,” arXiv\\npreprint arXiv:2302.05128 , 2023.\\n[55] R. Hazra, P. Z. D. Martires, and L. De Raedt, “Saycanpay: Heuristic\\nplanning with large language models using learnable domain knowl-\\nedge,” arXiv preprint arXiv:2308.12682 , 2023.\\n[56] K. Rana, J. Haviland, S. Garg, J. Abou-Chakra, I. Reid, and N. Suen-\\nderhauf, “Sayplan: Grounding large language models using 3d scene\\ngraphs for scalable task planning,” arXiv preprint arXiv:2307.06135 ,\\n2023.\\n[57] Z. Zhou, J. Song, K. Yao, Z. Shu, and L. Ma, “Isr-llm: Iterative\\nself-refined large language model for long-horizon sequential task\\nplanning,” arXiv preprint arXiv:2308.13724 , 2023.\\n[58] Z. Wang, S. Cai, A. Liu, X. Ma, and Y . Liang, “Describe, explain,\\nplan and select: Interactive planning with large language models en-\\nables open-world multi-task agents,” arXiv preprint arXiv:2302.01560 ,\\n2023.\\n[59] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V . Kosaraju, W. Saunders, et al. , “Webgpt: Browser-\\nassisted question-answering with human feedback,” arXiv preprint\\narXiv:2112.09332 , 2021.\\n[60] A. Lazaridou, E. Gribovskaya, W. Stokowiec, and N. Grigorev,\\n“Internet-augmented language models through few-shot prompting for\\nopen-domain question answering,” arXiv preprint arXiv:2203.05115 ,\\n2022.\\n[61] A. Madaan, N. Tandon, P. Clark, and Y . Yang, “Memory-assisted\\nprompt editing to improve gpt-3 after deployment,” 2023.\\n[62] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\\nmoyer, and W.-t. Yih, “Replug: Retrieval-augmented black-box lan-\\nguage models,” arXiv preprint arXiv:2301.12652 , 2023.\\n[63] W. Chen, X. Ma, X. Wang, and W. W. Cohen, “Program of thoughts\\nprompting: Disentangling computation from reasoning for numerical\\nreasoning tasks,” arXiv preprint arXiv:2211.12588 , 2022.\\n[64] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y . Yang, J. Callan,\\nand G. Neubig, “Pal: Program-aided language models,” arXiv preprint\\narXiv:2211.10435 , 2022.\\n[65] T. Schick, J. Dwivedi-Yu, R. Dess `ı, R. Raileanu, M. Lomeli, L. Zettle-\\nmoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\\ncan teach themselves to use tools,” arXiv preprint arXiv:2302.04761 ,\\n2023.\\n[66] Q. Lyu, S. Havaldar, A. Stein, L. Zhang, D. Rao, E. Wong, M. Apid-\\nianaki, and C. Callison-Burch, “Faithful chain-of-thought reasoning,”\\narXiv preprint arXiv:2301.13379 , 2023.[67] J. Seipp, ´A. Torralba, and J. Hoffmann, “PDDL generators,” https:\\n//doi.org/10.5281/zenodo.6382173, 2022.\\n[68] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y . Cao, and\\nK. Narasimhan, “Tree of thoughts: Deliberate problem solving with\\nlarge language models,” arXiv preprint arXiv:2305.10601 , 2023.', metadata={'source': '/content/f/2304.11477.pdf', 'page': 7})"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAhPUdLaWXPr",
        "outputId": "9f1ec019-69b4-4157-a7fe-3e9dc7278a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Wang H, Ge S, Lipton Z, Xing EP (2019) Learning robust\\nglobal representations by penalizing local predictive power.\\nIn: NeurIPS\\nXiao J, Hays J, Ehinger KA, Oliva A, Torralba A (2010) Sun\\ndatabase: Large-scale scene recognition from abbey to zoo.\\nIn: CVPR\\nYuan L, Chen D, Chen YL, Codella N, Dai X, Gao J,\\nHu H, Huang X, Li B, Li C, et al. (2021) Florence: A\\nnew foundation model for computer vision. arXiv preprint\\narXiv:211111432\\nZhang Y, Jiang H, Miura Y, Manning CD, Langlotz CP\\n(2020) Contrastive learning of medical visual represen-\\ntations from paired images and text. arXiv preprint\\narXiv:201000747\\nZhong Z, Friedman D, Chen D (2021) Factual probing is\\n[mask]: Learning vs. learning to recall. In: NAACL\\nZhou B, Lapedriza A, Khosla A, Oliva A, Torralba A (2017)\\nPlaces: A 10 million image database for scene recognition.\\nIEEE transactions on pattern analysis and machine intel-\\nligence 40(6):1452–1464\\nZhou K, Liu Z, Qiao Y, Xiang T, Loy CC (2021) Domain\\ngeneralization: A survey. arXiv preprint arXiv:210302503\\nZhou K, Yang J, Loy CC, Liu Z (2022a) Conditional\\nprompt learning for vision-language models. arXiv preprint\\narXiv:220305557\\nZhou K, Zhang Y, Zang Y, Yang J, Loy CC, Liu Z\\n(2022b) On-device domain generalization. arXiv preprint\\narXiv:220907521', metadata={'source': '/content/f/2109.01134.pdf', 'page': 12})"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHABx3ybYX-R",
        "outputId": "4a5cf889-bfde-4b86-f642-27d6a8bb54f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT98tgtVXUtg",
        "outputId": "954dc683-b90f-4f83-b55d-3eeb52de0be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreIndexWrapper(vectorstore=<langchain_community.vectorstores.inmemory.InMemoryVectorStore object at 0x7cbeafcced10>)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"who is Ms dhoni?\"\n",
        "\n",
        "index.query(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rudgcmvBZr-c",
        "outputId": "960cee16-0409-4976-db62-1729abb8a0d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I don't know.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J0tWZoJHab03"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}