{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZY4EWL8nBwh"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install shap\n",
        "!pip install -q transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import torch\n",
        "from transformers import AutoTokenizer, pipeline, TextClassificationPipeline, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import shap\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from itertools import combinations\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score"
      ],
      "metadata": {
        "id": "NFaRr6EapvTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_state = 42\n",
        "epochs = 1\n",
        "learning_rate = 2e-5\n",
        "batch_size = 4\n",
        "metric_name = \"f1\"\n",
        "model_name = \"bert-base-uncased\"\n",
        "model_name2 = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
        "tokenizer2.pad_token = tokenizer2.eos_token\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "RDID5DTtpzVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"final-data.csv\")"
      ],
      "metadata": {
        "id": "bddJzhudqanp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val = train_test_split(df, test_size = 0.2 , random_state = random_state)"
      ],
      "metadata": {
        "id": "TgLpyjZaqgy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\"bug_reports\" , \"features_request\" , 'experience']\n",
        "\n",
        "id2label = {\n",
        "    0 : \"bug_reports\",\n",
        "    1 : \"features_request\" ,\n",
        "    2 : 'experience'\n",
        "}\n",
        "\n",
        "\n",
        "label2id = {\n",
        "    \"bug_reports\" : 0 ,\n",
        "    \"features_request\" : 1 ,\n",
        "    'experience' : 2\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "reWvslr2q1No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bert\n",
        "def preprocess_data(examples):\n",
        "  # take a batch of texts\n",
        "  text = examples[\"content\"]\n",
        "  # encode them\n",
        "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512)\n",
        "  # add labels\n",
        "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
        "  # create numpy array of shape (batch_size, num_labels)\n",
        "  labels_matrix = np.zeros((len(text), len(labels)))\n",
        "  # fill numpy array\n",
        "  for idx, label in enumerate(labels):\n",
        "    labels_matrix[:, idx] = labels_batch[label]\n",
        "\n",
        "  encoding[\"labels\"] = labels_matrix.tolist()\n",
        "\n",
        "  return encoding\n",
        "\n",
        "\n",
        "#Gpt-2\n",
        "def preprocess_data2(examples):\n",
        "  # take a batch of texts\n",
        "  text = examples[\"content\"]\n",
        "  # encode them\n",
        "  encoding = tokenizer2(text, padding=\"max_length\", truncation=True, max_length=512)\n",
        "  # add labels\n",
        "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
        "  # create numpy array of shape (batch_size, num_labels)\n",
        "  labels_matrix = np.zeros((len(text), len(labels)))\n",
        "  # fill numpy array\n",
        "  for idx, label in enumerate(labels):\n",
        "    labels_matrix[:, idx] = labels_batch[label]\n",
        "\n",
        "  encoding[\"labels\"] = labels_matrix.tolist()\n",
        "\n",
        "  return encoding"
      ],
      "metadata": {
        "id": "n4vLtXnBq5h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = Dataset.from_pandas(df_train)\n",
        "df_val = Dataset.from_pandas(df_val)"
      ],
      "metadata": {
        "id": "wq8zF9jvKnZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bert\n",
        "encoded_dataset_train = df_train.map(preprocess_data, batched=True , remove_columns=df_train.column_names)\n",
        "encoded_dataset_val = df_val.map(preprocess_data, batched=True , remove_columns=df_val.column_names)\n",
        "\n",
        "#Gpt-2\n",
        "encoded_dataset_train2 = df_train.map(preprocess_data2, batched=True , remove_columns=df_train.column_names)\n",
        "encoded_dataset_val2 = df_val.map(preprocess_data2, batched=True , remove_columns=df_val.column_names)"
      ],
      "metadata": {
        "id": "fsGY_ve1q_Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dataset_train.set_format(\"torch\")\n",
        "encoded_dataset_val.set_format(\"torch\")\n",
        "\n",
        "\n",
        "encoded_dataset_train2.set_format(\"torch\")\n",
        "encoded_dataset_val2.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "wFNu_Ug4rQyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
        "                                                           problem_type=\"multi_label_classification\",\n",
        "                                                           num_labels=len(labels),\n",
        "                                                           id2label=id2label,\n",
        "                                                           label2id=label2id)\n",
        "\n",
        "\n",
        "model2 = AutoModelForSequenceClassification.from_pretrained(model_name2,\n",
        "                                                           problem_type=\"multi_label_classification\",\n",
        "                                                           num_labels=len(labels),\n",
        "                                                           id2label=id2label,\n",
        "                                                           label2id=label2id)\n",
        "\n",
        "model2.config.pad_token_id = model2.config.eos_token_id"
      ],
      "metadata": {
        "id": "EU-KyL1LrWL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "f\"/Model/bert\",\n",
        "evaluation_strategy = \"epoch\",\n",
        "save_strategy = \"epoch\",\n",
        "learning_rate=learning_rate,\n",
        "per_device_train_batch_size=batch_size,\n",
        "per_device_eval_batch_size=batch_size,\n",
        "num_train_epochs=epochs ,\n",
        "weight_decay=0.01,\n",
        "save_total_limit=2,\n",
        "metric_for_best_model=metric_name,\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "args2 = TrainingArguments(\n",
        "f\"/Model/gpt2\",\n",
        "evaluation_strategy = \"epoch\",\n",
        "save_strategy = \"epoch\",\n",
        "learning_rate=learning_rate,\n",
        "per_device_train_batch_size=batch_size,\n",
        "per_device_eval_batch_size=batch_size,\n",
        "num_train_epochs=epochs ,\n",
        "weight_decay=0.01,\n",
        "save_total_limit=1,\n",
        "metric_for_best_model=metric_name,\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TgQSXbDAsZY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
        "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
        "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(predictions))\n",
        "    # next, use threshold to turn them into integer predictions\n",
        "    y_pred = np.zeros(probs.shape)\n",
        "    y_pred[np.where(probs >= threshold)] = 1\n",
        "    # finally, compute metrics\n",
        "    y_true = labels\n",
        "    f1_ = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n",
        "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    # return as dictionary\n",
        "    metrics = {\"f1\": f1_,\n",
        "               \"roc_auc\": roc_auc,\n",
        "               \"accuracy\": accuracy}\n",
        "    return metrics\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions,\n",
        "            tuple) else p.predictions\n",
        "    result = multi_label_metrics(\n",
        "        predictions=preds,\n",
        "        labels=p.label_ids)\n",
        "    return result"
      ],
      "metadata": {
        "id": "ITXduxNStv8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset_train,\n",
        "    eval_dataset=encoded_dataset_val,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "-bxm8kd4wkQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = trainer.train()\n",
        "e = trainer.evaluate()\n",
        "trainer.save_model()\n",
        "\n",
        "\n",
        "trainer.log_metrics(\"train\", t.metrics)\n",
        "trainer.save_metrics(\"train\", t.metrics)\n",
        "\n",
        "trainer.log_metrics(\"eval\", e)\n",
        "trainer.save_metrics(\"eval\", e)"
      ],
      "metadata": {
        "id": "aCxSXP89wg_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer2 = Trainer(\n",
        "    model2,\n",
        "    args2,\n",
        "    train_dataset=encoded_dataset_train2,\n",
        "    eval_dataset=encoded_dataset_val2,\n",
        "    tokenizer=tokenizer2,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "K5Sc6QK5w0cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t2 = trainer2.train()\n",
        "e2 = trainer2.evaluate()\n",
        "trainer2.save_model()\n",
        "\n",
        "\n",
        "trainer2.log_metrics(\"train\", t2.metrics)\n",
        "trainer2.save_metrics(\"train\", t2.metrics)\n",
        "\n",
        "trainer2.log_metrics(\"eval\", e2)\n",
        "trainer2.save_metrics(\"eval\", e2)"
      ],
      "metadata": {
        "id": "wDhh3ePew188"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}